{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd49aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577ebc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "938d2c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11e641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c10561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# notebooks_v2 -> repo root\n",
    "os.chdir(Path.cwd().parent)\n",
    "print(\"CWD =\", Path.cwd())\n",
    "assert (Path.cwd() / \"src\").exists(), \"Still not at repo root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d4751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py.bak_20260221_004939\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py.bak_20260221_004939\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py.bak_20260221_004939\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py.bak_20260221_004939\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py\n",
      "\n",
      "✅ FULL AUTONOMY ENGINE + UPDATED DASHBOARD WRITTEN.\n",
      "Next:\n",
      "1) Restart backend\n",
      "   PYTHONPATH=src uv run python -m uvicorn careeragent.api.main:app --host 127.0.0.1 --port 8000 --reload\n",
      "2) Restart frontend\n",
      "   API_URL=http://127.0.0.1:8000 PYTHONPATH='.:src' uv run streamlit run app/main.py --server.port 8501\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK CELL — FULL ONE-CLICK AUTONOMY UPGRADE (Engine + Dashboard)\n",
    "# Overwrites:\n",
    "# - src/careeragent/orchestration/engine.py\n",
    "# - src/careeragent/orchestration/__init__.py\n",
    "# - app/ui/dashboard.py  (adds F1/OPT sponsorship toggle + recency + max jobs)\n",
    "# - app/main.py          (robust bootstrap)\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "assert (ROOT / \"src\").exists(), f\"Run from repo root. CWD={ROOT}\"\n",
    "\n",
    "def backup_write(rel_path: str, content: str) -> None:\n",
    "    p = ROOT / rel_path\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts}\")\n",
    "        bak.write_text(p.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(\"BACKUP:\", bak)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    print(\"WROTE:\", p)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Engine (full L0-L9 loop)\n",
    "# ---------------------------\n",
    "ENGINE_PY = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from uuid import uuid4\n",
    "\n",
    "import httpx\n",
    "\n",
    "from careeragent.config import artifacts_root, get_settings\n",
    "from careeragent.orchestration.state import OrchestrationState\n",
    "from careeragent.services.db_service import SqliteStateStore\n",
    "from careeragent.services.notification_service import NotificationService\n",
    "from careeragent.services.health_service import HealthService\n",
    "\n",
    "from careeragent.agents.security_agent import SanitizeAgent\n",
    "\n",
    "# Optional (if present in your repo from previous batches)\n",
    "try:\n",
    "    from careeragent.agents.guardrail_service import OutputGuard  # type: ignore\n",
    "except Exception:\n",
    "    OutputGuard = None  # type: ignore\n",
    "\n",
    "from careeragent.agents.parser_agent_service import ParserAgentService, ExtractedResume\n",
    "from careeragent.agents.parser_evaluator_service import ParserEvaluatorService\n",
    "\n",
    "# If your existing matcher services exist, we use them. Otherwise we use local scoring.\n",
    "try:\n",
    "    from careeragent.agents.matcher_agent_schema import JobDescription  # type: ignore\n",
    "    from careeragent.agents.matcher_agent_service import MatcherAgentService  # type: ignore\n",
    "except Exception:\n",
    "    JobDescription = None  # type: ignore\n",
    "    MatcherAgentService = None  # type: ignore\n",
    "\n",
    "# Draft/apply/XAI are best-effort if available\n",
    "try:\n",
    "    from careeragent.agents.cover_letter_service import CoverLetterService  # type: ignore\n",
    "    from careeragent.agents.cover_letter_evaluator_service import CoverLetterEvaluatorService  # type: ignore\n",
    "except Exception:\n",
    "    CoverLetterService = None  # type: ignore\n",
    "    CoverLetterEvaluatorService = None  # type: ignore\n",
    "\n",
    "try:\n",
    "    from careeragent.agents.strategy_agent_service import StrategyAgentService  # type: ignore\n",
    "except Exception:\n",
    "    StrategyAgentService = None  # type: ignore\n",
    "\n",
    "try:\n",
    "    from careeragent.agents.apply_executor_service import ApplyExecutorService  # type: ignore\n",
    "except Exception:\n",
    "    ApplyExecutorService = None  # type: ignore\n",
    "\n",
    "try:\n",
    "    from careeragent.services.xai_service import XAIService  # type: ignore\n",
    "    from careeragent.services.exporter import CareerDossierExporter  # type: ignore\n",
    "except Exception:\n",
    "    XAIService = None  # type: ignore\n",
    "    CareerDossierExporter = None  # type: ignore\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class JobBoard:\n",
    "    name: str\n",
    "    domain: str\n",
    "\n",
    "\n",
    "DEFAULT_JOB_BOARDS: Tuple[JobBoard, ...] = (\n",
    "    JobBoard(\"LinkedIn Jobs\", \"linkedin.com/jobs\"),\n",
    "    JobBoard(\"Indeed\", \"indeed.com\"),\n",
    "    JobBoard(\"Glassdoor\", \"glassdoor.com\"),\n",
    "    JobBoard(\"ZipRecruiter\", \"ziprecruiter.com\"),\n",
    "    JobBoard(\"Monster\", \"monster.com\"),\n",
    "    JobBoard(\"Dice\", \"dice.com\"),\n",
    "    JobBoard(\"Lever\", \"jobs.lever.co\"),\n",
    "    JobBoard(\"Greenhouse\", \"boards.greenhouse.io\"),\n",
    ")\n",
    "\n",
    "\n",
    "VISA_NEGATIVE = (\n",
    "    \"unable to sponsor\",\n",
    "    \"cannot sponsor\",\n",
    "    \"no sponsorship\",\n",
    "    \"do not sponsor\",\n",
    "    \"not sponsor\",\n",
    "    \"without sponsorship\",\n",
    "    \"must be authorized to work\",\n",
    "    \"no visa\",\n",
    "    \"cannot provide visa\",\n",
    ")\n",
    "\n",
    "VISA_POSITIVE = (\n",
    "    \"visa sponsorship\",\n",
    "    \"sponsorship available\",\n",
    "    \"h1b\",\n",
    "    \"opt\",\n",
    "    \"cpt\",\n",
    "    \"stem opt\",\n",
    "    \"work visa\",\n",
    ")\n",
    "\n",
    "COMMON_SKILL_DICTIONARY = [\n",
    "    \"python\",\"sql\",\"fastapi\",\"docker\",\"kubernetes\",\"mlflow\",\"dvc\",\"azure\",\"aws\",\"gcp\",\n",
    "    \"pytorch\",\"tensorflow\",\"sklearn\",\"pandas\",\"numpy\",\"spark\",\"databricks\",\"snowflake\",\n",
    "    \"llm\",\"rag\",\"langgraph\",\"langchain\",\"vector database\",\"faiss\",\"chroma\",\"postgres\",\"redis\",\n",
    "    \"etl\",\"airflow\",\"kafka\",\"terraform\",\"github actions\",\"sagemaker\",\"azure ml\",\"openai\"\n",
    "]\n",
    "\n",
    "\n",
    "def _run_dir(run_id: str) -> Path:\n",
    "    d = artifacts_root() / \"runs\" / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _save_json(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "class LiveFeed:\n",
    "    \"\"\"\n",
    "    Description: Live Agent Feed logger (UI consumes state.meta['live_feed']).\n",
    "    Layer: L1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def emit(state: OrchestrationState, *, layer: str, agent: str, message: str) -> None:\n",
    "        state.meta.setdefault(\"live_feed\", [])\n",
    "        state.meta[\"live_feed\"].append({\"layer\": layer, \"agent\": agent, \"message\": message})\n",
    "        state.touch()\n",
    "\n",
    "\n",
    "class LocalResumeExtractor:\n",
    "    \"\"\"\n",
    "    Description: Extract resume text from PDF/TXT/DOCX bytes.\n",
    "    Layer: L2\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def extract_text(*, filename: str, data: bytes) -> str:\n",
    "        name = (filename or \"\").lower()\n",
    "        if name.endswith(\".txt\"):\n",
    "            return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "        if name.endswith(\".pdf\"):\n",
    "            try:\n",
    "                from pypdf import PdfReader  # type: ignore\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"PDF extraction needs pypdf. Install: uv add pypdf\") from e\n",
    "            import io\n",
    "            reader = PdfReader(io.BytesIO(data))\n",
    "            return \"\\n\".join([(pg.extract_text() or \"\") for pg in reader.pages])\n",
    "\n",
    "        if name.endswith(\".docx\"):\n",
    "            try:\n",
    "                import docx  # type: ignore\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"DOCX extraction needs python-docx. Install: uv add python-docx\") from e\n",
    "            import io\n",
    "            f = io.BytesIO(data)\n",
    "            d = docx.Document(f)\n",
    "            return \"\\n\".join([p.text for p in d.paragraphs if p.text])\n",
    "\n",
    "        # fallback\n",
    "        return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"\n",
    "    Description: Optional local LLM used for query refinement + job summaries.\n",
    "    Layer: L4-L5\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url: Optional[str], model: str) -> None:\n",
    "        self._base = (base_url or \"\").strip().rstrip(\"/\")\n",
    "        self._model = model\n",
    "\n",
    "    def available(self) -> bool:\n",
    "        return bool(self._base)\n",
    "\n",
    "    def generate(self, *, prompt: str, timeout: float = 45.0) -> str:\n",
    "        if not self.available():\n",
    "            return \"\"\n",
    "        try:\n",
    "            url = self._base + \"/api/generate\"\n",
    "            payload = {\"model\": self._model, \"prompt\": prompt, \"stream\": False}\n",
    "            with httpx.Client(timeout=timeout) as client:\n",
    "                r = client.post(url, json=payload)\n",
    "            if r.status_code >= 400:\n",
    "                return \"\"\n",
    "            return (r.json().get(\"response\") or \"\").strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "class SerperDiscoveryService:\n",
    "    \"\"\"\n",
    "    Description: Serper discovery across job boards with best-effort recency.\n",
    "    Layer: L3\n",
    "    \"\"\"\n",
    "    SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "    def __init__(self, *, api_key: str, health: HealthService) -> None:\n",
    "        self._key = api_key\n",
    "        self._health = health\n",
    "\n",
    "    def search(self, *, state: OrchestrationState, step_id: str, query: str, num: int = 20, tbs: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        headers = {\"X-API-KEY\": self._key, \"Content-Type\": \"application/json\"}\n",
    "        body: Dict[str, Any] = {\"q\": query, \"num\": num}\n",
    "        if tbs:\n",
    "            # Serper may accept tbs; if it doesn't, it will just ignore it (best-effort)\n",
    "            body[\"tbs\"] = tbs\n",
    "\n",
    "        try:\n",
    "            with httpx.Client(timeout=30.0) as client:\n",
    "                r = client.post(self.SERPER_URL, headers=headers, json=body)\n",
    "        except Exception as e:\n",
    "            state.status = \"api_failure\"\n",
    "            state.meta[\"run_failure_code\"] = \"API_FAILURE\"\n",
    "            state.meta[\"run_failure_provider\"] = \"serper\"\n",
    "            LiveFeed.emit(state, layer=\"L3\", agent=\"DiscoveryService\", message=f\"Serper request failed: {e}\")\n",
    "            return []\n",
    "\n",
    "        if self._health.quota.handle_serper_response(\n",
    "            state=state, step_id=step_id, status_code=r.status_code, tool_name=\"serper.search\", error_detail=r.text[:200]\n",
    "        ):\n",
    "            return []\n",
    "\n",
    "        if r.status_code >= 400:\n",
    "            state.status = \"api_failure\"\n",
    "            state.meta[\"run_failure_code\"] = \"API_FAILURE\"\n",
    "            state.meta[\"run_failure_provider\"] = \"serper\"\n",
    "            LiveFeed.emit(state, layer=\"L3\", agent=\"DiscoveryService\", message=f\"Serper error {r.status_code}: {r.text[:200]}\")\n",
    "            return []\n",
    "\n",
    "        organic = (r.json().get(\"organic\") or [])\n",
    "        out = []\n",
    "        for it in organic:\n",
    "            out.append({\"title\": it.get(\"title\") or \"\", \"link\": it.get(\"link\") or \"\", \"snippet\": it.get(\"snippet\") or \"\"})\n",
    "        return out\n",
    "\n",
    "\n",
    "class ScraperService:\n",
    "    \"\"\"\n",
    "    Description: Scrape job pages best-effort. Falls back to snippet.\n",
    "    Layer: L3-L4\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def fetch_text(*, url: str, snippet: str) -> str:\n",
    "        if not url:\n",
    "            return snippet or \"\"\n",
    "        try:\n",
    "            with httpx.Client(timeout=18.0, follow_redirects=True) as client:\n",
    "                r = client.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            if r.status_code >= 400:\n",
    "                return snippet or \"\"\n",
    "            html = r.text\n",
    "            txt = re.sub(r\"<(script|style)[^>]*>.*?</\\1>\", \" \", html, flags=re.S | re.I)\n",
    "            txt = re.sub(r\"<[^>]+>\", \" \", txt)\n",
    "            txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "            if not txt:\n",
    "                return snippet or \"\"\n",
    "            return txt[:16000]\n",
    "        except Exception:\n",
    "            return snippet or \"\"\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-zA-Z][a-zA-Z0-9\\+\\#\\.-]{1,}\", (text or \"\").lower())\n",
    "\n",
    "\n",
    "def cosine_sim(a: Counter, b: Counter) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    dot = sum(a[t] * b.get(t, 0) for t in a)\n",
    "    na = sum(v*v for v in a.values()) ** 0.5\n",
    "    nb = sum(v*v for v in b.values()) ** 0.5\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return float(dot / (na * nb))\n",
    "\n",
    "\n",
    "def parse_recency_hours(text: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Best-effort parsing from snippets like \"12 hours ago\", \"1 day ago\", \"yesterday\", \"today\".\n",
    "    \"\"\"\n",
    "    s = (text or \"\").lower()\n",
    "    if \"today\" in s:\n",
    "        return 6.0\n",
    "    if \"yesterday\" in s:\n",
    "        return 24.0\n",
    "    m = re.search(r\"(\\d+)\\s*(hour|hours)\\s*ago\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "    m = re.search(r\"(\\d+)\\s*(day|days)\\s*ago\", s)\n",
    "    if m:\n",
    "        return float(m.group(1)) * 24.0\n",
    "    m = re.search(r\"(\\d+)\\s*(minute|minutes)\\s*ago\", s)\n",
    "    if m:\n",
    "        return max(1.0, float(m.group(1)) / 60.0)\n",
    "    return None\n",
    "\n",
    "\n",
    "def ats_score_from_text(resume_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Lightweight ATS heuristic for the resume itself (0-1).\n",
    "    \"\"\"\n",
    "    t = resume_text.lower()\n",
    "    score = 0.0\n",
    "    # contact signals\n",
    "    if re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", t):\n",
    "        score += 0.18\n",
    "    if re.search(r\"\\+?\\d[\\d\\-\\s\\(\\)]{8,}\\d\", t):\n",
    "        score += 0.10\n",
    "    # headings\n",
    "    for h in [\"summary\", \"experience\", \"education\", \"skills\", \"projects\"]:\n",
    "        if h in t:\n",
    "            score += 0.12\n",
    "    # bullets / structure\n",
    "    if \"-\" in resume_text or \"•\" in resume_text:\n",
    "        score += 0.12\n",
    "    # length (not too short)\n",
    "    if len(resume_text) > 1500:\n",
    "        score += 0.12\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "def visa_ok(job_text: str) -> bool:\n",
    "    low = (job_text or \"\").lower()\n",
    "    return not any(x in low for x in VISA_NEGATIVE)\n",
    "\n",
    "\n",
    "def visa_positive(job_text: str) -> bool:\n",
    "    low = (job_text or \"\").lower()\n",
    "    return any(x in low for x in VISA_POSITIVE)\n",
    "\n",
    "\n",
    "def extract_job_skills(job_text: str, resume_skills: List[str]) -> List[str]:\n",
    "    low = (job_text or \"\").lower()\n",
    "    skills = []\n",
    "    pool = list(dict.fromkeys([*(resume_skills or []), *COMMON_SKILL_DICTIONARY]))\n",
    "    for s in pool:\n",
    "        s2 = str(s).lower().strip()\n",
    "        if not s2:\n",
    "            continue\n",
    "        if s2 in low:\n",
    "            skills.append(s2)\n",
    "    return list(dict.fromkeys(skills))[:30]\n",
    "\n",
    "\n",
    "def compute_interview_chance(*, skill_overlap: float, exp_align: float, ats: float, market: float) -> float:\n",
    "    market = max(1.0, float(market))\n",
    "    score = (0.45 * skill_overlap + 0.35 * exp_align + 0.20 * ats) / market\n",
    "    return max(0.0, min(1.0, float(score)))\n",
    "\n",
    "\n",
    "class DiscoveryEvaluatorAgent:\n",
    "    \"\"\"\n",
    "    Description: L5 evaluator that gates discovery quality and triggers query refinement.\n",
    "    Layer: L5\n",
    "    \"\"\"\n",
    "    def evaluate(self, *, top_score: float, avg_score: float, n: int, threshold: float) -> Tuple[float, List[str]]:\n",
    "        fb: List[str] = []\n",
    "        # Confidence model: top weighted more than avg.\n",
    "        conf = min(1.0, (0.65 * top_score) + (0.35 * avg_score))\n",
    "        if n < 10:\n",
    "            conf -= 0.10\n",
    "            fb.append(\"Too few jobs after filters. Broaden search terms or expand boards.\")\n",
    "        if top_score < threshold:\n",
    "            conf -= 0.15\n",
    "            fb.append(f\"Top match below threshold ({top_score*100:.1f}% < {threshold*100:.0f}%). Refining query.\")\n",
    "        conf = max(0.0, min(1.0, conf))\n",
    "        return conf, fb\n",
    "\n",
    "\n",
    "class OneClickAutomationEngine:\n",
    "    \"\"\"\n",
    "    Description: CareerOS-style One-Click autonomous workflow with HITL gates.\n",
    "    Layer: L0-L9\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._s = get_settings()\n",
    "        self._store = SqliteStateStore()\n",
    "\n",
    "        self._health = HealthService()\n",
    "        self._health.load_env(dotenv_path=str(Path(\".env\")))\n",
    "        self._health.enable_langsmith_tracing(project=self._s.langsmith_project)\n",
    "\n",
    "        self._notifier = NotificationService(dry_run=not bool(self._s.twilio_account_sid))\n",
    "        self._sanitize = SanitizeAgent()\n",
    "\n",
    "        self._ollama = OllamaClient(self._s.ollama_base_url, getattr(self._s, \"ollama_model\", \"llama3.2\"))\n",
    "\n",
    "        self._parser = ParserAgentService()\n",
    "        self._parser_eval = ParserEvaluatorService()\n",
    "\n",
    "        self._matcher = MatcherAgentService() if MatcherAgentService else None\n",
    "        self._disc_eval = DiscoveryEvaluatorAgent()\n",
    "\n",
    "        self._out_guard = OutputGuard() if OutputGuard else None\n",
    "        self._cover = CoverLetterService() if CoverLetterService else None\n",
    "        self._cover_eval = CoverLetterEvaluatorService() if CoverLetterEvaluatorService else None\n",
    "        self._strategy = StrategyAgentService() if StrategyAgentService else None\n",
    "        self._apply = ApplyExecutorService() if ApplyExecutorService else None\n",
    "\n",
    "    def _persist(self, state: OrchestrationState) -> None:\n",
    "        d = _run_dir(state.run_id)\n",
    "        _save_json(d / \"state.json\", state.model_dump())\n",
    "        self._store.upsert_state(run_id=state.run_id, status=state.status, state=state.model_dump(), updated_at_utc=state.updated_at_utc)\n",
    "\n",
    "    def load(self, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        return self._store.get_state(run_id=run_id)\n",
    "\n",
    "    def start_run(self, *, filename: str, data: bytes, prefs: Dict[str, Any]) -> OrchestrationState:\n",
    "        st = OrchestrationState.new(env=self._s.environment, mode=\"agentic\", git_sha=None)\n",
    "        st.meta[\"preferences\"] = prefs\n",
    "        st.meta.setdefault(\"job_scores\", {})\n",
    "        st.meta.setdefault(\"job_components\", {})\n",
    "        st.meta.setdefault(\"job_meta\", {})\n",
    "        LiveFeed.emit(st, layer=\"L1\", agent=\"Dashboard\", message=\"Run created. Starting autonomous pipeline…\")\n",
    "        self._persist(st)\n",
    "\n",
    "        t = threading.Thread(target=self._run_pipeline, args=(st.run_id, filename, data), daemon=True)\n",
    "        t.start()\n",
    "        return st\n",
    "\n",
    "    def submit_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any]) -> OrchestrationState:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            raise ValueError(\"run_id not found\")\n",
    "        st = OrchestrationState(**raw)\n",
    "        st.meta[\"last_user_action\"] = {\"type\": action_type, \"payload\": payload}\n",
    "        LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=f\"User action received: {action_type}\")\n",
    "        self._persist(st)\n",
    "\n",
    "        t = threading.Thread(target=self._continue_after_hitl, args=(run_id,), daemon=True)\n",
    "        t.start()\n",
    "        return st\n",
    "\n",
    "    # ---------------- PIPELINE ----------------\n",
    "    def _run_pipeline(self, run_id: str, filename: str, data: bytes) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        st = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "\n",
    "        prefs = st.meta.get(\"preferences\", {}) or {}\n",
    "        recency_hours = float(prefs.get(\"recency_hours\", 36))\n",
    "        max_jobs = int(prefs.get(\"max_jobs\", 40))\n",
    "        target_role = str(prefs.get(\"target_role\", \"Data Scientist\"))\n",
    "        location = str(prefs.get(\"location\", \"United States\"))\n",
    "        remote = bool(prefs.get(\"remote\", True))\n",
    "        wfo_ok = bool(prefs.get(\"wfo_ok\", True))\n",
    "        salary = str(prefs.get(\"salary\", \"\")).strip()\n",
    "        visa_required = bool(prefs.get(\"visa_sponsorship_required\", False))\n",
    "        threshold = float(prefs.get(\"discovery_threshold\", 0.70))\n",
    "        max_refinements = int(prefs.get(\"max_refinements\", 3))\n",
    "\n",
    "        # L2 extract resume text\n",
    "        st.start_step(\"l2_extract\", layer_id=\"L2\", tool_name=\"ResumeExtractor\", input_ref={\"filename\": filename})\n",
    "        LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=\"Extracting resume text from upload…\")\n",
    "        try:\n",
    "            resume_text = LocalResumeExtractor.extract_text(filename=filename, data=data)\n",
    "        except Exception as e:\n",
    "            st.end_step(\"l2_extract\", status=\"failed\", output_ref={}, message=str(e))\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"resume_extract_failed\"\n",
    "            LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=f\"Resume extraction failed: {e}\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        (run_dir / \"resume_raw.txt\").write_text(resume_text, encoding=\"utf-8\")\n",
    "        st.add_artifact(\"resume_raw\", str(run_dir / \"resume_raw.txt\"), content_type=\"text/plain\")\n",
    "        st.end_step(\"l2_extract\", status=\"ok\", output_ref={\"artifact_key\": \"resume_raw\"}, message=\"extracted\")\n",
    "        self._persist(st)\n",
    "\n",
    "        # L0 sanitize\n",
    "        st.start_step(\"l0_sanitize\", layer_id=\"L0\", tool_name=\"SanitizeAgent\", input_ref={})\n",
    "        safe = self._sanitize.sanitize_before_llm(\n",
    "            state=st, step_id=\"l0_sanitize\", tool_name=\"sanitize_before_llm\",\n",
    "            user_text=resume_text, context=\"resume\"\n",
    "        )\n",
    "        if safe is None:\n",
    "            LiveFeed.emit(st, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Prompt injection detected. Run blocked.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "        st.end_step(\"l0_sanitize\", status=\"ok\", output_ref={\"sanitized\": True}, message=\"pass\")\n",
    "        LiveFeed.emit(st, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Security check passed.\")\n",
    "        self._persist(st)\n",
    "\n",
    "        # L2/L3 parse recursive gate\n",
    "        extracted = self._parse_with_gate(st=st, safe_text=safe, run_dir=run_dir)\n",
    "        self._persist(st)\n",
    "        if st.status == \"needs_human_approval\":\n",
    "            self._notify(st, event=\"needs_human_approval\")\n",
    "            return\n",
    "\n",
    "        resume_skills = [str(s).lower() for s in (extracted.skills or []) if s]\n",
    "        ats = ats_score_from_text(safe)\n",
    "        st.meta[\"ats_score\"] = ats\n",
    "        LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=f\"Profile extracted. Skills={len(resume_skills)} ATS={ats:.2f}\")\n",
    "\n",
    "        if not self._s.serper_api_key:\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"missing_serper_key\"\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=\"Missing SERPER_API_KEY in .env.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        discovery = SerperDiscoveryService(api_key=self._s.serper_api_key, health=self._health)\n",
    "\n",
    "        # ----- L3-L5 discovery refinement loop -----\n",
    "        base_query = self._build_query(target_role=target_role, location=location, remote=remote, wfo_ok=wfo_ok, salary=salary, visa_required=visa_required, skills=resume_skills)\n",
    "        tbs = \"qdr:d\" if recency_hours <= 36 else None\n",
    "\n",
    "        all_ranked: List[Dict[str, Any]] = []\n",
    "        for attempt in range(max_refinements + 1):\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=f\"Hunt attempt {attempt+1}: searching 8 job boards…\")\n",
    "            st.start_step(f\"l3_discovery_{attempt+1}\", layer_id=\"L3\", tool_name=\"DiscoveryService\", input_ref={\"query\": base_query, \"tbs\": tbs})\n",
    "\n",
    "            results, board_counts = self._search_boards(st=st, discovery=discovery, base_query=base_query, tbs=tbs)\n",
    "            st.end_step(f\"l3_discovery_{attempt+1}\", status=\"ok\", output_ref={\"results\": len(results), \"boards\": board_counts}, message=\"discovered\")\n",
    "            self._persist(st)\n",
    "\n",
    "            if st.status in (\"blocked\", \"api_failure\"):\n",
    "                self._notify(st, event=\"quota_error\", extra={\"provider\": \"serper\"})\n",
    "                return\n",
    "\n",
    "            # L4 scrape + score\n",
    "            LiveFeed.emit(st, layer=\"L4\", agent=\"ScraperAgent\", message=f\"Scraping + normalizing up to {max_jobs} jobs…\")\n",
    "            ranked = self._scrape_and_score(\n",
    "                st=st,\n",
    "                results=results[:max_jobs],\n",
    "                extracted=extracted,\n",
    "                resume_text=safe,\n",
    "                resume_skills=resume_skills,\n",
    "                ats=ats,\n",
    "                recency_hours=recency_hours,\n",
    "                visa_required=visa_required,\n",
    "            )\n",
    "            all_ranked = ranked\n",
    "            _save_json(run_dir / \"ranking.json\", ranked)\n",
    "            st.add_artifact(\"ranking\", str(run_dir / \"ranking.json\"), content_type=\"application/json\")\n",
    "            _save_json(run_dir / \"search_summary.json\", {\"query\": base_query, \"tbs\": tbs, \"boards\": board_counts, \"raw_results\": len(results), \"kept\": len(ranked)})\n",
    "            st.add_artifact(\"search_summary\", str(run_dir / \"search_summary.json\"), content_type=\"application/json\")\n",
    "            self._persist(st)\n",
    "\n",
    "            if not ranked:\n",
    "                conf, fb = 0.0, [\"No jobs survived filters. Widen recency or disable visa-required filter.\"]\n",
    "            else:\n",
    "                top = float(ranked[0][\"interview_chance_score\"])\n",
    "                avg = sum(float(x[\"interview_chance_score\"]) for x in ranked[:20]) / max(1, min(20, len(ranked)))\n",
    "                conf, fb = self._disc_eval.evaluate(top_score=top, avg_score=avg, n=len(ranked), threshold=threshold)\n",
    "\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=f\"Discovery confidence={conf:.2f} (threshold={threshold:.2f}).\")\n",
    "            if fb:\n",
    "                LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=\" | \".join(fb[:2]))\n",
    "\n",
    "            # Pass -> HITL review ranking\n",
    "            if ranked and float(ranked[0][\"interview_chance_score\"]) >= threshold:\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"review_ranking\"\n",
    "                LiveFeed.emit(st, layer=\"L1\", agent=\"Dashboard\", message=\"Ranking ready for review (HITL).\")\n",
    "                self._persist(st)\n",
    "                self._notify(st, event=\"needs_human_approval\")\n",
    "                return\n",
    "\n",
    "            # Low confidence after final attempt -> HITL\n",
    "            if attempt >= max_refinements:\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"low_confidence_discovery\"\n",
    "                LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=\"Low confidence after retries. Needs human guidance.\")\n",
    "                self._persist(st)\n",
    "                self._notify(st, event=\"needs_human_approval\")\n",
    "                return\n",
    "\n",
    "            # refine query\n",
    "            base_query = self._refine_query(\n",
    "                st=st,\n",
    "                base_query=base_query,\n",
    "                ranked=ranked,\n",
    "                resume_skills=resume_skills,\n",
    "                target_role=target_role,\n",
    "                location=location,\n",
    "                visa_required=visa_required,\n",
    "            )\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=f\"Refining query → {base_query[:160]}\")\n",
    "            self._persist(st)\n",
    "\n",
    "    def _continue_after_hitl(self, run_id: str) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        st = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "\n",
    "        pending = st.meta.get(\"pending_action\")\n",
    "        action = (st.meta.get(\"last_user_action\") or {}).get(\"type\")\n",
    "        payload = (st.meta.get(\"last_user_action\") or {}).get(\"payload\") or {}\n",
    "\n",
    "        # Reject ranking => restart discovery with extra notes\n",
    "        if pending == \"review_ranking\" and action == \"reject_ranking\":\n",
    "            st.status = \"running\"\n",
    "            st.meta[\"pending_action\"] = None\n",
    "            st.meta.setdefault(\"user_refinement_notes\", [])\n",
    "            st.meta[\"user_refinement_notes\"].append(payload.get(\"reason\", \"User rejected ranking\"))\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=\"Ranking rejected. Re-running discovery with refined intent…\")\n",
    "            self._persist(st)\n",
    "\n",
    "            # Resume text + extracted should exist already; just rerun pipeline discovery section by restarting whole run is heavy.\n",
    "            # Simple approach: re-run full pipeline with the already uploaded resume.\n",
    "            resume_path = run_dir / \"resume_raw.txt\"\n",
    "            if not resume_path.exists():\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"resume_missing\"\n",
    "                self._persist(st)\n",
    "                return\n",
    "            resume_text = resume_path.read_text(encoding=\"utf-8\")\n",
    "            # Fake a file name; we only need text. We'll re-enter pipeline by calling _run_pipeline with same text via TXT bytes.\n",
    "            self._run_pipeline(run_id, \"resume.txt\", resume_text.encode(\"utf-8\"))\n",
    "            return\n",
    "\n",
    "        # Approve ranking => generate drafts (L6) then pause for draft review\n",
    "        if pending == \"review_ranking\" and action == \"approve_ranking\":\n",
    "            LiveFeed.emit(st, layer=\"L6\", agent=\"Generator\", message=\"Ranking approved. Generating strategy + cover letters…\")\n",
    "            st.status = \"running\"\n",
    "            st.meta[\"pending_action\"] = None\n",
    "            self._persist(st)\n",
    "\n",
    "            ranking_path = run_dir / \"ranking.json\"\n",
    "            if not ranking_path.exists():\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"ranking_missing\"\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            ranked = json.loads(ranking_path.read_text(encoding=\"utf-8\"))\n",
    "            top_n = int((st.meta.get(\"preferences\", {}) or {}).get(\"draft_count\", 10))\n",
    "            ranked = ranked[:top_n]\n",
    "\n",
    "            # Load extracted resume\n",
    "            ex_ref = st.artifacts.get(\"extracted_resume\")\n",
    "            if not ex_ref or not Path(ex_ref.path).exists():\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"extracted_resume_missing\"\n",
    "                self._persist(st)\n",
    "                return\n",
    "            extracted = ExtractedResume(**json.loads(Path(ex_ref.path).read_text(encoding=\"utf-8\")))\n",
    "\n",
    "            drafts_bundle = self._generate_drafts(st=st, extracted=extracted, ranked=ranked, run_dir=run_dir)\n",
    "            _save_json(run_dir / \"drafts.json\", drafts_bundle)\n",
    "            st.add_artifact(\"drafts_bundle\", str(run_dir / \"drafts.json\"), content_type=\"application/json\")\n",
    "\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"review_drafts\"\n",
    "            LiveFeed.emit(st, layer=\"L1\", agent=\"Dashboard\", message=\"Drafts ready for review (HITL).\")\n",
    "            self._persist(st)\n",
    "            self._notify(st, event=\"needs_human_approval\")\n",
    "            return\n",
    "\n",
    "        # Approve drafts => apply + XAI + dossier => completed\n",
    "        if pending == \"review_drafts\" and action == \"approve_drafts\":\n",
    "            LiveFeed.emit(st, layer=\"L7\", agent=\"ApplyExecutor\", message=\"Drafts approved. Submitting (simulated)…\")\n",
    "            st.status = \"running\"\n",
    "            st.meta[\"pending_action\"] = None\n",
    "            self._persist(st)\n",
    "\n",
    "            ok = self._apply_and_finalize(st=st, run_dir=run_dir)\n",
    "            if ok:\n",
    "                st.status = \"completed\"\n",
    "                LiveFeed.emit(st, layer=\"L9\", agent=\"Analytics\", message=\"Run completed. Dossier ready.\")\n",
    "                self._persist(st)\n",
    "                self._notify(st, event=\"completed\")\n",
    "            else:\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"apply_failed\"\n",
    "                self._persist(st)\n",
    "                self._notify(st, event=\"needs_human_approval\")\n",
    "            return\n",
    "\n",
    "        # Reject drafts => back to ranking review\n",
    "        if pending == \"review_drafts\" and action == \"reject_drafts\":\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"review_ranking\"\n",
    "            st.meta.setdefault(\"user_refinement_notes\", [])\n",
    "            st.meta[\"user_refinement_notes\"].append(payload.get(\"reason\", \"User rejected drafts\"))\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=\"Drafts rejected. Returning to ranking review.\")\n",
    "            self._persist(st)\n",
    "            self._notify(st, event=\"needs_human_approval\")\n",
    "            return\n",
    "\n",
    "    # ---------------- helpers ----------------\n",
    "    def _notify(self, st: OrchestrationState, *, event: str, extra: Optional[Dict[str, Any]] = None) -> None:\n",
    "        prefs = st.meta.get(\"preferences\", {}) or {}\n",
    "        to = prefs.get(\"user_phone\") or getattr(self._s, \"user_phone\", None)\n",
    "        if to:\n",
    "            self._notifier.notify(state=st, event=event, extra=extra, to_phone=str(to))\n",
    "\n",
    "    def _parse_with_gate(self, *, st: OrchestrationState, safe_text: str, run_dir: Path) -> ExtractedResume:\n",
    "        fb: List[str] = []\n",
    "        extracted: Optional[ExtractedResume] = None\n",
    "\n",
    "        for attempt in range(4):\n",
    "            sid = f\"l2_parse_{attempt+1}\"\n",
    "            st.start_step(sid, layer_id=\"L2\", tool_name=\"ParserAgentService\", input_ref={\"attempt\": attempt+1})\n",
    "            LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=f\"Parsing resume (attempt {attempt+1})…\")\n",
    "            extracted = self._parser.parse(raw_text=safe_text, orchestration_state=st, feedback=fb)\n",
    "            p = run_dir / f\"extracted_resume_attempt_{attempt+1}.json\"\n",
    "            _save_json(p, extracted.to_json_dict())\n",
    "            st.add_artifact(f\"extracted_resume_attempt_{attempt+1}\", str(p), content_type=\"application/json\")\n",
    "            st.end_step(sid, status=\"ok\", output_ref={\"artifact_key\": f\"extracted_resume_attempt_{attempt+1}\"}, message=\"parsed\")\n",
    "\n",
    "            ev = self._parser_eval.evaluate(\n",
    "                orchestration_state=st, raw_text=safe_text, extracted=extracted,\n",
    "                target_id=\"resume_main\", threshold=0.80, retry_count=attempt, max_retries=3\n",
    "            )\n",
    "            decision = st.apply_recursive_gate(target_id=\"resume_main\", layer_id=\"L3\")\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"ParserEvaluator\", message=f\"Parse quality={ev.evaluation_score:.2f} decision={decision}\")\n",
    "            self._persist(st)\n",
    "\n",
    "            if decision == \"pass\":\n",
    "                st.add_artifact(\"extracted_resume\", str(p), content_type=\"application/json\")\n",
    "                return extracted\n",
    "\n",
    "            if decision == \"human_approval\":\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"resume_cleanup\"\n",
    "                return extracted\n",
    "\n",
    "            fb = ev.feedback\n",
    "\n",
    "        st.status = \"needs_human_approval\"\n",
    "        st.meta[\"pending_action\"] = \"resume_cleanup\"\n",
    "        return extracted or ExtractedResume()\n",
    "\n",
    "    def _build_query(self, *, target_role: str, location: str, remote: bool, wfo_ok: bool, salary: str, visa_required: bool, skills: List[str]) -> str:\n",
    "        intent = []\n",
    "        if remote:\n",
    "            intent.append(\"remote\")\n",
    "        if wfo_ok:\n",
    "            intent.append(\"hybrid\")\n",
    "        if not intent:\n",
    "            intent.append(\"on-site\")\n",
    "        intent_str = \" \".join(intent)\n",
    "\n",
    "        skill_str = \" \".join(skills[:6])\n",
    "        salary_part = f'\"{salary}\"' if salary else \"\"\n",
    "        visa_part = '\"visa sponsorship\" OR h1b OR opt OR cpt' if visa_required else \"\"\n",
    "        # Keep it Google-friendly\n",
    "        return f'{target_role} {location} {intent_str} {salary_part} {skill_str} ({visa_part}) apply'\n",
    "\n",
    "    def _search_boards(self, *, st: OrchestrationState, discovery: SerperDiscoveryService, base_query: str, tbs: Optional[str]) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:\n",
    "        seen = set()\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        counts: Dict[str, int] = {}\n",
    "        for b in DEFAULT_JOB_BOARDS:\n",
    "            q = f'{base_query} site:{b.domain}'\n",
    "            step_id = f\"l3_serper_{b.domain.replace('/','_')}\"\n",
    "            st.start_step(step_id, layer_id=\"L3\", tool_name=\"serper.search\", input_ref={\"board\": b.name})\n",
    "            items = discovery.search(state=st, step_id=step_id, query=q, num=10, tbs=tbs)\n",
    "            st.end_step(step_id, status=\"ok\", output_ref={\"count\": len(items)}, message=b.name)\n",
    "            counts[b.name] = len(items)\n",
    "\n",
    "            for it in items:\n",
    "                link = it.get(\"link\") or \"\"\n",
    "                if not link or link in seen:\n",
    "                    continue\n",
    "                seen.add(link)\n",
    "                it[\"board\"] = b.name\n",
    "                out.append(it)\n",
    "\n",
    "        LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=f\"Found {len(out)} unique roles across boards.\")\n",
    "        return out, counts\n",
    "\n",
    "    def _scrape_and_score(\n",
    "        self,\n",
    "        *,\n",
    "        st: OrchestrationState,\n",
    "        results: List[Dict[str, Any]],\n",
    "        extracted: ExtractedResume,\n",
    "        resume_text: str,\n",
    "        resume_skills: List[str],\n",
    "        ats: float,\n",
    "        recency_hours: float,\n",
    "        visa_required: bool,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        ranked: List[Dict[str, Any]] = []\n",
    "\n",
    "        res_tokens = Counter(_tokenize(resume_text))\n",
    "        exp_text = \" \".join([str(x) for x in (extracted.experience or [])])[:5000]\n",
    "        exp_tokens = Counter(_tokenize(exp_text)) if exp_text else res_tokens\n",
    "\n",
    "        # decide per-job\n",
    "        for idx, it in enumerate(results):\n",
    "            url = it.get(\"link\") or \"\"\n",
    "            title = it.get(\"title\") or \"\"\n",
    "            snippet = it.get(\"snippet\") or \"\"\n",
    "            board = it.get(\"board\") or \"unknown\"\n",
    "\n",
    "            # recency filter\n",
    "            rh = parse_recency_hours(snippet)\n",
    "            if rh is not None and rh > recency_hours:\n",
    "                continue\n",
    "\n",
    "            step_id = f\"l4_scrape_{idx+1}\"\n",
    "            st.start_step(step_id, layer_id=\"L4\", tool_name=\"ScraperService\", input_ref={\"url\": url, \"board\": board})\n",
    "            job_text = ScraperService.fetch_text(url=url, snippet=snippet)\n",
    "            st.end_step(step_id, status=\"ok\", output_ref={\"chars\": len(job_text)}, message=\"scraped\")\n",
    "\n",
    "            # visa filter\n",
    "            v_ok = visa_ok(job_text)\n",
    "            if visa_required and not v_ok:\n",
    "                continue\n",
    "\n",
    "            job_skills = extract_job_skills(job_text, resume_skills)\n",
    "            if job_skills:\n",
    "                overlap = len(set(job_skills) & set(resume_skills)) / max(1, len(set(job_skills)))\n",
    "            else:\n",
    "                overlap = 0.0\n",
    "\n",
    "            job_tokens = Counter(_tokenize(job_text))\n",
    "            exp_align = cosine_sim(exp_tokens, job_tokens)\n",
    "\n",
    "            market = 1.0\n",
    "            # very rough market factor heuristic from snippet\n",
    "            low = snippet.lower()\n",
    "            if \"applicants\" in low:\n",
    "                m = re.search(r\"(\\d+)\\+?\\s*applicants\", low)\n",
    "                if m:\n",
    "                    n = int(m.group(1))\n",
    "                    market = 1.0 + min(1.5, n / 200.0)  # 200 applicants => 2.0\n",
    "\n",
    "            score = compute_interview_chance(skill_overlap=overlap, exp_align=exp_align, ats=ats, market=market)\n",
    "\n",
    "            # boost if visa-positive signals and visa required\n",
    "            if visa_required and visa_positive(job_text):\n",
    "                score = min(1.0, score + 0.05)\n",
    "\n",
    "            rationale = [\n",
    "                f\"SkillOverlap={overlap:.2f}\",\n",
    "                f\"ExperienceAlignment={exp_align:.2f}\",\n",
    "                f\"ATS={ats:.2f}\",\n",
    "                f\"MarketFactor={market:.2f}\",\n",
    "                (\"VisaOK\" if v_ok else \"NoSponsorship\"),\n",
    "            ]\n",
    "\n",
    "            ranked.append(\n",
    "                {\n",
    "                    \"rank\": 0,\n",
    "                    \"title\": title,\n",
    "                    \"company\": board,\n",
    "                    \"board\": board,\n",
    "                    \"url\": url,\n",
    "                    \"recency_hours\": rh,\n",
    "                    \"visa_ok\": v_ok,\n",
    "                    \"matched_skills\": list(set(job_skills) & set(resume_skills))[:12],\n",
    "                    \"missing_skills\": [s for s in job_skills if s not in resume_skills][:12],\n",
    "                    \"skill_overlap\": overlap,\n",
    "                    \"experience_alignment\": exp_align,\n",
    "                    \"ats_score\": ats,\n",
    "                    \"market_factor\": market,\n",
    "                    \"interview_chance_score\": score,\n",
    "                    \"overall_match_percent\": round(score * 100.0, 2),\n",
    "                    \"rationale\": rationale,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # meta for UI/XAI\n",
    "            jid = url or str(uuid4().hex)\n",
    "            st.meta.setdefault(\"job_scores\", {})\n",
    "            st.meta.setdefault(\"job_components\", {})\n",
    "            st.meta.setdefault(\"job_meta\", {})\n",
    "            st.meta[\"job_scores\"][jid] = float(score)\n",
    "            st.meta[\"job_components\"][jid] = {\n",
    "                \"skill_overlap\": float(overlap),\n",
    "                \"experience_alignment\": float(exp_align),\n",
    "                \"ats_score\": float(ats),\n",
    "                \"market_competition_factor\": float(market),\n",
    "            }\n",
    "            st.meta[\"job_meta\"][jid] = {\"role_title\": title, \"company\": board, \"url\": url, \"source\": board}\n",
    "\n",
    "        ranked.sort(key=lambda x: float(x[\"interview_chance_score\"]), reverse=True)\n",
    "        for i, r in enumerate(ranked, start=1):\n",
    "            r[\"rank\"] = i\n",
    "\n",
    "        LiveFeed.emit(st, layer=\"L4\", agent=\"MatcherAgent\", message=f\"Scored {len(ranked)} jobs. Top score={ranked[0]['overall_match_percent'] if ranked else 'n/a'}\")\n",
    "        return ranked\n",
    "\n",
    "    def _refine_query(\n",
    "        self,\n",
    "        *,\n",
    "        st: OrchestrationState,\n",
    "        base_query: str,\n",
    "        ranked: List[Dict[str, Any]],\n",
    "        resume_skills: List[str],\n",
    "        target_role: str,\n",
    "        location: str,\n",
    "        visa_required: bool,\n",
    "    ) -> str:\n",
    "        top = ranked[0] if ranked else {}\n",
    "        top_sk = (top.get(\"matched_skills\") or [])[:6]\n",
    "        skill_hint = \" \".join(list(dict.fromkeys([*top_sk, *resume_skills]))[:8])\n",
    "\n",
    "        # include user refinement notes if any\n",
    "        notes = st.meta.get(\"user_refinement_notes\", []) or []\n",
    "        notes_hint = \" \".join([str(n)[:30] for n in notes[-2:]]) if notes else \"\"\n",
    "\n",
    "        if self._ollama.available():\n",
    "            prompt = (\n",
    "                \"You refine job search queries.\\n\"\n",
    "                f\"Target role: {target_role}\\nLocation: {location}\\n\"\n",
    "                f\"Current query: {base_query}\\n\"\n",
    "                f\"Skill hint: {skill_hint}\\n\"\n",
    "                f\"Visa required: {visa_required}\\n\"\n",
    "                f\"User notes: {notes_hint}\\n\"\n",
    "                \"Return ONE concise improved query string only.\"\n",
    "            )\n",
    "            safe = self._sanitize.sanitize_before_llm(state=st, step_id=\"l0_query_refine\", tool_name=\"sanitize_before_llm\", user_text=prompt, context=\"chat\")\n",
    "            if safe:\n",
    "                out = self._ollama.generate(prompt=safe, timeout=35.0)\n",
    "                if out:\n",
    "                    return out.replace(\"\\n\", \" \")[:220]\n",
    "\n",
    "        visa_part = '(\"visa sponsorship\" OR h1b OR opt)' if visa_required else \"\"\n",
    "        return f\"{target_role} {location} {skill_hint} {visa_part} apply\"\n",
    "\n",
    "    def _generate_drafts(self, *, st: OrchestrationState, extracted: ExtractedResume, ranked: List[Dict[str, Any]], run_dir: Path) -> Dict[str, Any]:\n",
    "        drafts: List[Dict[str, Any]] = []\n",
    "        strategies: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Simple deterministic fallback if CoverLetterService missing\n",
    "        def fallback_cover(job_title: str, company: str) -> str:\n",
    "            email = getattr(getattr(extracted, \"contact\", None), \"email\", \"\") or \"\"\n",
    "            name = getattr(extracted, \"name\", \"Candidate\")\n",
    "            return f\"\"\"{name}\n",
    "{email}\n",
    "\n",
    "Dear Hiring Manager,\n",
    "\n",
    "I’m applying for the {job_title} role. I bring strong experience in Python, ML/GenAI delivery, and production MLOps. I focus on shipping reliable systems, not demos—clean APIs, reproducible pipelines, and measurable business impact.\n",
    "\n",
    "I’d welcome the chance to discuss how I can help {company} deliver practical AI outcomes.\n",
    "\n",
    "Sincerely,\n",
    "{name}\n",
    "\"\"\"\n",
    "\n",
    "        for j in ranked:\n",
    "            title = j.get(\"title\") or j.get(\"role_title\") or \"Role\"\n",
    "            company = j.get(\"company\") or j.get(\"board\") or \"Company\"\n",
    "            url = j.get(\"url\") or \"\"\n",
    "\n",
    "            # Strategy (best effort)\n",
    "            if self._strategy:\n",
    "                try:\n",
    "                    # If you have a full MatchReport in your repo, wire it here later.\n",
    "                    # For now: strategy is lightweight.\n",
    "                    strategies.append({\"job\": title, \"company\": company, \"action_items\": [\"Emphasize top matched skills\", \"Quantify impact\", \"Align summary to role\"]})\n",
    "                except Exception as e:\n",
    "                    strategies.append({\"job\": title, \"company\": company, \"error\": str(e)})\n",
    "            else:\n",
    "                strategies.append({\"job\": title, \"company\": company, \"action_items\": [\"Emphasize top matched skills\", \"Quantify impact\", \"Align summary to role\"]})\n",
    "\n",
    "            # Cover letter\n",
    "            body = \"\"\n",
    "            eval_score = None\n",
    "            guard_action = None\n",
    "            guard_issues: List[str] = []\n",
    "\n",
    "            if self._cover and self._cover_eval and JobDescription and self._matcher:\n",
    "                try:\n",
    "                    # Create a minimal JobDescription for generation\n",
    "                    jd = JobDescription(\n",
    "                        job_id=str(uuid4().hex),\n",
    "                        role_title=title,\n",
    "                        company=company,\n",
    "                        country_code=str((st.meta.get(\"preferences\", {}) or {}).get(\"country\", \"US\")),\n",
    "                        required_skills=[],\n",
    "                        preferred_skills=[],\n",
    "                        responsibilities=[],\n",
    "                        requirements_text=\"\",\n",
    "                        applicants_count=None,\n",
    "                        market_competition_factor=None,\n",
    "                    )\n",
    "                    # Minimal match report substitute: reuse job rationale fields for context\n",
    "                    # If your CoverLetterService expects MatchReport, it will still work if implemented that way in your repo.\n",
    "                    draft = self._cover.draft(resume=extracted, job=jd, match_report=None, orchestration_state=st, feedback=[\"Include contact header\"])  # type: ignore\n",
    "                    body = draft.body\n",
    "                    # evaluator twin (best effort)\n",
    "                    ev = self._cover_eval.evaluate(orchestration_state=st, resume=extracted, job=jd, match_report=None, draft=draft, target_id=f\"cover::{title}\", threshold=0.80, retry_count=0, max_retries=3)  # type: ignore\n",
    "                    eval_score = float(getattr(ev, \"evaluation_score\", 0.0))\n",
    "                except Exception:\n",
    "                    body = fallback_cover(title, company)\n",
    "            else:\n",
    "                body = fallback_cover(title, company)\n",
    "\n",
    "            # Output guard\n",
    "            if self._out_guard:\n",
    "                try:\n",
    "                    gr = self._out_guard.check_cover_letter(state=st, draft_text=body, resume=extracted, match_report=None, country_code=str((st.meta.get(\"preferences\", {}) or {}).get(\"country\", \"US\")))  # type: ignore\n",
    "                    guard_action = gr.action\n",
    "                    guard_issues = gr.issues[:3]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            p = run_dir / f\"cover_letter_{re.sub(r'[^a-zA-Z0-9]+','_',title)[:40]}.md\"\n",
    "            p.write_text(body, encoding=\"utf-8\")\n",
    "            st.add_artifact(p.stem, str(p), content_type=\"text/markdown\")\n",
    "\n",
    "            drafts.append({\n",
    "                \"title\": title,\n",
    "                \"company\": company,\n",
    "                \"url\": url,\n",
    "                \"cover_path\": str(p),\n",
    "                \"evaluator_score\": eval_score,\n",
    "                \"guard_action\": guard_action,\n",
    "                \"guard_issues\": guard_issues,\n",
    "            })\n",
    "\n",
    "        LiveFeed.emit(st, layer=\"L6\", agent=\"CoverLetterAgent\", message=f\"Generated {len(drafts)} cover letters.\")\n",
    "        return {\"drafts\": drafts, \"strategies\": strategies}\n",
    "\n",
    "    def _apply_and_finalize(self, *, st: OrchestrationState, run_dir: Path) -> bool:\n",
    "        # Apply simulation (if ApplyExecutorService exists)\n",
    "        if not self._apply:\n",
    "            LiveFeed.emit(st, layer=\"L7\", agent=\"ApplyExecutor\", message=\"Apply executor missing. Skipping apply.\")\n",
    "        else:\n",
    "            # Submit top 3 (configurable)\n",
    "            ranking_path = run_dir / \"ranking.json\"\n",
    "            if ranking_path.exists():\n",
    "                ranked = json.loads(ranking_path.read_text(encoding=\"utf-8\"))[:3]\n",
    "                subs: List[Dict[str, Any]] = []\n",
    "                for j in ranked:\n",
    "                    try:\n",
    "                        job_id = str(j.get(\"url\") or uuid4().hex)\n",
    "                        # pick any cover artifact key\n",
    "                        # (we stored markdown files with stems; for simplicity, apply uses placeholders here)\n",
    "                        sub = self._apply.submit(orchestration_state=st, job_id=job_id, resume_artifact_key=\"extracted_resume\", cover_letter_artifact_key=\"drafts_bundle\", notes=\"Simulated apply\")  # type: ignore\n",
    "                        subs.append(sub.model_dump())\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                _save_json(run_dir / \"submissions.json\", {\"submissions\": subs})\n",
    "                st.add_artifact(\"submissions\", str(run_dir / \"submissions.json\"), content_type=\"application/json\")\n",
    "\n",
    "        # XAI + dossier\n",
    "        if XAIService and CareerDossierExporter:\n",
    "            try:\n",
    "                xai = XAIService()\n",
    "                xai_paths = xai.write_outputs(state=st, require_reportlab=False)\n",
    "                st.add_artifact(\"xai_transparency_pdf\", xai_paths[\"pdf\"], content_type=\"application/pdf\")\n",
    "                st.add_artifact(\"transparency_matrix_json\", xai_paths[\"json\"], content_type=\"application/json\")\n",
    "\n",
    "                exporter = CareerDossierExporter()\n",
    "                bundle = exporter.bundle_reports(run_id=st.run_id, final_pdf_path=xai_paths[\"pdf\"])\n",
    "                st.add_artifact(\"career_dossier_zip\", bundle[\"zip\"], content_type=\"application/zip\")\n",
    "            except Exception as e:\n",
    "                LiveFeed.emit(st, layer=\"L9\", agent=\"Analytics\", message=f\"XAI/Dossier generation failed: {e}\")\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "ENGINE = OneClickAutomationEngine()\n",
    "'''\n",
    "backup_write(\"src/careeragent/orchestration/engine.py\", ENGINE_PY)\n",
    "\n",
    "INIT_PY = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "from .state import OrchestrationState  # noqa: F401\n",
    "from .engine import OneClickAutomationEngine, ENGINE  # noqa: F401\n",
    "'''\n",
    "backup_write(\"src/careeragent/orchestration/__init__.py\", INIT_PY)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Dashboard (add F1/OPT toggle + recency + max jobs)\n",
    "# ---------------------------\n",
    "DASHBOARD = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import streamlit as st\n",
    "\n",
    "REPO_ROOT = Path(__file__).resolve().parents[2]\n",
    "SRC = REPO_ROOT / \"src\"\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "DEFAULT_API = os.getenv(\"API_URL\", \"http://127.0.0.1:8000\")\n",
    "\n",
    "\n",
    "def _safe_json(resp: requests.Response) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return resp.json()\n",
    "    except Exception:\n",
    "        return {\"_raw\": resp.text[:1500], \"_status_code\": resp.status_code}\n",
    "\n",
    "\n",
    "def _api_get(api_base: str, path: str, timeout: int = 20) -> requests.Response:\n",
    "    return requests.get(f\"{api_base}{path}\", timeout=timeout)\n",
    "\n",
    "\n",
    "def _api_post(api_base: str, path: str, timeout: int = 60, **kwargs) -> requests.Response:\n",
    "    return requests.post(f\"{api_base}{path}\", timeout=timeout, **kwargs)\n",
    "\n",
    "\n",
    "def _exists(path: Optional[str]) -> bool:\n",
    "    return bool(path) and Path(path).exists()\n",
    "\n",
    "\n",
    "def _download(label: str, path: str, mime: str) -> None:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        st.warning(f\"Missing file: {path}\")\n",
    "        return\n",
    "    st.download_button(label, data=p.read_bytes(), file_name=p.name, mime=mime, use_container_width=True)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    st.set_page_config(page_title=\"CareerAgent-AI Mission Control\", layout=\"wide\")\n",
    "    st.title(\"CareerAgent-AI — Mission Control (One-Click Automation)\")\n",
    "    st.caption(\"Upload resume → autonomous ingestion + discovery + ranking → HITL approvals → drafts + dossier downloads\")\n",
    "\n",
    "    api_base = st.sidebar.text_input(\"API Base URL\", value=DEFAULT_API)\n",
    "\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Backend\")\n",
    "    try:\n",
    "        h = _api_get(api_base, \"/health\", timeout=3)\n",
    "        if h.status_code == 200:\n",
    "            st.sidebar.success(\"🟢 API Online\")\n",
    "        else:\n",
    "            st.sidebar.warning(f\"🟠 API issue ({h.status_code})\")\n",
    "    except Exception as e:\n",
    "        st.sidebar.error(\"🔴 API Offline\")\n",
    "        st.sidebar.caption(str(e))\n",
    "        st.stop()\n",
    "\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Resume Upload\")\n",
    "    resume_file = st.sidebar.file_uploader(\"Upload Resume (PDF/TXT/DOCX)\", type=[\"pdf\", \"txt\", \"docx\"])\n",
    "\n",
    "    st.sidebar.subheader(\"Preferences\")\n",
    "    target_role = st.sidebar.text_input(\"Target role\", value=\"Data Scientist\")\n",
    "    country = st.sidebar.text_input(\"Country\", value=\"US\")\n",
    "    location = st.sidebar.text_input(\"Location\", value=\"United States\")\n",
    "\n",
    "    remote = st.sidebar.checkbox(\"Remote preferred\", value=True)\n",
    "    wfo_ok = st.sidebar.checkbox(\"On-site/WFO acceptable\", value=True)\n",
    "\n",
    "    salary = st.sidebar.text_input(\"Salary target (optional)\", value=\"\")\n",
    "    visa_required = st.sidebar.checkbox(\"Visa sponsorship required (F1/OPT)\", value=False)\n",
    "\n",
    "    recency_hours = st.sidebar.slider(\"Only jobs posted within (hours)\", 12, 168, 36, 6)\n",
    "    max_jobs = st.sidebar.slider(\"Max jobs to score per run\", 10, 80, 40, 5)\n",
    "\n",
    "    user_phone = st.sidebar.text_input(\"Phone for SMS (optional)\", value=\"\")\n",
    "\n",
    "    st.sidebar.subheader(\"Autonomy Controls\")\n",
    "    discovery_threshold = st.sidebar.slider(\"Discovery confidence threshold\", 0.50, 0.90, 0.70, 0.05)\n",
    "    max_refinements = st.sidebar.slider(\"Max query refinements\", 1, 6, 3, 1)\n",
    "    draft_count = st.sidebar.slider(\"Draft count (top jobs)\", 3, 25, 10, 1)\n",
    "\n",
    "    run_btn = st.sidebar.button(\"🚀 RUN ONE-CLICK\", type=\"primary\", use_container_width=True, disabled=(resume_file is None))\n",
    "\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Existing Run\")\n",
    "    run_id_in = st.sidebar.text_input(\"Run ID\", value=st.session_state.get(\"run_id\", \"\"))\n",
    "\n",
    "    if run_btn:\n",
    "        prefs = {\n",
    "            \"target_role\": target_role.strip() or \"Data Scientist\",\n",
    "            \"country\": country.strip() or \"US\",\n",
    "            \"location\": location.strip() or \"United States\",\n",
    "            \"remote\": bool(remote),\n",
    "            \"wfo_ok\": bool(wfo_ok),\n",
    "            \"salary\": salary.strip(),\n",
    "            \"visa_sponsorship_required\": bool(visa_required),\n",
    "            \"recency_hours\": float(recency_hours),\n",
    "            \"max_jobs\": int(max_jobs),\n",
    "            \"discovery_threshold\": float(discovery_threshold),\n",
    "            \"max_refinements\": int(max_refinements),\n",
    "            \"draft_count\": int(draft_count),\n",
    "            \"user_phone\": user_phone.strip() or None,\n",
    "        }\n",
    "\n",
    "        files = {\"resume\": (resume_file.name, resume_file.getvalue())}\n",
    "        data = {\"preferences_json\": json.dumps(prefs)}\n",
    "        r = _api_post(api_base, \"/analyze\", files=files, data=data, timeout=180)\n",
    "\n",
    "        if r.status_code >= 400:\n",
    "            st.error(f\"/analyze failed: {r.status_code}\\n\\n{r.text[:1500]}\")\n",
    "            st.stop()\n",
    "\n",
    "        out = _safe_json(r)\n",
    "        st.session_state[\"run_id\"] = out[\"run_id\"]\n",
    "        st.success(f\"Run started: {out['run_id']} (status: {out.get('status')})\")\n",
    "\n",
    "    run_id = st.session_state.get(\"run_id\") or run_id_in.strip()\n",
    "    if not run_id:\n",
    "        st.info(\"Upload a resume and click RUN, or paste an existing run_id.\")\n",
    "        return\n",
    "\n",
    "    colA, colB, colC = st.columns([1, 1, 1])\n",
    "    _ = st.button(\"🔄 Refresh\", use_container_width=True)\n",
    "\n",
    "    r = _api_get(api_base, f\"/status/{run_id}\", timeout=25)\n",
    "    if r.status_code != 200:\n",
    "        st.warning(f\"Run not found yet ({r.status_code}).\")\n",
    "        return\n",
    "\n",
    "    state = _safe_json(r)\n",
    "    status = state.get(\"status\", \"unknown\")\n",
    "    meta = state.get(\"meta\", {}) or {}\n",
    "    feed = meta.get(\"live_feed\", []) or []\n",
    "    steps = state.get(\"steps\", []) or []\n",
    "    artifacts = state.get(\"artifacts\", {}) or {}\n",
    "    pending = meta.get(\"pending_action\")\n",
    "\n",
    "    with colA:\n",
    "        st.metric(\"Run ID\", run_id)\n",
    "    with colB:\n",
    "        st.metric(\"Status\", status)\n",
    "    with colC:\n",
    "        st.metric(\"Pending\", str(pending))\n",
    "\n",
    "    total = max(1, len(steps))\n",
    "    done = sum(1 for s in steps if s.get(\"finished_at_utc\"))\n",
    "    st.progress(done / total)\n",
    "\n",
    "    left, right = st.columns([1.25, 0.75], vertical_alignment=\"top\")\n",
    "\n",
    "    with left:\n",
    "        st.markdown(\"### Live Agent Feed\")\n",
    "        if not feed:\n",
    "            st.info(\"No live feed yet.\")\n",
    "        else:\n",
    "            for ev in feed[-160:]:\n",
    "                st.write(f\"**[{ev.get('layer')} {ev.get('agent')}]** {ev.get('message')}\")\n",
    "\n",
    "        with st.expander(\"Audit Trail (Steps)\"):\n",
    "            for s in steps:\n",
    "                st.write(f\"- [{s.get('layer_id')}] {s.get('tool_name')} | {s.get('status')} | {s.get('step_id')}\")\n",
    "\n",
    "    with right:\n",
    "        st.markdown(\"### Key Scores\")\n",
    "        # Show top score if ranking exists\n",
    "        ranking_ref = artifacts.get(\"ranking\")\n",
    "        if ranking_ref and _exists(ranking_ref.get(\"path\")):\n",
    "            ranking = json.loads(Path(ranking_ref[\"path\"]).read_text(encoding=\"utf-8\"))\n",
    "            if ranking:\n",
    "                st.metric(\"Top Match\", f\"{ranking[0]['overall_match_percent']}%\")\n",
    "        else:\n",
    "            st.caption(\"Scores not available yet.\")\n",
    "\n",
    "        st.markdown(\"### Downloads\")\n",
    "        zip_ref = artifacts.get(\"career_dossier_zip\")\n",
    "        pdf_ref = artifacts.get(\"xai_transparency_pdf\")\n",
    "        if zip_ref and _exists(zip_ref.get(\"path\")):\n",
    "            _download(\"Download Career Dossier (ZIP)\", zip_ref[\"path\"], \"application/zip\")\n",
    "        else:\n",
    "            st.caption(\"Dossier ZIP not ready.\")\n",
    "        if pdf_ref and _exists(pdf_ref.get(\"path\")):\n",
    "            _download(\"Download XAI Transparency (PDF)\", pdf_ref[\"path\"], \"application/pdf\")\n",
    "        else:\n",
    "            st.caption(\"XAI PDF not ready.\")\n",
    "\n",
    "    st.markdown(\"### Ranking\")\n",
    "    ranking_ref = artifacts.get(\"ranking\")\n",
    "    if ranking_ref and _exists(ranking_ref.get(\"path\")):\n",
    "        ranking = json.loads(Path(ranking_ref[\"path\"]).read_text(encoding=\"utf-8\"))\n",
    "        st.dataframe(\n",
    "            [{\n",
    "                \"rank\": x.get(\"rank\"),\n",
    "                \"score_%\": x.get(\"overall_match_percent\"),\n",
    "                \"title\": x.get(\"title\"),\n",
    "                \"board\": x.get(\"board\"),\n",
    "                \"recency_h\": x.get(\"recency_hours\"),\n",
    "                \"visa_ok\": x.get(\"visa_ok\"),\n",
    "                \"url\": x.get(\"url\"),\n",
    "                \"missing_skills\": \", \".join((x.get(\"missing_skills\") or [])[:6]),\n",
    "            } for x in ranking],\n",
    "            use_container_width=True\n",
    "        )\n",
    "    else:\n",
    "        st.caption(\"Ranking not available yet.\")\n",
    "\n",
    "    st.markdown(\"### Human-in-the-Loop Controls\")\n",
    "    if status == \"needs_human_approval\" and pending == \"review_ranking\":\n",
    "        c1, c2 = st.columns(2)\n",
    "        with c1:\n",
    "            if st.button(\"✅ Approve Ranking → Generate Drafts\", type=\"primary\", use_container_width=True):\n",
    "                _api_post(api_base, f\"/action/{run_id}\", json={\"action_type\": \"approve_ranking\", \"payload\": {}}, timeout=30)\n",
    "                st.success(\"Approved. Draft generation started.\")\n",
    "        with c2:\n",
    "            reason = st.text_input(\"Reason to refine ranking\", value=\"\")\n",
    "            if st.button(\"❌ Reject Ranking → Re-run Discovery\", use_container_width=True):\n",
    "                _api_post(api_base, f\"/action/{run_id}\", json={\"action_type\": \"reject_ranking\", \"payload\": {\"reason\": reason}}, timeout=30)\n",
    "                st.warning(\"Rejected. Discovery refinement started.\")\n",
    "\n",
    "    if status == \"needs_human_approval\" and pending == \"review_drafts\":\n",
    "        c1, c2 = st.columns(2)\n",
    "        with c1:\n",
    "            if st.button(\"✅ Approve Drafts → Submit (Simulated)\", type=\"primary\", use_container_width=True):\n",
    "                _api_post(api_base, f\"/action/{run_id}\", json={\"action_type\": \"approve_drafts\", \"payload\": {}}, timeout=30)\n",
    "                st.success(\"Approved. Submission started.\")\n",
    "        with c2:\n",
    "            reason = st.text_input(\"Reason to revise drafts\", value=\"\")\n",
    "            if st.button(\"❌ Reject Drafts → Back to Ranking\", use_container_width=True):\n",
    "                _api_post(api_base, f\"/action/{run_id}\", json={\"action_type\": \"reject_drafts\", \"payload\": {\"reason\": reason}}, timeout=30)\n",
    "                st.warning(\"Rejected drafts. Returning to ranking review.\")\n",
    "\n",
    "    st.markdown(\"### Drafts Bundle (preview)\")\n",
    "    drafts_ref = artifacts.get(\"drafts_bundle\")\n",
    "    if drafts_ref and _exists(drafts_ref.get(\"path\")):\n",
    "        bundle = json.loads(Path(drafts_ref[\"path\"]).read_text(encoding=\"utf-8\"))\n",
    "        st.json(bundle)\n",
    "    else:\n",
    "        st.caption(\"Draft bundle not available yet.\")\n",
    "\n",
    "    with st.expander(\"Full State JSON\"):\n",
    "        st.json(state)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "backup_write(\"app/ui/dashboard.py\", DASHBOARD)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) app/main.py (robust)\n",
    "# ---------------------------\n",
    "APP_MAIN = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "from app.ui.dashboard import main\n",
    "main()\n",
    "'''\n",
    "backup_write(\"app/main.py\", APP_MAIN)\n",
    "\n",
    "print(\"\\n✅ FULL AUTONOMY ENGINE + UPDATED DASHBOARD WRITTEN.\")\n",
    "print(\"Next:\")\n",
    "print(\"1) Restart backend\")\n",
    "print(\"   PYTHONPATH=src uv run python -m uvicorn careeragent.api.main:app --host 127.0.0.1 --port 8000 --reload\")\n",
    "print(\"2) Restart frontend\")\n",
    "print(\"   API_URL=http://127.0.0.1:8000 PYTHONPATH='.:src' uv run streamlit run app/main.py --server.port 8501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665656f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70c8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d7e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcd36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_agent_service.py.bak_20260221_011632\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_agent_service.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_evaluator_service.py.bak_20260221_011632\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_evaluator_service.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py.bak_20260221_011632\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py.bak_20260221_011632\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py.bak_20260221_011632\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py.bak_20260221_011632\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py\n",
      "\n",
      "✅ PATCH COMPLETE.\n",
      "Restart backend + frontend now.\n",
      "\n",
      "Backend:\n",
      "  PYTHONPATH=src uv run python -m uvicorn careeragent.api.main:app --host 127.0.0.1 --port 8000 --reload\n",
      "Frontend:\n",
      "  API_URL=http://127.0.0.1:8000 PYTHONPATH='.:src' uv run streamlit run app/main.py --server.port 8501\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- auto-chdir to repo root even if notebook runs from notebooks_v2 ---\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name == \"notebooks_v2\":\n",
    "    os.chdir(ROOT.parent)\n",
    "ROOT = Path.cwd().resolve()\n",
    "assert (ROOT / \"src\").exists(), f\"Run from repo root. CWD={ROOT}\"\n",
    "print(\"✅ CWD =\", ROOT)\n",
    "\n",
    "def backup_write(rel_path: str, content: str) -> None:\n",
    "    p = ROOT / rel_path\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts}\")\n",
    "        bak.write_text(p.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(\"BACKUP:\", bak)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    print(\"WROTE:\", p)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Strong deterministic parser (CareerOS-style)\n",
    "# ---------------------------\n",
    "PARSER_AGENT = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Description: Contact info extracted from resume text.\n",
    "    Layer: L2\n",
    "    Input: raw resume text\n",
    "    Output: structured contact object\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    email: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "    linkedin: Optional[str] = None\n",
    "    github: Optional[str] = None\n",
    "    location: Optional[str] = None\n",
    "\n",
    "\n",
    "class ExperienceItem(BaseModel):\n",
    "    \"\"\"\n",
    "    Description: Experience item extracted from resume text.\n",
    "    Layer: L2\n",
    "    Input: raw resume text\n",
    "    Output: structured experience item\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    title: Optional[str] = None\n",
    "    company: Optional[str] = None\n",
    "    start: Optional[str] = None\n",
    "    end: Optional[str] = None\n",
    "    bullets: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EducationItem(BaseModel):\n",
    "    \"\"\"\n",
    "    Description: Education item extracted from resume text.\n",
    "    Layer: L2\n",
    "    Input: raw resume text\n",
    "    Output: structured education item\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    degree: Optional[str] = None\n",
    "    school: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "\n",
    "\n",
    "COMMON_SKILLS = [\n",
    "    \"python\",\"sql\",\"fastapi\",\"docker\",\"kubernetes\",\"mlflow\",\"dvc\",\n",
    "    \"aws\",\"azure\",\"gcp\",\"sagemaker\",\"azure ml\",\"databricks\",\"spark\",\n",
    "    \"pytorch\",\"tensorflow\",\"scikit-learn\",\"pandas\",\"numpy\",\n",
    "    \"langchain\",\"langgraph\",\"rag\",\"vector\",\"faiss\",\"chroma\",\n",
    "    \"terraform\",\"github actions\",\"ci/cd\",\"kafka\",\"airflow\",\"snowflake\"\n",
    "]\n",
    "\n",
    "\n",
    "def _find_email(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text or \"\")\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "\n",
    "def _find_phone(text: str) -> Optional[str]:\n",
    "    # handles +1 (xxx) xxx-xxxx, xxx-xxx-xxxx, etc\n",
    "    m = re.search(r\"(\\+?\\d{1,3}[\\s\\-]?)?(\\(?\\d{3}\\)?[\\s\\-]?)\\d{3}[\\s\\-]?\\d{4}\", text or \"\")\n",
    "    return m.group(0).strip() if m else None\n",
    "\n",
    "\n",
    "def _find_url(text: str, keyword: str) -> Optional[str]:\n",
    "    # capture URLs containing linkedin/github\n",
    "    pat = rf\"(https?://[^\\s]+{keyword}[^\\s]*)\"\n",
    "    m = re.search(pat, text or \"\", flags=re.IGNORECASE)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "\n",
    "def _split_sections(text: str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Description: Split text into sections using heading keywords.\n",
    "    Layer: L2\n",
    "    Input: raw text\n",
    "    Output: dict section->content\n",
    "    \"\"\"\n",
    "    t = text or \"\"\n",
    "    # Normalize headings\n",
    "    headings = [\"summary\", \"skills\", \"experience\", \"education\", \"projects\", \"certifications\"]\n",
    "    idxs = []\n",
    "    low = t.lower()\n",
    "    for h in headings:\n",
    "        m = re.search(rf\"(^|\\n)\\s*{h}\\s*[:\\-]?\\s*\", low)\n",
    "        if m:\n",
    "            idxs.append((m.start(), h))\n",
    "    if not idxs:\n",
    "        return {\"full\": t}\n",
    "\n",
    "    idxs.sort(key=lambda x: x[0])\n",
    "    out: dict[str, str] = {}\n",
    "    for i, (pos, h) in enumerate(idxs):\n",
    "        end = idxs[i + 1][0] if i + 1 < len(idxs) else len(t)\n",
    "        out[h] = t[pos:end].strip()\n",
    "    out[\"full\"] = t\n",
    "    return out\n",
    "\n",
    "\n",
    "def _extract_skills(text: str) -> List[str]:\n",
    "    low = (text or \"\").lower()\n",
    "    found = []\n",
    "    # 1) explicit skills section comma split\n",
    "    m = re.search(r\"skills\\s*[:\\-]\\s*(.+)\", text or \"\", flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        chunk = m.group(1)\n",
    "        parts = re.split(r\"[,\\|/•\\n]+\", chunk)\n",
    "        found.extend([p.strip().lower() for p in parts if p.strip()])\n",
    "    # 2) dictionary scan\n",
    "    for s in COMMON_SKILLS:\n",
    "        if s in low:\n",
    "            found.append(s)\n",
    "    # cleanup\n",
    "    found = [re.sub(r\"\\s+\", \" \", x) for x in found]\n",
    "    return sorted(list(dict.fromkeys([x for x in found if 2 <= len(x) <= 40])))[:50]\n",
    "\n",
    "\n",
    "def _extract_name(text: str) -> Optional[str]:\n",
    "    # first non-empty line that isn't a heading\n",
    "    lines = [ln.strip() for ln in (text or \"\").splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return None\n",
    "    cand = lines[0]\n",
    "    if cand.lower().startswith((\"summary\", \"skills\", \"experience\", \"education\")):\n",
    "        return None\n",
    "    # avoid emails/phones\n",
    "    if \"@\" in cand or re.search(r\"\\d{3}[\\s\\-]?\\d{3}\", cand):\n",
    "        return None\n",
    "    return cand[:60]\n",
    "\n",
    "\n",
    "class ExtractedResume(BaseModel):\n",
    "    \"\"\"\n",
    "    Description: Core parsed resume model for downstream layers (L3-L9).\n",
    "    Layer: L2\n",
    "    Input: raw resume text\n",
    "    Output: structured resume JSON\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    name: Optional[str] = None\n",
    "    contact: ContactInfo = Field(default_factory=ContactInfo)\n",
    "    summary: Optional[str] = None\n",
    "    skills: List[str] = Field(default_factory=list)\n",
    "    experience: List[ExperienceItem] = Field(default_factory=list)\n",
    "    education: List[EducationItem] = Field(default_factory=list)\n",
    "    raw_text: Optional[str] = None\n",
    "    warnings: List[str] = Field(default_factory=list)\n",
    "\n",
    "    def to_json_dict(self) -> dict[str, Any]:\n",
    "        return self.model_dump()\n",
    "\n",
    "\n",
    "class ParserAgentService:\n",
    "    \"\"\"\n",
    "    Description: Convert raw resume string into ExtractedResume JSON (deterministic, ATS-friendly extraction).\n",
    "    Layer: L2\n",
    "    Input: raw resume string\n",
    "    Output: ExtractedResume\n",
    "    \"\"\"\n",
    "    def parse(self, *, raw_text: str, orchestration_state: Any, feedback: List[str] | None = None) -> ExtractedResume:\n",
    "        text = raw_text or \"\"\n",
    "        sections = _split_sections(text)\n",
    "\n",
    "        name = _extract_name(text)\n",
    "        email = _find_email(text)\n",
    "        phone = _find_phone(text)\n",
    "        linkedin = _find_url(text, \"linkedin\")\n",
    "        github = _find_url(text, \"github\")\n",
    "\n",
    "        skills = _extract_skills(sections.get(\"skills\") or text)\n",
    "\n",
    "        summary = None\n",
    "        if \"summary\" in sections:\n",
    "            summary = re.sub(r\"(?i)^summary\\s*[:\\-]?\\s*\", \"\", sections[\"summary\"]).strip()\n",
    "        else:\n",
    "            # fallback: first 2-3 lines after name\n",
    "            lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "            if len(lines) >= 2:\n",
    "                summary = \" \".join(lines[1:3])[:400]\n",
    "\n",
    "        # Experience: keep it simple but structured\n",
    "        exp_items: List[ExperienceItem] = []\n",
    "        exp_text = sections.get(\"experience\")\n",
    "        if exp_text:\n",
    "            body = re.sub(r\"(?i)^experience\\s*[:\\-]?\\s*\", \"\", exp_text).strip()\n",
    "            bullets = [b.strip(\"-• \").strip() for b in re.split(r\"\\n+\", body) if b.strip()]\n",
    "            bullets = [b for b in bullets if len(b) > 15][:10]\n",
    "            if bullets:\n",
    "                exp_items.append(ExperienceItem(bullets=bullets))\n",
    "\n",
    "        edu_items: List[EducationItem] = []\n",
    "        edu_text = sections.get(\"education\")\n",
    "        if edu_text:\n",
    "            body = re.sub(r\"(?i)^education\\s*[:\\-]?\\s*\", \"\", edu_text).strip()\n",
    "            # naive degree/school capture\n",
    "            edu_items.append(EducationItem(degree=body[:120]))\n",
    "\n",
    "        warnings: List[str] = []\n",
    "        if not email and not phone:\n",
    "            warnings.append(\"Missing email/phone in resume text.\")\n",
    "        if len(skills) < 5:\n",
    "            warnings.append(\"Low skill extraction count; consider adding a clearer Skills section.\")\n",
    "        if not exp_items:\n",
    "            warnings.append(\"Experience section not clearly detected.\")\n",
    "\n",
    "        return ExtractedResume(\n",
    "            name=name,\n",
    "            contact=ContactInfo(email=email, phone=phone, linkedin=linkedin, github=github),\n",
    "            summary=summary,\n",
    "            skills=skills,\n",
    "            experience=exp_items,\n",
    "            education=edu_items,\n",
    "            raw_text=text,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "'''\n",
    "backup_write(\"src/careeragent/agents/parser_agent_service.py\", PARSER_AGENT)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Parser evaluator becomes soft-gate (auto-continue unless truly broken)\n",
    "# ---------------------------\n",
    "PARSER_EVAL = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "from careeragent.agents.parser_agent_service import ExtractedResume\n",
    "\n",
    "\n",
    "class EvaluationEvent(BaseModel):\n",
    "    \"\"\"\n",
    "    Description: Evaluation output for recursive gate decisions.\n",
    "    Layer: L3\n",
    "    Input: generator output\n",
    "    Output: score + feedback + gate decision support\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    evaluation_score: float = 0.0\n",
    "    threshold: float = 0.0\n",
    "    feedback: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "def _ats_structure_score(text: str) -> float:\n",
    "    t = (text or \"\").lower()\n",
    "    score = 0.0\n",
    "    if \"summary\" in t: score += 0.15\n",
    "    if \"skills\" in t: score += 0.20\n",
    "    if \"experience\" in t: score += 0.25\n",
    "    if \"education\" in t: score += 0.10\n",
    "    if \"-\" in text or \"•\" in text: score += 0.15\n",
    "    if len(text) > 1200: score += 0.15\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "class ParserEvaluatorService:\n",
    "    \"\"\"\n",
    "    Description: Critique Parser output for completeness + ATS readiness (soft gate).\n",
    "    Layer: L3\n",
    "    Input: raw_text + ExtractedResume\n",
    "    Output: EvaluationEvent (score + feedback)\n",
    "    \"\"\"\n",
    "    def evaluate(\n",
    "        self,\n",
    "        *,\n",
    "        orchestration_state: Any,\n",
    "        raw_text: str,\n",
    "        extracted: ExtractedResume,\n",
    "        target_id: str,\n",
    "        threshold: float = 0.55,\n",
    "        retry_count: int = 0,\n",
    "        max_retries: int = 3,\n",
    "    ) -> Any:\n",
    "        feedback: List[str] = []\n",
    "\n",
    "        contact = extracted.contact or None\n",
    "        has_contact = bool(getattr(contact, \"email\", None) or getattr(contact, \"phone\", None))\n",
    "        has_skills = len(extracted.skills or []) >= 6\n",
    "        has_exp = bool(extracted.experience and len(extracted.experience) > 0)\n",
    "        ats = _ats_structure_score(raw_text)\n",
    "\n",
    "        # weights (so missing contact doesn't kill automation)\n",
    "        w_contact = 0.15\n",
    "        w_skills = 0.35\n",
    "        w_exp = 0.35\n",
    "        w_ats = 0.15\n",
    "\n",
    "        s_contact = 1.0 if has_contact else 0.0\n",
    "        s_skills = min(1.0, len(extracted.skills or []) / 12.0)\n",
    "        s_exp = 1.0 if has_exp else 0.0\n",
    "        s_ats = ats\n",
    "\n",
    "        score = (w_contact*s_contact) + (w_skills*s_skills) + (w_exp*s_exp) + (w_ats*s_ats)\n",
    "        score = max(0.0, min(1.0, float(score)))\n",
    "\n",
    "        if not has_contact:\n",
    "            feedback.append(\"Contact info missing (email/phone). Optional: paste improved resume to continue with stronger ATS fit.\")\n",
    "        if not has_exp:\n",
    "            feedback.append(\"Experience section unclear. Add an Experience heading with bullet points.\")\n",
    "        if len(extracted.skills or []) < 6:\n",
    "            feedback.append(\"Skills extraction low. Add a clearer Skills section with comma-separated skills.\")\n",
    "        if ats < 0.5:\n",
    "            feedback.append(\"ATS structure weak. Add headings: Summary, Skills, Experience, Education; use bullets.\")\n",
    "\n",
    "        # Record in orchestration state (your OrchestrationState already has record_evaluation)\n",
    "        ev = orchestration_state.record_evaluation(\n",
    "            layer_id=\"L3\",\n",
    "            target_id=target_id,\n",
    "            generator_agent=\"parser_agent_service\",\n",
    "            evaluator_agent=\"parser_evaluator_service\",\n",
    "            evaluation_score=score,\n",
    "            threshold=threshold,\n",
    "            feedback=feedback,\n",
    "            retry_count=retry_count,\n",
    "            max_retries=max_retries,\n",
    "            interview_chance={\n",
    "                \"weights\": {\"w1_skill_overlap\": 0.45, \"w2_experience_alignment\": 0.35, \"w3_ats_score\": 0.20},\n",
    "                \"components\": {\"skill_overlap\": 0.0, \"experience_alignment\": 0.0, \"ats_score\": float(ats), \"market_competition_factor\": 1.0},\n",
    "                \"interview_chance_score\": float((0.20*ats)),  # parser-stage proxy only\n",
    "            },\n",
    "        )\n",
    "        return ev\n",
    "'''\n",
    "backup_write(\"src/careeragent/agents/parser_evaluator_service.py\", PARSER_EVAL)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Engine patch: soft-gate resume_cleanup + add HITL resume submit action\n",
    "#    (We keep your current full autonomy engine file name and overwrite it)\n",
    "# ---------------------------\n",
    "ENGINE_PATCH = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from uuid import uuid4\n",
    "\n",
    "import httpx\n",
    "\n",
    "from careeragent.config import artifacts_root, get_settings\n",
    "from careeragent.orchestration.state import OrchestrationState\n",
    "from careeragent.services.db_service import SqliteStateStore\n",
    "from careeragent.services.notification_service import NotificationService\n",
    "\n",
    "# HealthService is optional; if missing, we degrade gracefully\n",
    "try:\n",
    "    from careeragent.services.health_service import HealthService  # type: ignore\n",
    "except Exception:\n",
    "    HealthService = None  # type: ignore\n",
    "\n",
    "from careeragent.agents.security_agent import SanitizeAgent\n",
    "from careeragent.agents.parser_agent_service import ParserAgentService, ExtractedResume\n",
    "from careeragent.agents.parser_evaluator_service import ParserEvaluatorService\n",
    "\n",
    "# --------------------------- helpers ---------------------------\n",
    "def _run_dir(run_id: str) -> Path:\n",
    "    d = artifacts_root() / \"runs\" / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def _save_json(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "class LiveFeed:\n",
    "    \"\"\"\n",
    "    Description: Live Agent Feed logger.\n",
    "    Layer: L1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def emit(st: OrchestrationState, *, layer: str, agent: str, message: str) -> None:\n",
    "        st.meta.setdefault(\"live_feed\", [])\n",
    "        st.meta[\"live_feed\"].append({\"layer\": layer, \"agent\": agent, \"message\": message})\n",
    "        st.touch()\n",
    "\n",
    "\n",
    "class LocalResumeExtractor:\n",
    "    \"\"\"\n",
    "    Description: Extract resume text from PDF/TXT/DOCX.\n",
    "    Layer: L2\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def extract_text(*, filename: str, data: bytes) -> str:\n",
    "        name = (filename or \"\").lower()\n",
    "        if name.endswith(\".txt\"):\n",
    "            return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "        if name.endswith(\".pdf\"):\n",
    "            from pypdf import PdfReader  # type: ignore\n",
    "            import io\n",
    "            reader = PdfReader(io.BytesIO(data))\n",
    "            return \"\\n\".join([(pg.extract_text() or \"\") for pg in reader.pages])\n",
    "\n",
    "        if name.endswith(\".docx\"):\n",
    "            import docx  # type: ignore\n",
    "            import io\n",
    "            d = docx.Document(io.BytesIO(data))\n",
    "            return \"\\n\".join([p.text for p in d.paragraphs if p.text])\n",
    "\n",
    "        return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "# -------- Job discovery (Serper across 8 boards) --------\n",
    "JOB_BOARDS = [\n",
    "    (\"LinkedIn Jobs\", \"linkedin.com/jobs\"),\n",
    "    (\"Indeed\", \"indeed.com\"),\n",
    "    (\"Glassdoor\", \"glassdoor.com\"),\n",
    "    (\"ZipRecruiter\", \"ziprecruiter.com\"),\n",
    "    (\"Monster\", \"monster.com\"),\n",
    "    (\"Dice\", \"dice.com\"),\n",
    "    (\"Lever\", \"jobs.lever.co\"),\n",
    "    (\"Greenhouse\", \"boards.greenhouse.io\"),\n",
    "]\n",
    "\n",
    "VISA_NEGATIVE = (\"unable to sponsor\",\"cannot sponsor\",\"no sponsorship\",\"do not sponsor\",\"not sponsor\",\"without sponsorship\",\"no visa\")\n",
    "VISA_POSITIVE = (\"visa sponsorship\",\"h1b\",\"opt\",\"cpt\",\"stem opt\",\"work visa\")\n",
    "\n",
    "\n",
    "def _parse_recency_hours(snippet: str) -> Optional[float]:\n",
    "    s = (snippet or \"\").lower()\n",
    "    if \"today\" in s: return 6.0\n",
    "    if \"yesterday\" in s: return 24.0\n",
    "    m = re.search(r\"(\\d+)\\s*hours?\\s*ago\", s)\n",
    "    if m: return float(m.group(1))\n",
    "    m = re.search(r\"(\\d+)\\s*days?\\s*ago\", s)\n",
    "    if m: return float(m.group(1)) * 24.0\n",
    "    return None\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-zA-Z][a-zA-Z0-9\\+\\#\\.-]{1,}\", (text or \"\").lower())\n",
    "\n",
    "\n",
    "def _cosine(a: Dict[str,int], b: Dict[str,int]) -> float:\n",
    "    if not a or not b: return 0.0\n",
    "    dot = sum(v * b.get(k,0) for k,v in a.items())\n",
    "    na = sum(v*v for v in a.values()) ** 0.5\n",
    "    nb = sum(v*v for v in b.values()) ** 0.5\n",
    "    if na == 0 or nb == 0: return 0.0\n",
    "    return float(dot/(na*nb))\n",
    "\n",
    "\n",
    "def _ats_score(resume_text: str) -> float:\n",
    "    t = (resume_text or \"\").lower()\n",
    "    score = 0.0\n",
    "    if re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", t): score += 0.20\n",
    "    if re.search(r\"\\+?\\d[\\d\\-\\s\\(\\)]{8,}\\d\", t): score += 0.10\n",
    "    for h in [\"summary\",\"skills\",\"experience\",\"education\",\"projects\"]:\n",
    "        if h in t: score += 0.12\n",
    "    if \"-\" in resume_text or \"•\" in resume_text: score += 0.10\n",
    "    if len(resume_text) > 1200: score += 0.12\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "def _compute_interview_chance(skill_overlap: float, exp_align: float, ats: float, market: float) -> float:\n",
    "    market = max(1.0, float(market))\n",
    "    score = (0.45*skill_overlap + 0.35*exp_align + 0.20*ats) / market\n",
    "    return max(0.0, min(1.0, float(score)))\n",
    "\n",
    "\n",
    "class SerperClient:\n",
    "    SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "    def __init__(self, api_key: str) -> None:\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def search(self, query: str, num: int = 10, tbs: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        headers = {\"X-API-KEY\": self.api_key, \"Content-Type\": \"application/json\"}\n",
    "        body: Dict[str, Any] = {\"q\": query, \"num\": num}\n",
    "        if tbs:\n",
    "            body[\"tbs\"] = tbs\n",
    "        with httpx.Client(timeout=30.0) as client:\n",
    "            r = client.post(self.SERPER_URL, headers=headers, json=body)\n",
    "        if r.status_code >= 400:\n",
    "            return []\n",
    "        organic = (r.json().get(\"organic\") or [])\n",
    "        return [{\"title\": it.get(\"title\") or \"\", \"link\": it.get(\"link\") or \"\", \"snippet\": it.get(\"snippet\") or \"\"} for it in organic]\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    @staticmethod\n",
    "    def fetch_text(url: str, snippet: str) -> str:\n",
    "        if not url:\n",
    "            return snippet or \"\"\n",
    "        try:\n",
    "            with httpx.Client(timeout=18.0, follow_redirects=True) as client:\n",
    "                r = client.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "            if r.status_code >= 400:\n",
    "                return snippet or \"\"\n",
    "            html = r.text\n",
    "            txt = re.sub(r\"<(script|style)[^>]*>.*?</\\1>\", \" \", html, flags=re.S|re.I)\n",
    "            txt = re.sub(r\"<[^>]+>\", \" \", txt)\n",
    "            txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "            return txt[:16000] if txt else (snippet or \"\")\n",
    "        except Exception:\n",
    "            return snippet or \"\"\n",
    "\n",
    "\n",
    "class OneClickAutomationEngine:\n",
    "    \"\"\"\n",
    "    Description: Full one-click engine with soft-gated parser + full discovery/matching/ranking.\n",
    "    Layer: L0-L9\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._s = get_settings()\n",
    "        self._store = SqliteStateStore()\n",
    "        self._notifier = NotificationService(dry_run=not bool(self._s.twilio_account_sid))\n",
    "\n",
    "        self._sanitize = SanitizeAgent()\n",
    "        self._parser = ParserAgentService()\n",
    "        self._parser_eval = ParserEvaluatorService()\n",
    "\n",
    "        self._health = HealthService() if HealthService else None\n",
    "\n",
    "    def _persist(self, st: OrchestrationState) -> None:\n",
    "        d = _run_dir(st.run_id)\n",
    "        _save_json(d / \"state.json\", st.model_dump())\n",
    "        self._store.upsert_state(run_id=st.run_id, status=st.status, state=st.model_dump(), updated_at_utc=st.updated_at_utc)\n",
    "\n",
    "    def load(self, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        return self._store.get_state(run_id=run_id)\n",
    "\n",
    "    def start_run(self, *, filename: str, data: bytes, prefs: Dict[str, Any]) -> OrchestrationState:\n",
    "        st = OrchestrationState.new(env=self._s.environment, mode=\"agentic\", git_sha=None)\n",
    "        st.meta[\"preferences\"] = prefs\n",
    "        st.meta.setdefault(\"live_feed\", [])\n",
    "        st.meta.setdefault(\"job_scores\", {})\n",
    "        st.meta.setdefault(\"job_components\", {})\n",
    "        st.meta.setdefault(\"job_meta\", {})\n",
    "        LiveFeed.emit(st, layer=\"L1\", agent=\"Dashboard\", message=\"Run created. Starting autonomous pipeline…\")\n",
    "        self._persist(st)\n",
    "        t = threading.Thread(target=self._run, args=(st.run_id, filename, data), daemon=True)\n",
    "        t.start()\n",
    "        return st\n",
    "\n",
    "    def submit_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any]) -> OrchestrationState:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            raise ValueError(\"run_id not found\")\n",
    "        st = OrchestrationState(**raw)\n",
    "        st.meta[\"last_user_action\"] = {\"type\": action_type, \"payload\": payload}\n",
    "        LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=f\"User action received: {action_type}\")\n",
    "        self._persist(st)\n",
    "        t = threading.Thread(target=self._continue, args=(run_id,), daemon=True)\n",
    "        t.start()\n",
    "        return st\n",
    "\n",
    "    def _run(self, run_id: str, filename: str, data: bytes) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        st = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "        prefs = st.meta.get(\"preferences\", {}) or {}\n",
    "\n",
    "        # ---- L2 Extract ----\n",
    "        st.start_step(\"l2_extract\", layer_id=\"L2\", tool_name=\"ResumeExtractor\", input_ref={\"filename\": filename})\n",
    "        LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=\"Extracting resume text from upload…\")\n",
    "        try:\n",
    "            resume_text = LocalResumeExtractor.extract_text(filename=filename, data=data)\n",
    "        except Exception as e:\n",
    "            st.end_step(\"l2_extract\", status=\"failed\", output_ref={}, message=str(e))\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"resume_cleanup\"\n",
    "            LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=f\"Resume extraction failed: {e}\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        (run_dir / \"resume_raw.txt\").write_text(resume_text, encoding=\"utf-8\")\n",
    "        st.add_artifact(\"resume_raw\", str(run_dir / \"resume_raw.txt\"), content_type=\"text/plain\")\n",
    "        st.end_step(\"l2_extract\", status=\"ok\", output_ref={\"artifact_key\":\"resume_raw\"}, message=\"extracted\")\n",
    "        self._persist(st)\n",
    "\n",
    "        # ---- L0 Sanitize ----\n",
    "        st.start_step(\"l0_sanitize\", layer_id=\"L0\", tool_name=\"SanitizeAgent\", input_ref={})\n",
    "        safe = self._sanitize.sanitize_before_llm(state=st, step_id=\"l0_sanitize\", tool_name=\"sanitize_before_llm\", user_text=resume_text, context=\"resume\")\n",
    "        if safe is None:\n",
    "            st.status = \"blocked\"\n",
    "            LiveFeed.emit(st, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Prompt injection detected. Run blocked.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "        st.end_step(\"l0_sanitize\", status=\"ok\", output_ref={\"sanitized\": True}, message=\"pass\")\n",
    "        LiveFeed.emit(st, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Security check passed.\")\n",
    "        self._persist(st)\n",
    "\n",
    "        # ---- L2 Parse (single strong pass) ----\n",
    "        st.start_step(\"l2_parse\", layer_id=\"L2\", tool_name=\"ParserAgentService\", input_ref={\"attempt\": 1})\n",
    "        extracted = self._parser.parse(raw_text=safe, orchestration_state=st, feedback=[])\n",
    "        p = run_dir / \"extracted_resume.json\"\n",
    "        _save_json(p, extracted.to_json_dict())\n",
    "        st.add_artifact(\"extracted_resume\", str(p), content_type=\"application/json\")\n",
    "        st.end_step(\"l2_parse\", status=\"ok\", output_ref={\"artifact_key\":\"extracted_resume\"}, message=\"parsed\")\n",
    "\n",
    "        # ---- L3 Evaluate (SOFT gate) ----\n",
    "        # strict gate optional; default False for automation\n",
    "        strict_gate = bool(prefs.get(\"resume_strict_gate\", False))\n",
    "        threshold = 0.80 if strict_gate else float(prefs.get(\"resume_threshold\", 0.55))\n",
    "\n",
    "        ev = self._parser_eval.evaluate(\n",
    "            orchestration_state=st,\n",
    "            raw_text=safe,\n",
    "            extracted=extracted,\n",
    "            target_id=\"resume_main\",\n",
    "            threshold=threshold,\n",
    "            retry_count=0,\n",
    "            max_retries=0,\n",
    "        )\n",
    "\n",
    "        score = float(getattr(ev, \"evaluation_score\", 0.0))\n",
    "        LiveFeed.emit(st, layer=\"L3\", agent=\"ParserEvaluator\", message=f\"Resume quality={score:.2f} (threshold={threshold:.2f}).\")\n",
    "\n",
    "        # If truly broken, stop; otherwise continue but allow optional cleanup\n",
    "        if score < 0.30:\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"resume_cleanup\"\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"ParserEvaluator\", message=\"Resume parsing too weak. Needs manual cleanup.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "        elif score < threshold:\n",
    "            st.meta[\"pending_action\"] = \"resume_cleanup_optional\"\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"ParserEvaluator\", message=\"Proceeding automatically, but resume cleanup is recommended.\")\n",
    "\n",
    "        self._persist(st)\n",
    "\n",
    "        # ---- L3 Discovery ----\n",
    "        if not self._s.serper_api_key:\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"missing_serper_key\"\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=\"Missing SERPER_API_KEY in .env.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        target_role = str(prefs.get(\"target_role\",\"Data Scientist\"))\n",
    "        location = str(prefs.get(\"location\",\"United States\"))\n",
    "        remote = bool(prefs.get(\"remote\", True))\n",
    "        wfo_ok = bool(prefs.get(\"wfo_ok\", True))\n",
    "        salary = str(prefs.get(\"salary\",\"\")).strip()\n",
    "        visa_required = bool(prefs.get(\"visa_sponsorship_required\", False))\n",
    "        recency_hours = float(prefs.get(\"recency_hours\", 36))\n",
    "        max_jobs = int(prefs.get(\"max_jobs\", 40))\n",
    "        discovery_threshold = float(prefs.get(\"discovery_threshold\", 0.70))\n",
    "        max_refinements = int(prefs.get(\"max_refinements\", 3))\n",
    "\n",
    "        resume_skills = [s.lower() for s in (extracted.skills or [])]\n",
    "        ats = _ats_score(safe)\n",
    "        st.meta[\"ats_score\"] = ats\n",
    "\n",
    "        serper = SerperClient(self._s.serper_api_key)\n",
    "        tbs = \"qdr:d\" if recency_hours <= 36 else None\n",
    "\n",
    "        base_query = self._build_query(target_role, location, remote, wfo_ok, salary, visa_required, resume_skills)\n",
    "\n",
    "        for attempt in range(max_refinements + 1):\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=f\"Hunt attempt {attempt+1}: searching 8 boards…\")\n",
    "            results = self._discover_all(serper, base_query, tbs=tbs)\n",
    "\n",
    "            # ---- L4 Scrape + score ----\n",
    "            ranked = self._score_jobs(\n",
    "                resume_text=safe,\n",
    "                extracted=extracted,\n",
    "                ats=ats,\n",
    "                visa_required=visa_required,\n",
    "                recency_hours=recency_hours,\n",
    "                results=results[:max_jobs],\n",
    "            )\n",
    "            _save_json(run_dir / \"ranking.json\", ranked)\n",
    "            st.add_artifact(\"ranking\", str(run_dir / \"ranking.json\"), content_type=\"application/json\")\n",
    "            self._persist(st)\n",
    "\n",
    "            if not ranked:\n",
    "                conf = 0.0\n",
    "            else:\n",
    "                top = float(ranked[0][\"interview_chance_score\"])\n",
    "                avg = sum(float(x[\"interview_chance_score\"]) for x in ranked[:20]) / max(1, min(20, len(ranked)))\n",
    "                conf = min(1.0, 0.65*top + 0.35*avg)\n",
    "\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=f\"Discovery confidence={conf:.2f} (need ≥ {discovery_threshold:.2f}).\")\n",
    "\n",
    "            if ranked and float(ranked[0][\"interview_chance_score\"]) >= discovery_threshold:\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"review_ranking\"\n",
    "                LiveFeed.emit(st, layer=\"L1\", agent=\"Dashboard\", message=\"Ranking ready for review (HITL).\")\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            if attempt >= max_refinements:\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"low_confidence_discovery\"\n",
    "                LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=\"Low confidence after retries. Needs guidance.\")\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            base_query = self._refine_query(base_query, ranked, resume_skills, visa_required, target_role, location)\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=f\"Refining query → {base_query[:160]}\")\n",
    "            self._persist(st)\n",
    "\n",
    "    def _continue(self, run_id: str) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        st = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "        pending = st.meta.get(\"pending_action\")\n",
    "        action = (st.meta.get(\"last_user_action\") or {}).get(\"type\")\n",
    "        payload = (st.meta.get(\"last_user_action\") or {}).get(\"payload\") or {}\n",
    "\n",
    "        # NEW: Resume cleanup submit\n",
    "        if pending in (\"resume_cleanup\", \"resume_cleanup_optional\") and action == \"resume_cleanup_submit\":\n",
    "            new_text = str(payload.get(\"resume_text\",\"\")).strip()\n",
    "            if not new_text:\n",
    "                LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=\"Resume cleanup submitted but text was empty.\")\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            (run_dir / \"resume_raw.txt\").write_text(new_text, encoding=\"utf-8\")\n",
    "            st.add_artifact(\"resume_raw\", str(run_dir / \"resume_raw.txt\"), content_type=\"text/plain\")\n",
    "            st.meta[\"pending_action\"] = None\n",
    "            st.status = \"running\"\n",
    "            LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=\"Resume updated by user. Restarting pipeline…\")\n",
    "            self._persist(st)\n",
    "\n",
    "            # restart the run using updated text as txt bytes (no need for original upload)\n",
    "            self._run(run_id, \"resume.txt\", new_text.encode(\"utf-8\"))\n",
    "            return\n",
    "\n",
    "        # Ranking approval/reject can be wired next (your next step).\n",
    "        LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=f\"No handler for pending={pending}, action={action}\")\n",
    "        self._persist(st)\n",
    "\n",
    "    def _build_query(self, target_role: str, location: str, remote: bool, wfo_ok: bool, salary: str, visa_required: bool, skills: List[str]) -> str:\n",
    "        intent = []\n",
    "        if remote: intent.append(\"remote\")\n",
    "        if wfo_ok: intent.append(\"hybrid\")\n",
    "        if not intent: intent.append(\"on-site\")\n",
    "        skill_str = \" \".join(skills[:6])\n",
    "        salary_part = f'\"{salary}\"' if salary else \"\"\n",
    "        visa_part = '\"visa sponsorship\" OR h1b OR opt OR cpt' if visa_required else \"\"\n",
    "        return f'{target_role} {location} {\" \".join(intent)} {salary_part} {skill_str} ({visa_part}) apply'\n",
    "\n",
    "    def _discover_all(self, serper: SerperClient, base_query: str, tbs: Optional[str]) -> List[Dict[str, Any]]:\n",
    "        seen = set()\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        for name, domain in JOB_BOARDS:\n",
    "            q = f\"{base_query} site:{domain}\"\n",
    "            items = serper.search(q, num=10, tbs=tbs)\n",
    "            for it in items:\n",
    "                link = it.get(\"link\") or \"\"\n",
    "                if not link or link in seen:\n",
    "                    continue\n",
    "                seen.add(link)\n",
    "                it[\"board\"] = name\n",
    "                out.append(it)\n",
    "        return out\n",
    "\n",
    "    def _score_jobs(self, *, resume_text: str, extracted: ExtractedResume, ats: float, visa_required: bool, recency_hours: float, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        ranked: List[Dict[str, Any]] = []\n",
    "        res_tokens = {}\n",
    "        for t in _tokenize(resume_text):\n",
    "            res_tokens[t] = res_tokens.get(t, 0) + 1\n",
    "        exp_text = \" \".join([\" \".join(x.bullets) for x in (extracted.experience or [])]) if extracted.experience else resume_text\n",
    "        exp_tokens = {}\n",
    "        for t in _tokenize(exp_text):\n",
    "            exp_tokens[t] = exp_tokens.get(t, 0) + 1\n",
    "        resume_skills = [s.lower() for s in (extracted.skills or [])]\n",
    "\n",
    "        for it in results:\n",
    "            snippet = it.get(\"snippet\") or \"\"\n",
    "            rh = _parse_recency_hours(snippet)\n",
    "            if rh is not None and rh > recency_hours:\n",
    "                continue\n",
    "\n",
    "            url = it.get(\"link\") or \"\"\n",
    "            board = it.get(\"board\") or \"unknown\"\n",
    "            title = it.get(\"title\") or \"\"\n",
    "\n",
    "            job_text = Scraper.fetch_text(url, snippet)\n",
    "            low = job_text.lower()\n",
    "\n",
    "            # visa filter\n",
    "            v_ok = not any(x in low for x in VISA_NEGATIVE)\n",
    "            if visa_required and not v_ok:\n",
    "                continue\n",
    "\n",
    "            # skills overlap (from resume skills present in job text)\n",
    "            present = [s for s in resume_skills if s and s in low]\n",
    "            overlap = len(set(present)) / max(1, len(set(resume_skills))) if resume_skills else 0.0\n",
    "\n",
    "            # exp alignment cosine\n",
    "            job_tokens = {}\n",
    "            for t in _tokenize(job_text):\n",
    "                job_tokens[t] = job_tokens.get(t, 0) + 1\n",
    "            exp_align = _cosine(exp_tokens, job_tokens)\n",
    "\n",
    "            market = 1.0\n",
    "            if \"applicants\" in snippet.lower():\n",
    "                m = re.search(r\"(\\d+)\\+?\\s*applicants\", snippet.lower())\n",
    "                if m:\n",
    "                    n = int(m.group(1))\n",
    "                    market = 1.0 + min(1.5, n/200.0)\n",
    "\n",
    "            score = _compute_interview_chance(overlap, exp_align, ats, market)\n",
    "            if visa_required and any(x in low for x in VISA_POSITIVE):\n",
    "                score = min(1.0, score + 0.05)\n",
    "\n",
    "            ranked.append({\n",
    "                \"title\": title,\n",
    "                \"board\": board,\n",
    "                \"url\": url,\n",
    "                \"recency_hours\": rh,\n",
    "                \"visa_ok\": v_ok,\n",
    "                \"skill_overlap\": overlap,\n",
    "                \"experience_alignment\": exp_align,\n",
    "                \"ats_score\": ats,\n",
    "                \"market_factor\": market,\n",
    "                \"interview_chance_score\": score,\n",
    "                \"overall_match_percent\": round(score*100.0, 2),\n",
    "                \"matched_skills\": present[:12],\n",
    "                \"rationale\": [\n",
    "                    f\"SkillOverlap={overlap:.2f}\",\n",
    "                    f\"ExperienceAlignment={exp_align:.2f}\",\n",
    "                    f\"ATS={ats:.2f}\",\n",
    "                    f\"MarketFactor={market:.2f}\",\n",
    "                    (\"VisaOK\" if v_ok else \"NoSponsorship\"),\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        ranked.sort(key=lambda x: float(x[\"interview_chance_score\"]), reverse=True)\n",
    "        for i, r in enumerate(ranked, start=1):\n",
    "            r[\"rank\"] = i\n",
    "        return ranked\n",
    "\n",
    "    def _refine_query(self, base_query: str, ranked: List[Dict[str, Any]], resume_skills: List[str], visa_required: bool, target_role: str, location: str) -> str:\n",
    "        top = ranked[0] if ranked else {}\n",
    "        top_skills = (top.get(\"matched_skills\") or [])[:6]\n",
    "        hint = \" \".join(list(dict.fromkeys([*top_skills, *resume_skills]))[:8])\n",
    "        visa = '(\"visa sponsorship\" OR h1b OR opt)' if visa_required else \"\"\n",
    "        return f\"{target_role} {location} {hint} {visa} apply\"\n",
    "\n",
    "\n",
    "ENGINE = OneClickAutomationEngine()\n",
    "'''\n",
    "backup_write(\"src/careeragent/orchestration/engine.py\", ENGINE_PATCH)\n",
    "\n",
    "backup_write(\"src/careeragent/orchestration/__init__.py\", \"from .state import OrchestrationState\\nfrom .engine import ENGINE, OneClickAutomationEngine\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Update dashboard: add Resume Cleanup HITL UI + L0-L9 layer panels\n",
    "# ---------------------------\n",
    "DASHBOARD_PATCH = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import streamlit as st\n",
    "\n",
    "REPO_ROOT = Path(__file__).resolve().parents[2]\n",
    "SRC = REPO_ROOT / \"src\"\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "DEFAULT_API = os.getenv(\"API_URL\", \"http://127.0.0.1:8000\")\n",
    "\n",
    "\n",
    "def _safe_json(resp: requests.Response) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return resp.json()\n",
    "    except Exception:\n",
    "        return {\"_raw\": resp.text[:1500], \"_status_code\": resp.status_code}\n",
    "\n",
    "\n",
    "def _api_get(api: str, path: str, timeout: int = 25) -> requests.Response:\n",
    "    return requests.get(f\"{api}{path}\", timeout=timeout)\n",
    "\n",
    "\n",
    "def _api_post(api: str, path: str, timeout: int = 30, **kwargs) -> requests.Response:\n",
    "    return requests.post(f\"{api}{path}\", timeout=timeout, **kwargs)\n",
    "\n",
    "\n",
    "def _exists(path: Optional[str]) -> bool:\n",
    "    return bool(path) and Path(path).exists()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    st.set_page_config(page_title=\"CareerAgent-AI Mission Control\", layout=\"wide\")\n",
    "    st.title(\"CareerAgent-AI — Mission Control (One-Click Automation)\")\n",
    "    st.caption(\"Upload resume → autonomous ingestion + discovery + ranking → HITL approvals → drafts + dossier downloads\")\n",
    "\n",
    "    api = st.sidebar.text_input(\"API Base URL\", value=DEFAULT_API)\n",
    "\n",
    "    # Backend status\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Backend\")\n",
    "    try:\n",
    "        h = _api_get(api, \"/health\", timeout=3)\n",
    "        if h.status_code == 200:\n",
    "            st.sidebar.success(\"🟢 API Online\")\n",
    "        else:\n",
    "            st.sidebar.warning(f\"🟠 API issue ({h.status_code})\")\n",
    "    except Exception as e:\n",
    "        st.sidebar.error(\"🔴 API Offline\")\n",
    "        st.sidebar.caption(str(e))\n",
    "        st.stop()\n",
    "\n",
    "    # Inputs\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Resume Upload\")\n",
    "    resume_file = st.sidebar.file_uploader(\"Upload Resume (PDF/TXT/DOCX)\", type=[\"pdf\", \"txt\", \"docx\"])\n",
    "\n",
    "    st.sidebar.subheader(\"Preferences\")\n",
    "    target_role = st.sidebar.text_input(\"Target role\", value=\"Data Scientist\")\n",
    "    country = st.sidebar.text_input(\"Country\", value=\"US\")\n",
    "    location = st.sidebar.text_input(\"Location\", value=\"United States\")\n",
    "    remote = st.sidebar.checkbox(\"Remote preferred\", value=True)\n",
    "    wfo_ok = st.sidebar.checkbox(\"On-site/WFO acceptable\", value=True)\n",
    "    salary = st.sidebar.text_input(\"Salary target (optional)\", value=\"\")\n",
    "    visa_required = st.sidebar.checkbox(\"Visa sponsorship required (F1/OPT)\", value=False)\n",
    "\n",
    "    recency_hours = st.sidebar.slider(\"Only jobs posted within (hours)\", 12, 168, 36, 6)\n",
    "    max_jobs = st.sidebar.slider(\"Max jobs to score per run\", 10, 80, 40, 5)\n",
    "\n",
    "    st.sidebar.subheader(\"Autonomy Controls\")\n",
    "    discovery_threshold = st.sidebar.slider(\"Discovery confidence threshold\", 0.50, 0.90, 0.70, 0.05)\n",
    "    max_refinements = st.sidebar.slider(\"Max query refinements\", 1, 6, 3, 1)\n",
    "\n",
    "    # NEW: soft gate controls\n",
    "    resume_strict_gate = st.sidebar.checkbox(\"Strict resume gate (stop on low parse)\", value=False)\n",
    "    resume_threshold = st.sidebar.slider(\"Resume quality threshold (soft gate)\", 0.35, 0.90, 0.55, 0.05)\n",
    "\n",
    "    run_btn = st.sidebar.button(\"🚀 RUN ONE-CLICK\", type=\"primary\", use_container_width=True, disabled=(resume_file is None))\n",
    "\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Existing Run\")\n",
    "    run_id_in = st.sidebar.text_input(\"Run ID\", value=st.session_state.get(\"run_id\", \"\"))\n",
    "\n",
    "    if run_btn:\n",
    "        prefs = {\n",
    "            \"target_role\": target_role.strip() or \"Data Scientist\",\n",
    "            \"country\": country.strip() or \"US\",\n",
    "            \"location\": location.strip() or \"United States\",\n",
    "            \"remote\": bool(remote),\n",
    "            \"wfo_ok\": bool(wfo_ok),\n",
    "            \"salary\": salary.strip(),\n",
    "            \"visa_sponsorship_required\": bool(visa_required),\n",
    "            \"recency_hours\": float(recency_hours),\n",
    "            \"max_jobs\": int(max_jobs),\n",
    "            \"discovery_threshold\": float(discovery_threshold),\n",
    "            \"max_refinements\": int(max_refinements),\n",
    "            \"resume_strict_gate\": bool(resume_strict_gate),\n",
    "            \"resume_threshold\": float(resume_threshold),\n",
    "        }\n",
    "        files = {\"resume\": (resume_file.name, resume_file.getvalue())}\n",
    "        data = {\"preferences_json\": json.dumps(prefs)}\n",
    "        r = _api_post(api, \"/analyze\", files=files, data=data, timeout=180)\n",
    "        if r.status_code >= 400:\n",
    "            st.error(f\"/analyze failed: {r.status_code}\\n\\n{r.text[:1500]}\")\n",
    "            st.stop()\n",
    "        out = _safe_json(r)\n",
    "        st.session_state[\"run_id\"] = out[\"run_id\"]\n",
    "        st.success(f\"Run started: {out['run_id']} (status: {out.get('status')})\")\n",
    "\n",
    "    run_id = st.session_state.get(\"run_id\") or run_id_in.strip()\n",
    "    if not run_id:\n",
    "        st.info(\"Upload resume and click RUN, or paste a run_id.\")\n",
    "        return\n",
    "\n",
    "    # Poll\n",
    "    _ = st.button(\"🔄 Refresh\", use_container_width=True)\n",
    "    r = _api_get(api, f\"/status/{run_id}\", timeout=25)\n",
    "    if r.status_code != 200:\n",
    "        st.warning(f\"Run not found yet ({r.status_code}).\")\n",
    "        return\n",
    "    state = _safe_json(r)\n",
    "\n",
    "    status = state.get(\"status\", \"unknown\")\n",
    "    meta = state.get(\"meta\", {}) or {}\n",
    "    pending = meta.get(\"pending_action\")\n",
    "    feed = meta.get(\"live_feed\", []) or []\n",
    "    steps = state.get(\"steps\", []) or []\n",
    "    artifacts = state.get(\"artifacts\", {}) or {}\n",
    "    evals = state.get(\"evaluations\", []) or []\n",
    "\n",
    "    c1, c2, c3 = st.columns([1,1,1])\n",
    "    with c1: st.metric(\"Run ID\", run_id)\n",
    "    with c2: st.metric(\"Status\", status)\n",
    "    with c3: st.metric(\"Pending\", str(pending))\n",
    "\n",
    "    # Layer map (aligned with your architecture)\n",
    "    with st.expander(\"Architecture Layer Map (L0–L9)\", expanded=False):\n",
    "        st.markdown(\"\"\"\n",
    "- **L0** Security & Guardrails  \n",
    "- **L1** Mission Control (UI)  \n",
    "- **L2** Parsing & Profile Extraction  \n",
    "- **L3** Discovery / Connectors  \n",
    "- **L4** Scrape + Matching / Vectorization  \n",
    "- **L5** Evaluator + Ranking + HITL gates  \n",
    "- **L6** Drafting (Resume/Cover/Answers)  \n",
    "- **L7** Execution (Apply/Notifications)  \n",
    "- **L8** Tracking (Status/CRM/DB)  \n",
    "- **L9** Analytics + XAI + Dossier Export\n",
    "        \"\"\")\n",
    "\n",
    "    # Feed\n",
    "    st.markdown(\"### Live Agent Feed\")\n",
    "    for ev in feed[-180:]:\n",
    "        st.write(f\"**[{ev.get('layer')} {ev.get('agent')}]** {ev.get('message')}\")\n",
    "\n",
    "    # Evaluations\n",
    "    with st.expander(\"Evaluations (Scores + Reasons)\", expanded=False):\n",
    "        if not evals:\n",
    "            st.caption(\"No evaluations yet.\")\n",
    "        else:\n",
    "            for e in evals[-10:]:\n",
    "                st.write(f\"**{e.get('layer_id')} / {e.get('target_id')}** score={e.get('evaluation_score'):.2f} threshold={e.get('threshold')}\")\n",
    "                fb = e.get(\"feedback\") or []\n",
    "                if fb:\n",
    "                    st.write(\"- \" + \"\\n- \".join(fb[:6]))\n",
    "\n",
    "    # Resume cleanup HITL\n",
    "    if status == \"needs_human_approval\" and pending in (\"resume_cleanup\", \"resume_cleanup_optional\"):\n",
    "        st.markdown(\"## Human-in-the-Loop: Resume Cleanup\")\n",
    "        resume_ref = (artifacts.get(\"resume_raw\") or {}).get(\"path\")\n",
    "        current_text = \"\"\n",
    "        if resume_ref and Path(resume_ref).exists():\n",
    "            current_text = Path(resume_ref).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        st.info(\"Your resume is parseable, but quality is below target. You can paste an improved version (add email/phone + clearer headings) and continue automatically.\")\n",
    "        new_text = st.text_area(\"Paste improved resume text (ATS headings + contact)\", value=current_text, height=220)\n",
    "\n",
    "        if st.button(\"✅ Submit cleaned resume & continue\", type=\"primary\"):\n",
    "            rr = _api_post(api, f\"/action/{run_id}\", json={\"action_type\": \"resume_cleanup_submit\", \"payload\": {\"resume_text\": new_text}}, timeout=60)\n",
    "            if rr.status_code >= 400:\n",
    "                st.error(rr.text[:1200])\n",
    "            else:\n",
    "                st.success(\"Submitted. Refresh in a few seconds.\")\n",
    "\n",
    "    # Ranking\n",
    "    st.markdown(\"### Ranking\")\n",
    "    ranking_ref = artifacts.get(\"ranking\")\n",
    "    if ranking_ref and _exists(ranking_ref.get(\"path\")):\n",
    "        ranking = json.loads(Path(ranking_ref[\"path\"]).read_text(encoding=\"utf-8\"))\n",
    "        st.dataframe(\n",
    "            [{\n",
    "                \"rank\": x.get(\"rank\"),\n",
    "                \"score_%\": x.get(\"overall_match_percent\"),\n",
    "                \"title\": x.get(\"title\"),\n",
    "                \"board\": x.get(\"board\"),\n",
    "                \"recency_h\": x.get(\"recency_hours\"),\n",
    "                \"visa_ok\": x.get(\"visa_ok\"),\n",
    "                \"url\": x.get(\"url\"),\n",
    "            } for x in ranking],\n",
    "            use_container_width=True\n",
    "        )\n",
    "    else:\n",
    "        st.caption(\"Ranking not available yet.\")\n",
    "\n",
    "    with st.expander(\"Full State JSON\", expanded=False):\n",
    "        st.json(state)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "backup_write(\"app/ui/dashboard.py\", DASHBOARD_PATCH)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) app/main.py robust bootstrap (keeps app import stable)\n",
    "# ---------------------------\n",
    "APP_MAIN = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "from app.ui.dashboard import main\n",
    "main()\n",
    "'''\n",
    "backup_write(\"app/main.py\", APP_MAIN)\n",
    "\n",
    "print(\"\\n✅ PATCH COMPLETE.\")\n",
    "print(\"Restart backend + frontend now.\\n\")\n",
    "print(\"Backend:\")\n",
    "print(\"  PYTHONPATH=src uv run python -m uvicorn careeragent.api.main:app --host 127.0.0.1 --port 8000 --reload\")\n",
    "print(\"Frontend:\")\n",
    "print(\"  API_URL=http://127.0.0.1:8000 PYTHONPATH='.:src' uv run streamlit run app/main.py --server.port 8501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd11898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc96023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bda29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaeb58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/services/db_service.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/services/db_service.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/services/health_service.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/services/health_service.py\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/services/learning_resource_service.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_agent_service.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_agent_service.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_evaluator_service.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/agents/parser_evaluator_service.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py.bak_20260221_021010\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py\n",
      "\n",
      "✅ FULL AUTOMATION PATCH WRITTEN.\n",
      "Restart backend + frontend.\n",
      "\n",
      "Backend:\n",
      "  PYTHONPATH=src uv run python -m uvicorn careeragent.api.main:app --host 127.0.0.1 --port 8000 --reload\n",
      "Frontend:\n",
      "  API_URL=http://127.0.0.1:8000 PYTHONPATH='.:src' uv run streamlit run app/main.py --server.port 8501\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Auto-chdir to repo root even when notebook runs in notebooks_v2\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name == \"notebooks_v2\":\n",
    "    os.chdir(ROOT.parent)\n",
    "ROOT = Path.cwd().resolve()\n",
    "assert (ROOT / \"src\").exists(), f\"Not at repo root. CWD={ROOT}\"\n",
    "print(\"✅ CWD =\", ROOT)\n",
    "\n",
    "def backup_write(rel_path: str, content: str) -> None:\n",
    "    p = ROOT / rel_path\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts}\")\n",
    "        bak.write_text(p.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(\"BACKUP:\", bak)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    print(\"WROTE:\", p)\n",
    "\n",
    "# Ensure packages\n",
    "(ROOT / \"src/careeragent/services\").mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / \"src/careeragent/agents\").mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / \"src/careeragent/orchestration\").mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / \"app/ui\").mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / \"app\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(ROOT / \"src/careeragent/services/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(ROOT / \"src/careeragent/agents/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(ROOT / \"src/careeragent/orchestration/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(ROOT / \"app/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(ROOT / \"app/ui/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# -------------------------\n",
    "# services/db_service.py\n",
    "# -------------------------\n",
    "backup_write(\"src/careeragent/services/db_service.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from careeragent.config import artifacts_root\n",
    "\n",
    "\n",
    "class SqliteStateStore:\n",
    "    \"\"\"\n",
    "    Description: Local-first persistence for OrchestrationState.\n",
    "    Layer: L8\n",
    "    Input: run state snapshots\n",
    "    Output: sqlite persisted state for polling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        db_dir = artifacts_root() / \"db\"\n",
    "        db_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self._db_path = db_dir / \"careeragent.db\"\n",
    "        self._init_schema()\n",
    "\n",
    "    def _init_schema(self) -> None:\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS runs (\n",
    "                    run_id TEXT PRIMARY KEY,\n",
    "                    status TEXT NOT NULL,\n",
    "                    state_json TEXT NOT NULL,\n",
    "                    updated_at_utc TEXT NOT NULL\n",
    "                )\n",
    "            \"\"\")\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS actions (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    run_id TEXT NOT NULL,\n",
    "                    action_type TEXT NOT NULL,\n",
    "                    payload_json TEXT NOT NULL,\n",
    "                    created_at_utc TEXT NOT NULL\n",
    "                )\n",
    "            \"\"\")\n",
    "            con.commit()\n",
    "\n",
    "    def upsert_state(self, *, run_id: str, status: str, state: Dict[str, Any], updated_at_utc: str) -> None:\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO runs(run_id, status, state_json, updated_at_utc)\n",
    "                VALUES(?,?,?,?)\n",
    "                ON CONFLICT(run_id) DO UPDATE SET\n",
    "                    status=excluded.status,\n",
    "                    state_json=excluded.state_json,\n",
    "                    updated_at_utc=excluded.updated_at_utc\n",
    "            \"\"\", (run_id, status, json.dumps(state), updated_at_utc))\n",
    "            con.commit()\n",
    "\n",
    "    def get_state(self, *, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            cur = con.execute(\"SELECT state_json FROM runs WHERE run_id=?\", (run_id,))\n",
    "            row = cur.fetchone()\n",
    "            return json.loads(row[0]) if row else None\n",
    "\n",
    "    def insert_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any], created_at_utc: str) -> None:\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            con.execute(\n",
    "                \"INSERT INTO actions(run_id, action_type, payload_json, created_at_utc) VALUES(?,?,?,?)\",\n",
    "                (run_id, action_type, json.dumps(payload), created_at_utc),\n",
    "            )\n",
    "            con.commit()\n",
    "''')\n",
    "\n",
    "# -------------------------\n",
    "# services/health_service.py (quota + tracing hooks)\n",
    "# -------------------------\n",
    "backup_write(\"src/careeragent/services/health_service.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "from careeragent.orchestration.state import OrchestrationState\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuotaManager:\n",
    "    \"\"\"\n",
    "    Description: Tracks quota errors for external APIs and blocks runs.\n",
    "    Layer: L0\n",
    "    \"\"\"\n",
    "    serper_quota_exceeded: bool = False\n",
    "    last_error: Optional[str] = None\n",
    "\n",
    "    def handle_serper_response(self, *, state: OrchestrationState, step_id: str, status_code: int, tool_name: str, error_detail: str) -> bool:\n",
    "        if status_code == 403:\n",
    "            self.serper_quota_exceeded = True\n",
    "            self.last_error = error_detail\n",
    "            state.status = \"blocked\"\n",
    "            state.meta[\"run_failure_code\"] = \"API_FAILURE\"\n",
    "            state.meta[\"run_failure_provider\"] = \"serper\"\n",
    "            state.meta[\"run_failure_detail\"] = error_detail\n",
    "            # mark step blocked if possible\n",
    "            try:\n",
    "                state.end_step(step_id, status=\"blocked\", output_ref={\"error\": \"quota_exceeded\"}, message=\"serper_quota_exceeded\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class HealthService:\n",
    "    \"\"\"\n",
    "    Description: Local health + tracing wiring.\n",
    "    Layer: L0\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.quota = QuotaManager()\n",
    "\n",
    "    def load_env(self, *, dotenv_path: str) -> None:\n",
    "        # No-op: settings already load .env via pydantic-settings in your repo.\n",
    "        return\n",
    "\n",
    "    def enable_langsmith_tracing(self, *, project: str) -> None:\n",
    "        # No-op: optional integration later.\n",
    "        return\n",
    "''')\n",
    "\n",
    "# -------------------------\n",
    "# services/learning_resource_service.py\n",
    "# -------------------------\n",
    "backup_write(\"src/careeragent/services/learning_resource_service.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import httpx\n",
    "\n",
    "\n",
    "class LearningResourceService:\n",
    "    \"\"\"\n",
    "    Description: Fetch tutorials / YouTube / docs for missing skills using Serper.\n",
    "    Layer: L9\n",
    "    Input: skill list\n",
    "    Output: list of learning links\n",
    "    \"\"\"\n",
    "\n",
    "    SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "    def __init__(self, *, serper_api_key: Optional[str]) -> None:\n",
    "        self._key = (serper_api_key or \"\").strip()\n",
    "\n",
    "    def available(self) -> bool:\n",
    "        return bool(self._key)\n",
    "\n",
    "    def search_links(self, *, query: str, num: int = 5) -> List[Dict[str, Any]]:\n",
    "        if not self.available():\n",
    "            return []\n",
    "        headers = {\"X-API-KEY\": self._key, \"Content-Type\": \"application/json\"}\n",
    "        with httpx.Client(timeout=25.0) as client:\n",
    "            r = client.post(self.SERPER_URL, headers=headers, json={\"q\": query, \"num\": num})\n",
    "        if r.status_code >= 400:\n",
    "            return []\n",
    "        organic = (r.json().get(\"organic\") or [])\n",
    "        out = []\n",
    "        for it in organic[:num]:\n",
    "            out.append({\"title\": it.get(\"title\"), \"link\": it.get(\"link\"), \"snippet\": it.get(\"snippet\")})\n",
    "        return out\n",
    "\n",
    "    def build_learning_plan(self, *, missing_skills: List[str]) -> Dict[str, Any]:\n",
    "        plan: Dict[str, Any] = {}\n",
    "        for s in missing_skills[:12]:\n",
    "            skill = str(s).strip()\n",
    "            if not skill:\n",
    "                continue\n",
    "            plan[skill] = {\n",
    "                \"youtube\": self.search_links(query=f\"{skill} tutorial youtube\", num=3),\n",
    "                \"docs\": self.search_links(query=f\"{skill} official documentation\", num=3),\n",
    "                \"course\": self.search_links(query=f\"best course to learn {skill}\", num=3),\n",
    "            }\n",
    "        return plan\n",
    "''')\n",
    "\n",
    "# -------------------------\n",
    "# agents/parser_agent_service.py (deterministic intake bundle)\n",
    "# -------------------------\n",
    "backup_write(\"src/careeragent/agents/parser_agent_service.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    email: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "    linkedin: Optional[str] = None\n",
    "    github: Optional[str] = None\n",
    "\n",
    "\n",
    "class ExperienceItem(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    bullets: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ExtractedResume(BaseModel):\n",
    "    \"\"\"\n",
    "    Description: L2 Intake Bundle (profile extracted from raw resume).\n",
    "    Layer: L2\n",
    "    Input: raw resume text\n",
    "    Output: structured profile JSON\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(extra=\"ignore\")\n",
    "    name: Optional[str] = None\n",
    "    contact: ContactInfo = Field(default_factory=ContactInfo)\n",
    "    summary: Optional[str] = None\n",
    "    skills: List[str] = Field(default_factory=list)\n",
    "    experience: List[ExperienceItem] = Field(default_factory=list)\n",
    "    raw_text: Optional[str] = None\n",
    "    warnings: List[str] = Field(default_factory=list)\n",
    "\n",
    "    def to_json_dict(self) -> dict[str, Any]:\n",
    "        return self.model_dump()\n",
    "\n",
    "\n",
    "COMMON_SKILLS = [\n",
    "    \"python\",\"sql\",\"fastapi\",\"docker\",\"kubernetes\",\"mlflow\",\"dvc\",\"aws\",\"azure\",\"gcp\",\n",
    "    \"pytorch\",\"tensorflow\",\"scikit-learn\",\"pandas\",\"numpy\",\"spark\",\"databricks\",\"snowflake\",\n",
    "    \"langchain\",\"langgraph\",\"rag\",\"faiss\",\"chroma\",\"terraform\",\"github actions\",\"kafka\",\"airflow\"\n",
    "]\n",
    "\n",
    "\n",
    "def _find_email(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text or \"\")\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "\n",
    "def _find_phone(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"(\\+?\\d{1,3}[\\s\\-]?)?(\\(?\\d{3}\\)?[\\s\\-]?)\\d{3}[\\s\\-]?\\d{4}\", text or \"\")\n",
    "    return m.group(0).strip() if m else None\n",
    "\n",
    "\n",
    "def _find_url(text: str, kw: str) -> Optional[str]:\n",
    "    m = re.search(rf\"(https?://[^\\s]*{kw}[^\\s]*)\", text or \"\", flags=re.I)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def _extract_name(text: str) -> Optional[str]:\n",
    "    lines = [ln.strip() for ln in (text or \"\").splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return None\n",
    "    cand = lines[0]\n",
    "    if \"@\" in cand:\n",
    "        return None\n",
    "    return cand[:60]\n",
    "\n",
    "\n",
    "def _extract_skills(text: str) -> List[str]:\n",
    "    low = (text or \"\").lower()\n",
    "    found = []\n",
    "    for s in COMMON_SKILLS:\n",
    "        if s in low:\n",
    "            found.append(s)\n",
    "    # also parse a \"skills:\" line\n",
    "    m = re.search(r\"skills\\s*[:\\-]\\s*(.+)\", text or \"\", flags=re.I)\n",
    "    if m:\n",
    "        parts = re.split(r\"[,\\|/•\\n]+\", m.group(1))\n",
    "        found.extend([p.strip().lower() for p in parts if p.strip()])\n",
    "    # unique + clean\n",
    "    out = []\n",
    "    for x in found:\n",
    "        x = re.sub(r\"\\s+\", \" \", x.strip())\n",
    "        if 2 <= len(x) <= 40 and x not in out:\n",
    "            out.append(x)\n",
    "    return out[:60]\n",
    "\n",
    "\n",
    "class ParserAgentService:\n",
    "    \"\"\"\n",
    "    Description: Deterministic ParserAgent (CareerOS-style). Never blocks automation.\n",
    "    Layer: L2\n",
    "    Input: raw resume text\n",
    "    Output: ExtractedResume\n",
    "    \"\"\"\n",
    "    def parse(self, *, raw_text: str, orchestration_state: Any, feedback: List[str] | None = None) -> ExtractedResume:\n",
    "        text = raw_text or \"\"\n",
    "        email = _find_email(text)\n",
    "        phone = _find_phone(text)\n",
    "        linkedin = _find_url(text, \"linkedin\")\n",
    "        github = _find_url(text, \"github\")\n",
    "\n",
    "        skills = _extract_skills(text)\n",
    "\n",
    "        # summary: first 2-3 non-empty lines after name\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        name = _extract_name(text)\n",
    "        summary = None\n",
    "        if len(lines) >= 3:\n",
    "            summary = \" \".join(lines[1:3])[:400]\n",
    "\n",
    "        # experience bullets: take lines starting with dash/bullet\n",
    "        bullets = []\n",
    "        for ln in lines:\n",
    "            if ln.startswith((\"-\", \"•\")) and len(ln) > 15:\n",
    "                bullets.append(ln.strip(\"-• \").strip())\n",
    "        exp = [ExperienceItem(bullets=bullets[:12])] if bullets else []\n",
    "\n",
    "        warnings = []\n",
    "        if not (email or phone):\n",
    "            warnings.append(\"Contact info missing (email/phone).\")\n",
    "        if len(skills) < 6:\n",
    "            warnings.append(\"Low extracted skills; add a clearer Skills section.\")\n",
    "        if not bullets:\n",
    "            warnings.append(\"No bullet points detected; add bullets under Experience for ATS.\")\n",
    "\n",
    "        return ExtractedResume(\n",
    "            name=name,\n",
    "            contact=ContactInfo(email=email, phone=phone, linkedin=linkedin, github=github),\n",
    "            summary=summary,\n",
    "            skills=skills,\n",
    "            experience=exp,\n",
    "            raw_text=text,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "''')\n",
    "\n",
    "# -------------------------\n",
    "# agents/parser_evaluator_service.py (SOFT gate)\n",
    "# -------------------------\n",
    "backup_write(\"src/careeragent/agents/parser_evaluator_service.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Any, List\n",
    "from careeragent.agents.parser_agent_service import ExtractedResume\n",
    "\n",
    "\n",
    "def _ats_structure_score(text: str) -> float:\n",
    "    t = (text or \"\").lower()\n",
    "    score = 0.0\n",
    "    for h in [\"summary\",\"skills\",\"experience\",\"education\",\"projects\"]:\n",
    "        if h in t:\n",
    "            score += 0.14\n",
    "    if re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", t):\n",
    "        score += 0.15\n",
    "    if \"-\" in text or \"•\" in text:\n",
    "        score += 0.15\n",
    "    if len(text) > 1200:\n",
    "        score += 0.15\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "class ParserEvaluatorService:\n",
    "    \"\"\"\n",
    "    Description: Evaluates extracted profile quality (SOFT gate).\n",
    "    Layer: L3\n",
    "    \"\"\"\n",
    "    def evaluate(\n",
    "        self,\n",
    "        *,\n",
    "        orchestration_state: Any,\n",
    "        raw_text: str,\n",
    "        extracted: ExtractedResume,\n",
    "        target_id: str,\n",
    "        threshold: float = 0.55,\n",
    "        retry_count: int = 0,\n",
    "        max_retries: int = 0,\n",
    "    ) -> Any:\n",
    "        fb: List[str] = []\n",
    "        contact_ok = bool(extracted.contact.email or extracted.contact.phone)\n",
    "        skills_ok = len(extracted.skills or []) >= 6\n",
    "        exp_ok = bool(extracted.experience and extracted.experience[0].bullets)\n",
    "        ats = _ats_structure_score(raw_text)\n",
    "\n",
    "        score = (0.15*(1.0 if contact_ok else 0.0) +\n",
    "                 0.40*min(1.0, len(extracted.skills or [])/12.0) +\n",
    "                 0.30*(1.0 if exp_ok else 0.0) +\n",
    "                 0.15*ats)\n",
    "\n",
    "        if not contact_ok:\n",
    "            fb.append(\"Contact missing. Continue anyway; but adding email/phone improves ATS + recruiter trust.\")\n",
    "        if not exp_ok:\n",
    "            fb.append(\"No bullet points detected. Continue anyway; but bullets improve ATS scanability.\")\n",
    "        if not skills_ok:\n",
    "            fb.append(\"Skills list looks thin. Continue anyway; add more relevant tools/keywords.\")\n",
    "        if ats < 0.5:\n",
    "            fb.append(\"ATS structure weak. Add headings and bullets.\")\n",
    "\n",
    "        return orchestration_state.record_evaluation(\n",
    "            layer_id=\"L3\",\n",
    "            target_id=target_id,\n",
    "            generator_agent=\"parser_agent_service\",\n",
    "            evaluator_agent=\"parser_evaluator_service\",\n",
    "            evaluation_score=float(score),\n",
    "            threshold=float(threshold),\n",
    "            feedback=fb,\n",
    "            retry_count=int(retry_count),\n",
    "            max_retries=int(max_retries),\n",
    "            interview_chance={\n",
    "                \"weights\": {\"w1_skill_overlap\": 0.45, \"w2_experience_alignment\": 0.35, \"w3_ats_score\": 0.20},\n",
    "                \"components\": {\"skill_overlap\": 0.0, \"experience_alignment\": 0.0, \"ats_score\": float(ats), \"market_competition_factor\": 1.0},\n",
    "                \"interview_chance_score\": float(0.20*ats),\n",
    "            },\n",
    "        )\n",
    "''')\n",
    "\n",
    "# -------------------------\n",
    "# orchestration/engine.py (FULL automation: L2 intake → L3 hunt 8 boards → L4 scrape+match → L5 rank gate → L6 drafts → learning plan)\n",
    "# -------------------------\n",
    "backup_write(\"src/careeragent/orchestration/engine.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from uuid import uuid4\n",
    "\n",
    "import httpx\n",
    "\n",
    "from careeragent.config import artifacts_root, get_settings\n",
    "from careeragent.orchestration.state import OrchestrationState\n",
    "from careeragent.services.db_service import SqliteStateStore\n",
    "from careeragent.services.health_service import HealthService\n",
    "from careeragent.services.notification_service import NotificationService\n",
    "from careeragent.services.learning_resource_service import LearningResourceService\n",
    "\n",
    "from careeragent.agents.security_agent import SanitizeAgent\n",
    "from careeragent.agents.parser_agent_service import ParserAgentService, ExtractedResume\n",
    "from careeragent.agents.parser_evaluator_service import ParserEvaluatorService\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class JobBoard:\n",
    "    name: str\n",
    "    domain: str\n",
    "\n",
    "\n",
    "DEFAULT_JOB_BOARDS: Tuple[JobBoard, ...] = (\n",
    "    JobBoard(\"LinkedIn Jobs\", \"linkedin.com/jobs\"),\n",
    "    JobBoard(\"Indeed\", \"indeed.com\"),\n",
    "    JobBoard(\"Glassdoor\", \"glassdoor.com\"),\n",
    "    JobBoard(\"ZipRecruiter\", \"ziprecruiter.com\"),\n",
    "    JobBoard(\"Monster\", \"monster.com\"),\n",
    "    JobBoard(\"Dice\", \"dice.com\"),\n",
    "    JobBoard(\"Lever\", \"jobs.lever.co\"),\n",
    "    JobBoard(\"Greenhouse\", \"boards.greenhouse.io\"),\n",
    ")\n",
    "\n",
    "VISA_NEGATIVE = (\"unable to sponsor\",\"cannot sponsor\",\"no sponsorship\",\"do not sponsor\",\"not sponsor\",\"without sponsorship\",\"no visa\",\"cannot provide visa\")\n",
    "VISA_POSITIVE = (\"visa sponsorship\",\"h1b\",\"opt\",\"cpt\",\"stem opt\",\"work visa\")\n",
    "\n",
    "COMMON_SKILL_DICTIONARY = [\n",
    "    \"python\",\"sql\",\"fastapi\",\"docker\",\"kubernetes\",\"mlflow\",\"dvc\",\"aws\",\"azure\",\"gcp\",\"pytorch\",\n",
    "    \"tensorflow\",\"scikit-learn\",\"pandas\",\"numpy\",\"spark\",\"databricks\",\"snowflake\",\"airflow\",\"kafka\",\n",
    "    \"langchain\",\"langgraph\",\"rag\",\"faiss\",\"chroma\",\"terraform\",\"github actions\",\"postgres\",\"redis\"\n",
    "]\n",
    "\n",
    "\n",
    "def _run_dir(run_id: str) -> Path:\n",
    "    d = artifacts_root() / \"runs\" / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _save_json(p: Path, obj: Any) -> None:\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "class LiveFeed:\n",
    "    @staticmethod\n",
    "    def emit(st: OrchestrationState, *, layer: str, agent: str, message: str) -> None:\n",
    "        st.meta.setdefault(\"live_feed\", [])\n",
    "        st.meta[\"live_feed\"].append({\"layer\": layer, \"agent\": agent, \"message\": message})\n",
    "        st.touch()\n",
    "\n",
    "\n",
    "class LocalResumeExtractor:\n",
    "    @staticmethod\n",
    "    def extract_text(*, filename: str, data: bytes) -> str:\n",
    "        name = (filename or \"\").lower()\n",
    "        if name.endswith(\".txt\"):\n",
    "            return data.decode(\"utf-8\", errors=\"replace\")\n",
    "        if name.endswith(\".pdf\"):\n",
    "            from pypdf import PdfReader  # type: ignore\n",
    "            import io\n",
    "            reader = PdfReader(io.BytesIO(data))\n",
    "            return \"\\n\".join([(pg.extract_text() or \"\") for pg in reader.pages])\n",
    "        if name.endswith(\".docx\"):\n",
    "            import docx  # type: ignore\n",
    "            import io\n",
    "            d = docx.Document(io.BytesIO(data))\n",
    "            return \"\\n\".join([p.text for p in d.paragraphs if p.text])\n",
    "        return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "class SerperDiscovery:\n",
    "    SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "    def __init__(self, *, api_key: str, health: HealthService) -> None:\n",
    "        self._key = api_key\n",
    "        self._health = health\n",
    "\n",
    "    def search(self, *, st: OrchestrationState, step_id: str, query: str, num: int = 10, tbs: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        headers = {\"X-API-KEY\": self._key, \"Content-Type\": \"application/json\"}\n",
    "        body: Dict[str, Any] = {\"q\": query, \"num\": num}\n",
    "        if tbs:\n",
    "            body[\"tbs\"] = tbs\n",
    "        with httpx.Client(timeout=30.0) as client:\n",
    "            r = client.post(self.SERPER_URL, headers=headers, json=body)\n",
    "\n",
    "        if self._health.quota.handle_serper_response(\n",
    "            state=st, step_id=step_id, status_code=r.status_code, tool_name=\"serper.search\", error_detail=r.text[:200]\n",
    "        ):\n",
    "            return []\n",
    "\n",
    "        if r.status_code >= 400:\n",
    "            st.status = \"api_failure\"\n",
    "            st.meta[\"run_failure_code\"] = \"API_FAILURE\"\n",
    "            st.meta[\"run_failure_provider\"] = \"serper\"\n",
    "            st.meta[\"run_failure_detail\"] = r.text[:200]\n",
    "            return []\n",
    "\n",
    "        organic = (r.json().get(\"organic\") or [])\n",
    "        out = []\n",
    "        for it in organic:\n",
    "            out.append({\"title\": it.get(\"title\") or \"\", \"link\": it.get(\"link\") or \"\", \"snippet\": it.get(\"snippet\") or \"\"})\n",
    "        return out\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    @staticmethod\n",
    "    def fetch_text(*, url: str, snippet: str) -> str:\n",
    "        if not url:\n",
    "            return snippet or \"\"\n",
    "        try:\n",
    "            with httpx.Client(timeout=18.0, follow_redirects=True) as client:\n",
    "                r = client.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "            if r.status_code >= 400:\n",
    "                return snippet or \"\"\n",
    "            html = r.text\n",
    "            txt = re.sub(r\"<(script|style)[^>]*>.*?</\\\\1>\", \" \", html, flags=re.S|re.I)\n",
    "            txt = re.sub(r\"<[^>]+>\", \" \", txt)\n",
    "            txt = re.sub(r\"\\\\s+\", \" \", txt).strip()\n",
    "            return txt[:16000] if txt else (snippet or \"\")\n",
    "        except Exception:\n",
    "            return snippet or \"\"\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-zA-Z][a-zA-Z0-9\\\\+\\\\#\\\\.-]{1,}\", (text or \"\").lower())\n",
    "\n",
    "\n",
    "def cosine(a: Dict[str,int], b: Dict[str,int]) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    dot = sum(v * b.get(k,0) for k,v in a.items())\n",
    "    na = sum(v*v for v in a.values()) ** 0.5\n",
    "    nb = sum(v*v for v in b.values()) ** 0.5\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return float(dot/(na*nb))\n",
    "\n",
    "\n",
    "def recency_hours(snippet: str) -> Optional[float]:\n",
    "    s = (snippet or \"\").lower()\n",
    "    if \"today\" in s:\n",
    "        return 6.0\n",
    "    if \"yesterday\" in s:\n",
    "        return 24.0\n",
    "    m = re.search(r\"(\\\\d+)\\\\s*hours?\\\\s*ago\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "    m = re.search(r\"(\\\\d+)\\\\s*days?\\\\s*ago\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))*24.0\n",
    "    return None\n",
    "\n",
    "\n",
    "def ats_score(resume_text: str) -> float:\n",
    "    t = (resume_text or \"\").lower()\n",
    "    score = 0.0\n",
    "    if re.search(r\"[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+\", t): score += 0.20\n",
    "    if re.search(r\"\\\\+?\\\\d[\\\\d\\\\-\\\\s\\\\(\\\\)]{8,}\\\\d\", t): score += 0.10\n",
    "    for h in [\"summary\",\"skills\",\"experience\",\"education\",\"projects\"]:\n",
    "        if h in t: score += 0.12\n",
    "    if \"-\" in resume_text or \"•\" in resume_text: score += 0.10\n",
    "    if len(resume_text) > 1200: score += 0.12\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "def compute_interview_chance(skill_overlap: float, exp_align: float, ats_s: float, market: float) -> float:\n",
    "    market = max(1.0, float(market))\n",
    "    score = (0.45*skill_overlap + 0.35*exp_align + 0.20*ats_s) / market\n",
    "    return max(0.0, min(1.0, float(score)))\n",
    "\n",
    "\n",
    "def extract_job_skills(job_text: str, resume_skills: List[str]) -> List[str]:\n",
    "    low = (job_text or \"\").lower()\n",
    "    pool = list(dict.fromkeys([*(resume_skills or []), *COMMON_SKILL_DICTIONARY]))\n",
    "    found = []\n",
    "    for s in pool:\n",
    "        s2 = str(s).lower().strip()\n",
    "        if s2 and s2 in low:\n",
    "            found.append(s2)\n",
    "    return list(dict.fromkeys(found))[:40]\n",
    "\n",
    "\n",
    "class OneClickAutomationEngine:\n",
    "    \"\"\"\n",
    "    Description: Full Career Operating System loop (L0-L9).\n",
    "    Layer: L0-L9\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._s = get_settings()\n",
    "        self._store = SqliteStateStore()\n",
    "        self._health = HealthService()\n",
    "        self._notifier = NotificationService(dry_run=not bool(getattr(self._s, \"twilio_account_sid\", None)))\n",
    "        self._sanitize = SanitizeAgent()\n",
    "        self._parser = ParserAgentService()\n",
    "        self._parser_eval = ParserEvaluatorService()\n",
    "        self._learn = LearningResourceService(serper_api_key=getattr(self._s, \"serper_api_key\", None))\n",
    "\n",
    "    def _persist(self, st: OrchestrationState) -> None:\n",
    "        d = _run_dir(st.run_id)\n",
    "        _save_json(d / \"state.json\", st.model_dump())\n",
    "        self._store.upsert_state(run_id=st.run_id, status=st.status, state=st.model_dump(), updated_at_utc=st.updated_at_utc)\n",
    "\n",
    "    def load(self, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        return self._store.get_state(run_id=run_id)\n",
    "\n",
    "    def start_run(self, *, filename: str, data: bytes, prefs: Dict[str, Any]) -> OrchestrationState:\n",
    "        st = OrchestrationState.new(env=self._s.environment, mode=\"agentic\", git_sha=None)\n",
    "        st.meta[\"preferences\"] = prefs\n",
    "        st.meta.setdefault(\"job_scores\", {})\n",
    "        st.meta.setdefault(\"job_components\", {})\n",
    "        st.meta.setdefault(\"job_meta\", {})\n",
    "        LiveFeed.emit(st, layer=\"L1\", agent=\"Dashboard\", message=\"Run created. Starting autonomous pipeline…\")\n",
    "        self._persist(st)\n",
    "        t = threading.Thread(target=self._run, args=(st.run_id, filename, data), daemon=True)\n",
    "        t.start()\n",
    "        return st\n",
    "\n",
    "    def submit_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any]) -> OrchestrationState:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            raise ValueError(\"run_id not found\")\n",
    "        st = OrchestrationState(**raw)\n",
    "        st.meta[\"last_user_action\"] = {\"type\": action_type, \"payload\": payload}\n",
    "        LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=f\"User action received: {action_type}\")\n",
    "        self._persist(st)\n",
    "        t = threading.Thread(target=self._continue, args=(run_id,), daemon=True)\n",
    "        t.start()\n",
    "        return st\n",
    "\n",
    "    # ---------------- main loop ----------------\n",
    "    def _run(self, run_id: str, filename: str, data: bytes) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        st = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "        prefs = st.meta.get(\"preferences\", {}) or {}\n",
    "\n",
    "        # L2 extract\n",
    "        st.start_step(\"l2_extract\", layer_id=\"L2\", tool_name=\"ResumeExtractor\", input_ref={\"filename\": filename})\n",
    "        LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=\"Extracting resume text from upload…\")\n",
    "        resume_text = LocalResumeExtractor.extract_text(filename=filename, data=data)\n",
    "        (run_dir / \"resume_raw.txt\").write_text(resume_text, encoding=\"utf-8\")\n",
    "        st.add_artifact(\"resume_raw\", str(run_dir / \"resume_raw.txt\"), content_type=\"text/plain\")\n",
    "        st.end_step(\"l2_extract\", status=\"ok\", output_ref={\"artifact_key\":\"resume_raw\"}, message=\"extracted\")\n",
    "\n",
    "        # L0 sanitize\n",
    "        st.start_step(\"l0_sanitize\", layer_id=\"L0\", tool_name=\"SanitizeAgent\", input_ref={})\n",
    "        safe = self._sanitize.sanitize_before_llm(\n",
    "            state=st, step_id=\"l0_sanitize\", tool_name=\"sanitize_before_llm\", user_text=resume_text, context=\"resume\"\n",
    "        )\n",
    "        if safe is None:\n",
    "            st.status = \"blocked\"\n",
    "            LiveFeed.emit(st, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Prompt injection detected. Run blocked.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "        st.end_step(\"l0_sanitize\", status=\"ok\", output_ref={\"sanitized\": True}, message=\"pass\")\n",
    "        LiveFeed.emit(st, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Security check passed.\")\n",
    "\n",
    "        # L2 intake bundle\n",
    "        st.start_step(\"l2_parse\", layer_id=\"L2\", tool_name=\"ParserAgentService\", input_ref={})\n",
    "        extracted = self._parser.parse(raw_text=safe, orchestration_state=st, feedback=[])\n",
    "        _save_json(run_dir / \"intake_bundle.json\", extracted.to_json_dict())\n",
    "        st.add_artifact(\"intake_bundle\", str(run_dir / \"intake_bundle.json\"), content_type=\"application/json\")\n",
    "        st.end_step(\"l2_parse\", status=\"ok\", output_ref={\"artifact_key\":\"intake_bundle\"}, message=\"parsed\")\n",
    "\n",
    "        # L3 eval (soft)\n",
    "        ev = self._parser_eval.evaluate(orchestration_state=st, raw_text=safe, extracted=extracted, target_id=\"resume_main\", threshold=float(prefs.get(\"resume_threshold\", 0.55)))\n",
    "        score = float(getattr(ev, \"evaluation_score\", 0.0))\n",
    "        LiveFeed.emit(st, layer=\"L3\", agent=\"ParserEvaluator\", message=f\"Intake quality={score:.2f}. Continuing automation.\")\n",
    "\n",
    "        # Do NOT hard stop. Only mark optional cleanup.\n",
    "        if score < 0.55:\n",
    "            st.meta[\"pending_action\"] = \"resume_cleanup_optional\"\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=\"Optional: improve resume text for better ATS + scoring. Pipeline continues.\")\n",
    "\n",
    "        self._persist(st)\n",
    "\n",
    "        # L3 job hunt across 8 boards for multiple roles\n",
    "        roles = prefs.get(\"target_roles\") or []\n",
    "        if isinstance(roles, str):\n",
    "            roles = [r.strip() for r in roles.splitlines() if r.strip()]\n",
    "        if not roles:\n",
    "            roles = [str(prefs.get(\"target_role\",\"Data Scientist\"))]\n",
    "\n",
    "        location = str(prefs.get(\"location\",\"United States\"))\n",
    "        remote = bool(prefs.get(\"remote\", True))\n",
    "        wfo_ok = bool(prefs.get(\"wfo_ok\", True))\n",
    "        salary = str(prefs.get(\"salary\",\"\")).strip()\n",
    "        visa_required = bool(prefs.get(\"visa_sponsorship_required\", False))\n",
    "        recency_limit = float(prefs.get(\"recency_hours\", 36))\n",
    "        max_jobs = int(prefs.get(\"max_jobs\", 40))\n",
    "        discovery_threshold = float(prefs.get(\"discovery_threshold\", 0.70))\n",
    "        max_refinements = int(prefs.get(\"max_refinements\", 3))\n",
    "\n",
    "        serper_key = getattr(self._s, \"serper_api_key\", None)\n",
    "        if not serper_key:\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"missing_serper_key\"\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=\"Missing SERPER_API_KEY in .env.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        discovery = SerperDiscovery(api_key=serper_key, health=self._health)\n",
    "\n",
    "        resume_skills = [s.lower() for s in (extracted.skills or [])]\n",
    "        resume_ats = ats_score(safe)\n",
    "        resume_tokens = {}\n",
    "        for t in tokenize(safe):\n",
    "            resume_tokens[t] = resume_tokens.get(t, 0) + 1\n",
    "        exp_text = \" \".join([\" \".join(x.bullets) for x in (extracted.experience or [])]) or safe\n",
    "        exp_tokens = {}\n",
    "        for t in tokenize(exp_text):\n",
    "            exp_tokens[t] = exp_tokens.get(t, 0) + 1\n",
    "\n",
    "        # query builder\n",
    "        def build_query(role: str) -> str:\n",
    "            intent = []\n",
    "            if remote: intent.append(\"remote\")\n",
    "            if wfo_ok: intent.append(\"hybrid\")\n",
    "            if not intent: intent.append(\"on-site\")\n",
    "            salary_part = f'\"{salary}\"' if salary else \"\"\n",
    "            visa_part = '\"visa sponsorship\" OR h1b OR opt OR cpt' if visa_required else \"\"\n",
    "            skill_hint = \" \".join(resume_skills[:6])\n",
    "            return f'{role} {location} {\" \".join(intent)} {salary_part} {skill_hint} ({visa_part}) apply'\n",
    "\n",
    "        # tbs recency (best-effort)\n",
    "        tbs = \"qdr:d\" if recency_limit <= 36 else None\n",
    "\n",
    "        # refinement loop\n",
    "        query_variants = [build_query(r) for r in roles[:4]]\n",
    "        all_results: List[Dict[str, Any]] = []\n",
    "\n",
    "        for attempt in range(max_refinements + 1):\n",
    "            LiveFeed.emit(st, layer=\"L3\", agent=\"DiscoveryAgent\", message=f\"Hunt attempt {attempt+1}: searching 8 boards for {len(query_variants)} roles…\")\n",
    "            seen = set()\n",
    "            all_results = []\n",
    "            for qi, q in enumerate(query_variants):\n",
    "                for b in DEFAULT_JOB_BOARDS:\n",
    "                    step_id = f\"l3_serper_{attempt+1}_{qi}_{b.domain.replace('/','_')}\"\n",
    "                    st.start_step(step_id, layer_id=\"L3\", tool_name=\"serper.search\", input_ref={\"role_query\": q, \"board\": b.name})\n",
    "                    items = discovery.search(st=st, step_id=step_id, query=f\"{q} site:{b.domain}\", num=10, tbs=tbs)\n",
    "                    st.end_step(step_id, status=\"ok\", output_ref={\"count\": len(items)}, message=b.name)\n",
    "                    for it in items:\n",
    "                        link = it.get(\"link\") or \"\"\n",
    "                        if not link or link in seen:\n",
    "                            continue\n",
    "                        seen.add(link)\n",
    "                        it[\"board\"] = b.name\n",
    "                        it[\"role_hint\"] = roles[min(qi, len(roles)-1)]\n",
    "                        all_results.append(it)\n",
    "\n",
    "            # L4 scrape+score max_jobs\n",
    "            LiveFeed.emit(st, layer=\"L4\", agent=\"ScraperAgent\", message=f\"Scraping + scoring up to {max_jobs} jobs…\")\n",
    "            ranked: List[Dict[str, Any]] = []\n",
    "            for idx, it in enumerate(all_results[:max_jobs]):\n",
    "                url = it.get(\"link\") or \"\"\n",
    "                snippet = it.get(\"snippet\") or \"\"\n",
    "                title = it.get(\"title\") or \"\"\n",
    "                board = it.get(\"board\") or \"unknown\"\n",
    "                rh = recency_hours(snippet)\n",
    "                if rh is not None and rh > recency_limit:\n",
    "                    continue\n",
    "\n",
    "                step_id = f\"l4_scrape_{attempt+1}_{idx+1}\"\n",
    "                st.start_step(step_id, layer_id=\"L4\", tool_name=\"Scraper\", input_ref={\"url\": url, \"board\": board})\n",
    "                text = Scraper.fetch_text(url=url, snippet=snippet)\n",
    "                st.end_step(step_id, status=\"ok\", output_ref={\"chars\": len(text)}, message=\"scraped\")\n",
    "\n",
    "                low = text.lower()\n",
    "                visa_ok = not any(x in low for x in VISA_NEGATIVE)\n",
    "                if visa_required and not visa_ok:\n",
    "                    continue\n",
    "\n",
    "                job_skills = extract_job_skills(text, resume_skills)\n",
    "                overlap = len(set(job_skills) & set(resume_skills)) / max(1, len(set(job_skills))) if job_skills else 0.0\n",
    "\n",
    "                job_tokens = {}\n",
    "                for t in tokenize(text):\n",
    "                    job_tokens[t] = job_tokens.get(t, 0) + 1\n",
    "                exp_align = cosine(exp_tokens, job_tokens)\n",
    "\n",
    "                market = 1.0\n",
    "                if \"applicants\" in snippet.lower():\n",
    "                    m = re.search(r\"(\\\\d+)\\\\+?\\\\s*applicants\", snippet.lower())\n",
    "                    if m:\n",
    "                        n = int(m.group(1))\n",
    "                        market = 1.0 + min(1.5, n/200.0)\n",
    "\n",
    "                score = compute_interview_chance(overlap, exp_align, resume_ats, market)\n",
    "                if visa_required and any(x in low for x in VISA_POSITIVE):\n",
    "                    score = min(1.0, score + 0.05)\n",
    "\n",
    "                missing = [s for s in job_skills if s not in resume_skills][:12]\n",
    "                jid = url or uuid4().hex\n",
    "\n",
    "                ranked.append({\n",
    "                    \"job_id\": jid,\n",
    "                    \"rank\": 0,\n",
    "                    \"role_hint\": it.get(\"role_hint\"),\n",
    "                    \"title\": title,\n",
    "                    \"board\": board,\n",
    "                    \"url\": url,\n",
    "                    \"recency_hours\": rh,\n",
    "                    \"visa_ok\": visa_ok,\n",
    "                    \"matched_skills\": list(set(job_skills) & set(resume_skills))[:12],\n",
    "                    \"missing_skills\": missing,\n",
    "                    \"components\": {\n",
    "                        \"skill_overlap\": overlap,\n",
    "                        \"experience_alignment\": exp_align,\n",
    "                        \"ats_score\": resume_ats,\n",
    "                        \"market_competition_factor\": market,\n",
    "                    },\n",
    "                    \"interview_chance_score\": score,\n",
    "                    \"overall_match_percent\": round(score*100.0, 2),\n",
    "                    \"rationale\": [\n",
    "                        f\"SkillOverlap={overlap:.2f}\",\n",
    "                        f\"ExperienceAlignment={exp_align:.2f}\",\n",
    "                        f\"ATS={resume_ats:.2f}\",\n",
    "                        f\"MarketFactor={market:.2f}\",\n",
    "                        (\"VisaOK\" if visa_ok else \"NoSponsorship\"),\n",
    "                    ],\n",
    "                })\n",
    "\n",
    "                st.meta[\"job_scores\"][jid] = float(score)\n",
    "                st.meta[\"job_components\"][jid] = ranked[-1][\"components\"]\n",
    "                st.meta[\"job_meta\"][jid] = {\"role_title\": title, \"company\": board, \"url\": url, \"source\": board}\n",
    "\n",
    "            ranked.sort(key=lambda x: float(x[\"interview_chance_score\"]), reverse=True)\n",
    "            for i, r in enumerate(ranked, start=1):\n",
    "                r[\"rank\"] = i\n",
    "\n",
    "            _save_json(run_dir / \"ranking.json\", ranked)\n",
    "            st.add_artifact(\"ranking\", str(run_dir / \"ranking.json\"), content_type=\"application/json\")\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=f\"Ranked {len(ranked)} jobs. Top={ranked[0]['overall_match_percent'] if ranked else 'n/a'}%\")\n",
    "\n",
    "            # Gate decision\n",
    "            top_score = float(ranked[0][\"interview_chance_score\"]) if ranked else 0.0\n",
    "            if top_score >= discovery_threshold and len(ranked) >= min(20, max_jobs):\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"review_ranking\"\n",
    "                LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=\"Ranking ready for approval (HITL).\")\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            if attempt >= max_refinements:\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"low_confidence_discovery\"\n",
    "                LiveFeed.emit(st, layer=\"L5\", agent=\"EvaluatorAgent\", message=\"Low confidence after retries. Needs guidance.\")\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            # refine queries (simple deterministic refinement)\n",
    "            if ranked:\n",
    "                top = ranked[0]\n",
    "                hint = \" \".join((top.get(\"matched_skills\") or [])[:6])\n",
    "            else:\n",
    "                hint = \" \".join(resume_skills[:6])\n",
    "            query_variants = [f\"{r} {location} {hint} apply\" for r in roles[:4]]\n",
    "            self._persist(st)\n",
    "\n",
    "    def _continue(self, run_id: str) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        st = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "        pending = st.meta.get(\"pending_action\")\n",
    "        action = (st.meta.get(\"last_user_action\") or {}).get(\"type\")\n",
    "        payload = (st.meta.get(\"last_user_action\") or {}).get(\"payload\") or {}\n",
    "\n",
    "        # Optional resume improvement loop\n",
    "        if pending == \"resume_cleanup_optional\" and action == \"resume_cleanup_submit\":\n",
    "            new_text = str(payload.get(\"resume_text\",\"\")).strip()\n",
    "            if new_text:\n",
    "                (run_dir / \"resume_raw.txt\").write_text(new_text, encoding=\"utf-8\")\n",
    "                st.add_artifact(\"resume_raw\", str(run_dir / \"resume_raw.txt\"), content_type=\"text/plain\")\n",
    "                st.status = \"running\"\n",
    "                st.meta[\"pending_action\"] = None\n",
    "                LiveFeed.emit(st, layer=\"L2\", agent=\"ParserAgent\", message=\"Resume updated. Restarting run…\")\n",
    "                self._persist(st)\n",
    "                self._run(run_id, \"resume.txt\", new_text.encode(\"utf-8\"))\n",
    "            return\n",
    "\n",
    "        # Approve ranking -> generate drafts + learning plan for missing skills\n",
    "        if pending == \"review_ranking\" and action == \"approve_ranking\":\n",
    "            ranking_path = run_dir / \"ranking.json\"\n",
    "            intake_path = run_dir / \"intake_bundle.json\"\n",
    "            if not (ranking_path.exists() and intake_path.exists()):\n",
    "                st.status = \"needs_human_approval\"\n",
    "                st.meta[\"pending_action\"] = \"missing_artifacts\"\n",
    "                self._persist(st)\n",
    "                return\n",
    "\n",
    "            ranked = json.loads(ranking_path.read_text(encoding=\"utf-8\"))\n",
    "            intake = ExtractedResume(**json.loads(intake_path.read_text(encoding=\"utf-8\")))\n",
    "            top_n = int((st.meta.get(\"preferences\", {}) or {}).get(\"draft_count\", 10))\n",
    "            ranked = ranked[:top_n]\n",
    "\n",
    "            # Drafts (ATS resume + cover letter) + learning plan\n",
    "            drafts: List[Dict[str, Any]] = []\n",
    "            learning: Dict[str, Any] = {}\n",
    "\n",
    "            # ATS resume template built from intake (no user ATS required)\n",
    "            base_resume = self._build_ats_resume(intake)\n",
    "            (run_dir / \"ats_resume_base.md\").write_text(base_resume, encoding=\"utf-8\")\n",
    "            st.add_artifact(\"ats_resume_base\", str(run_dir / \"ats_resume_base.md\"), content_type=\"text/markdown\")\n",
    "\n",
    "            for j in ranked:\n",
    "                jid = j[\"job_id\"]\n",
    "                title = j.get(\"title\") or \"Role\"\n",
    "                company = j.get(\"board\") or \"Company\"\n",
    "                missing = j.get(\"missing_skills\") or []\n",
    "\n",
    "                # Tailored resume (keyword injection + bullets)\n",
    "                tailored = self._tailor_resume(base_resume, title, company, j.get(\"matched_skills\") or [], missing)\n",
    "                resume_path = run_dir / f\"resume_{jid[:10]}.md\"\n",
    "                resume_path.write_text(tailored, encoding=\"utf-8\")\n",
    "                st.add_artifact(f\"resume_{jid[:10]}\", str(resume_path), content_type=\"text/markdown\")\n",
    "\n",
    "                # Cover letter\n",
    "                cover = self._cover_letter(intake, title, company, j.get(\"matched_skills\") or [])\n",
    "                cover_path = run_dir / f\"cover_{jid[:10]}.md\"\n",
    "                cover_path.write_text(cover, encoding=\"utf-8\")\n",
    "                st.add_artifact(f\"cover_{jid[:10]}\", str(cover_path), content_type=\"text/markdown\")\n",
    "\n",
    "                # Learning plan for missing skills\n",
    "                if missing:\n",
    "                    learning[jid] = self._learn.build_learning_plan(missing_skills=missing)\n",
    "\n",
    "                drafts.append({\n",
    "                    \"job_id\": jid,\n",
    "                    \"title\": title,\n",
    "                    \"company\": company,\n",
    "                    \"url\": j.get(\"url\"),\n",
    "                    \"resume_path\": str(resume_path),\n",
    "                    \"cover_path\": str(cover_path),\n",
    "                    \"missing_skills\": missing,\n",
    "                })\n",
    "\n",
    "            _save_json(run_dir / \"drafts_bundle.json\", {\"drafts\": drafts, \"learning_plan\": learning})\n",
    "            st.add_artifact(\"drafts_bundle\", str(run_dir / \"drafts_bundle.json\"), content_type=\"application/json\")\n",
    "\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"review_drafts\"\n",
    "            LiveFeed.emit(st, layer=\"L6\", agent=\"DraftAgent\", message=f\"Generated {len(drafts)} resume+cover packages + learning plans.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        # Approve drafts -> mark completed (apply simulation can be added later)\n",
    "        if pending == \"review_drafts\" and action == \"approve_drafts\":\n",
    "            st.status = \"completed\"\n",
    "            st.meta[\"pending_action\"] = None\n",
    "            LiveFeed.emit(st, layer=\"L7\", agent=\"ApplyExecutor\", message=\"Drafts approved. (Simulated) submission completed.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        # Reject ranking -> rerun discovery with refined hint\n",
    "        if pending == \"review_ranking\" and action == \"reject_ranking\":\n",
    "            reason = str(payload.get(\"reason\",\"\")).strip()\n",
    "            st.meta.setdefault(\"user_refinement_notes\", [])\n",
    "            if reason:\n",
    "                st.meta[\"user_refinement_notes\"].append(reason)\n",
    "            st.status = \"running\"\n",
    "            st.meta[\"pending_action\"] = None\n",
    "            LiveFeed.emit(st, layer=\"L5\", agent=\"HITL\", message=\"Ranking rejected. Re-running discovery…\")\n",
    "            self._persist(st)\n",
    "\n",
    "            resume_path = run_dir / \"resume_raw.txt\"\n",
    "            if resume_path.exists():\n",
    "                self._run(run_id, \"resume.txt\", resume_path.read_bytes())\n",
    "            return\n",
    "\n",
    "        # Reject drafts -> go back to ranking approval\n",
    "        if pending == \"review_drafts\" and action == \"reject_drafts\":\n",
    "            st.status = \"needs_human_approval\"\n",
    "            st.meta[\"pending_action\"] = \"review_ranking\"\n",
    "            LiveFeed.emit(st, layer=\"L6\", agent=\"HITL\", message=\"Drafts rejected. Returning to ranking review.\")\n",
    "            self._persist(st)\n",
    "            return\n",
    "\n",
    "        self._persist(st)\n",
    "\n",
    "    # -------- ATS Resume generation (no user ATS needed) --------\n",
    "    def _build_ats_resume(self, intake: ExtractedResume) -> str:\n",
    "        email = intake.contact.email or \"\"\n",
    "        phone = intake.contact.phone or \"\"\n",
    "        linkedin = intake.contact.linkedin or \"\"\n",
    "        github = intake.contact.github or \"\"\n",
    "        name = intake.name or \"Candidate\"\n",
    "\n",
    "        skills = \", \".join((intake.skills or [])[:25])\n",
    "        bullets = []\n",
    "        if intake.experience and intake.experience[0].bullets:\n",
    "            bullets = intake.experience[0].bullets[:8]\n",
    "\n",
    "        bullets_md = \"\\n\".join([f\"- {b}\" for b in bullets]) if bullets else \"- Add 4–6 bullets with measurable impact (metrics, scope, tools).\"\n",
    "\n",
    "        return f\"\"\"# {name}\n",
    "{email} | {phone} | {linkedin} | {github}\n",
    "\n",
    "## Summary\n",
    "{intake.summary or \"AI/ML professional focused on production-grade GenAI and MLOps systems.\"}\n",
    "\n",
    "## Skills\n",
    "{skills}\n",
    "\n",
    "## Experience\n",
    "{bullets_md}\n",
    "\n",
    "## Education\n",
    "- (Auto-filled from resume intake)\n",
    "\"\"\"\n",
    "\n",
    "    def _tailor_resume(self, base: str, title: str, company: str, matched: List[str], missing: List[str]) -> str:\n",
    "        matched_str = \", \".join(matched[:12])\n",
    "        missing_str = \", \".join(missing[:8])\n",
    "        return base + f\"\\n\\n## Target Role Alignment\\n- Target Role: {title} @ {company}\\n- Matched Keywords: {matched_str}\\n- Gap Keywords (learning plan attached): {missing_str}\\n\"\n",
    "\n",
    "    def _cover_letter(self, intake: ExtractedResume, title: str, company: str, matched: List[str]) -> str:\n",
    "        name = intake.name or \"Candidate\"\n",
    "        email = intake.contact.email or \"\"\n",
    "        kw = \", \".join(matched[:8])\n",
    "        return f\"\"\"{name}\n",
    "{email}\n",
    "\n",
    "Dear Hiring Manager,\n",
    "\n",
    "I’m applying for the {title} role. My background includes production-grade AI/ML delivery, GenAI application development, and MLOps practices (CI/CD, reproducible pipelines, evaluation, and governance).\n",
    "\n",
    "I’m especially aligned to this role through: {kw}.\n",
    "\n",
    "I’d welcome a quick conversation on how I can help {company} deliver measurable AI impact.\n",
    "\n",
    "Sincerely,  \n",
    "{name}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ENGINE = OneClickAutomationEngine()\n",
    "''')\n",
    "\n",
    "# Export ENGINE\n",
    "backup_write(\"src/careeragent/orchestration/__init__.py\", \"from .state import OrchestrationState\\nfrom .engine import ENGINE, OneClickAutomationEngine\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# app/main.py (bootstrap)\n",
    "# -------------------------\n",
    "backup_write(\"app/main.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "from app.ui.dashboard import main\n",
    "main()\n",
    "''')\n",
    "\n",
    "# -------------------------\n",
    "# app/ui/dashboard.py (multi roles + layer panels + HITL)\n",
    "# -------------------------\n",
    "backup_write(\"app/ui/dashboard.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import streamlit as st\n",
    "\n",
    "REPO_ROOT = Path(__file__).resolve().parents[2]\n",
    "SRC = REPO_ROOT / \"src\"\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "DEFAULT_API = os.getenv(\"API_URL\", \"http://127.0.0.1:8000\")\n",
    "\n",
    "\n",
    "def _safe_json(resp: requests.Response) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return resp.json()\n",
    "    except Exception:\n",
    "        return {\"_raw\": resp.text[:1500], \"_status_code\": resp.status_code}\n",
    "\n",
    "\n",
    "def _api_get(api: str, path: str, timeout: int = 25) -> requests.Response:\n",
    "    return requests.get(f\"{api}{path}\", timeout=timeout)\n",
    "\n",
    "\n",
    "def _api_post(api: str, path: str, timeout: int = 60, **kwargs) -> requests.Response:\n",
    "    return requests.post(f\"{api}{path}\", timeout=timeout, **kwargs)\n",
    "\n",
    "\n",
    "def _exists(path: Optional[str]) -> bool:\n",
    "    return bool(path) and Path(path).exists()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    st.set_page_config(page_title=\"CareerAgent-AI Mission Control\", layout=\"wide\")\n",
    "    st.title(\"CareerAgent-AI — Mission Control (One-Click Automation)\")\n",
    "    st.caption(\"L0→L9: Intake → Hunt → Scrape/Match → Rank → Draft → Learning Plan → HITL approvals\")\n",
    "\n",
    "    api = st.sidebar.text_input(\"API Base URL\", value=DEFAULT_API)\n",
    "\n",
    "    # Backend status\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Backend\")\n",
    "    try:\n",
    "        h = _api_get(api, \"/health\", timeout=3)\n",
    "        st.sidebar.success(\"🟢 API Online\" if h.status_code == 200 else f\"🟠 API issue ({h.status_code})\")\n",
    "    except Exception as e:\n",
    "        st.sidebar.error(\"🔴 API Offline\")\n",
    "        st.sidebar.caption(str(e))\n",
    "        st.stop()\n",
    "\n",
    "    # Upload\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Resume Upload\")\n",
    "    resume_file = st.sidebar.file_uploader(\"Upload Resume (PDF/TXT/DOCX)\", type=[\"pdf\",\"txt\",\"docx\"])\n",
    "\n",
    "    # Preferences\n",
    "    st.sidebar.subheader(\"Targets\")\n",
    "    roles_text = st.sidebar.text_area(\n",
    "        \"Target Roles (1 per line, up to 4)\",\n",
    "        value=\"Data Scientist\\nML Engineer\\nGenAI Engineer\",\n",
    "        height=90\n",
    "    )\n",
    "    target_roles = [r.strip() for r in roles_text.splitlines() if r.strip()][:4]\n",
    "\n",
    "    country = st.sidebar.text_input(\"Country\", value=\"US\")\n",
    "    location = st.sidebar.text_input(\"Location\", value=\"United States\")\n",
    "    remote = st.sidebar.checkbox(\"Remote preferred\", value=True)\n",
    "    wfo_ok = st.sidebar.checkbox(\"On-site/WFO acceptable\", value=True)\n",
    "    salary = st.sidebar.text_input(\"Salary target (optional)\", value=\"\")\n",
    "    visa_required = st.sidebar.checkbox(\"Visa sponsorship required (F1/OPT)\", value=False)\n",
    "\n",
    "    st.sidebar.subheader(\"Job Hunt Controls\")\n",
    "    recency_hours = st.sidebar.slider(\"Only jobs posted within (hours)\", 12, 168, 36, 6)\n",
    "    max_jobs = st.sidebar.slider(\"Jobs to score per run\", 20, 60, 40, 5)\n",
    "\n",
    "    st.sidebar.subheader(\"Quality Gates\")\n",
    "    discovery_threshold = st.sidebar.slider(\"Min top-score to accept ranking\", 0.50, 0.90, 0.70, 0.05)\n",
    "    max_refinements = st.sidebar.slider(\"Max query refinements\", 1, 6, 3, 1)\n",
    "    resume_threshold = st.sidebar.slider(\"Resume quality threshold (soft)\", 0.35, 0.90, 0.55, 0.05)\n",
    "    draft_count = st.sidebar.slider(\"Draft packages (top jobs)\", 3, 20, 10, 1)\n",
    "\n",
    "    st.sidebar.subheader(\"Notifications\")\n",
    "    user_phone = st.sidebar.text_input(\"Phone for SMS (optional)\", value=\"\")\n",
    "\n",
    "    run_btn = st.sidebar.button(\"🚀 RUN ONE-CLICK\", type=\"primary\", use_container_width=True, disabled=(resume_file is None))\n",
    "\n",
    "    st.sidebar.divider()\n",
    "    st.sidebar.subheader(\"Existing Run\")\n",
    "    run_id_in = st.sidebar.text_input(\"Run ID\", value=st.session_state.get(\"run_id\", \"\"))\n",
    "\n",
    "    if run_btn:\n",
    "        prefs = {\n",
    "            \"target_roles\": target_roles,\n",
    "            \"country\": country.strip() or \"US\",\n",
    "            \"location\": location.strip() or \"United States\",\n",
    "            \"remote\": bool(remote),\n",
    "            \"wfo_ok\": bool(wfo_ok),\n",
    "            \"salary\": salary.strip(),\n",
    "            \"visa_sponsorship_required\": bool(visa_required),\n",
    "            \"recency_hours\": float(recency_hours),\n",
    "            \"max_jobs\": int(max_jobs),\n",
    "            \"discovery_threshold\": float(discovery_threshold),\n",
    "            \"max_refinements\": int(max_refinements),\n",
    "            \"resume_threshold\": float(resume_threshold),\n",
    "            \"draft_count\": int(draft_count),\n",
    "            \"user_phone\": user_phone.strip() or None,\n",
    "        }\n",
    "\n",
    "        files = {\"resume\": (resume_file.name, resume_file.getvalue())}\n",
    "        data = {\"preferences_json\": json.dumps(prefs)}\n",
    "        r = _api_post(api, \"/analyze\", files=files, data=data, timeout=180)\n",
    "        if r.status_code >= 400:\n",
    "            st.error(f\"/analyze failed: {r.status_code}\\n\\n{r.text[:1500]}\")\n",
    "            st.stop()\n",
    "        out = _safe_json(r)\n",
    "        st.session_state[\"run_id\"] = out[\"run_id\"]\n",
    "        st.success(f\"Run started: {out['run_id']} (status: {out.get('status')})\")\n",
    "\n",
    "    run_id = st.session_state.get(\"run_id\") or run_id_in.strip()\n",
    "    if not run_id:\n",
    "        st.info(\"Upload resume and click RUN, or paste a run_id to monitor.\")\n",
    "        return\n",
    "\n",
    "    st.button(\"🔄 Refresh\", use_container_width=True)\n",
    "\n",
    "    r = _api_get(api, f\"/status/{run_id}\", timeout=25)\n",
    "    if r.status_code != 200:\n",
    "        st.warning(f\"Run not found yet ({r.status_code}).\")\n",
    "        return\n",
    "    state = _safe_json(r)\n",
    "\n",
    "    status = state.get(\"status\",\"unknown\")\n",
    "    meta = state.get(\"meta\", {}) or {}\n",
    "    pending = meta.get(\"pending_action\")\n",
    "    feed = meta.get(\"live_feed\", []) or []\n",
    "    artifacts = state.get(\"artifacts\", {}) or {}\n",
    "    evals = state.get(\"evaluations\", []) or []\n",
    "\n",
    "    c1,c2,c3 = st.columns([1,1,1])\n",
    "    with c1: st.metric(\"Run ID\", run_id)\n",
    "    with c2: st.metric(\"Status\", status)\n",
    "    with c3: st.metric(\"Pending\", str(pending))\n",
    "\n",
    "    with st.expander(\"Architecture Layers (L0–L9)\"):\n",
    "        st.markdown(\"\"\"\n",
    "- **L0** Security & Guardrails  \n",
    "- **L1** Mission Control (UI)  \n",
    "- **L2** Intake Bundle (Profile extraction)  \n",
    "- **L3** Discovery (8 job boards)  \n",
    "- **L4** Scrape + Match + Score (20–40 jobs)  \n",
    "- **L5** Ranking + Evaluator + HITL gates  \n",
    "- **L6** ATS Resume + Cover Letter generation  \n",
    "- **L7** Apply (simulated for now)  \n",
    "- **L8** Tracking (SQLite snapshots)  \n",
    "- **L9** Learning Plan + Analytics  \n",
    "        \"\"\")\n",
    "\n",
    "    st.markdown(\"### Live Agent Feed\")\n",
    "    for ev in feed[-220:]:\n",
    "        st.write(f\"**[{ev.get('layer')} {ev.get('agent')}]** {ev.get('message')}\")\n",
    "\n",
    "    with st.expander(\"Evaluations (scores + reasons)\"):\n",
    "        if not evals:\n",
    "            st.caption(\"No evaluations yet.\")\n",
    "        else:\n",
    "            for e in evals[-10:]:\n",
    "                st.write(f\"**{e.get('layer_id')} / {e.get('target_id')}** score={e.get('evaluation_score'):.2f} threshold={e.get('threshold')}\")\n",
    "                fb = e.get(\"feedback\") or []\n",
    "                if fb:\n",
    "                    st.write(\"- \" + \"\\n- \".join(fb[:6]))\n",
    "\n",
    "    # Optional resume improvement\n",
    "    if pending == \"resume_cleanup_optional\":\n",
    "        st.markdown(\"## Optional Resume Cleanup (improves ATS + scoring)\")\n",
    "        resume_ref = (artifacts.get(\"resume_raw\") or {}).get(\"path\")\n",
    "        current = \"\"\n",
    "        if resume_ref and Path(resume_ref).exists():\n",
    "            current = Path(resume_ref).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        new_text = st.text_area(\"Paste improved resume text (add email/phone + headings + bullets)\", value=current, height=220)\n",
    "        if st.button(\"✅ Submit improved resume & rerun\"):\n",
    "            rr = _api_post(api, f\"/action/{run_id}\", json={\"action_type\":\"resume_cleanup_submit\",\"payload\":{\"resume_text\":new_text}}, timeout=60)\n",
    "            st.success(\"Submitted. Refresh in a few seconds.\")\n",
    "\n",
    "    # Ranking\n",
    "    st.markdown(\"### Ranking (Top 20–40)\")\n",
    "    ranking_ref = artifacts.get(\"ranking\")\n",
    "    if ranking_ref and _exists(ranking_ref.get(\"path\")):\n",
    "        ranking = json.loads(Path(ranking_ref[\"path\"]).read_text(encoding=\"utf-8\"))\n",
    "        st.dataframe(\n",
    "            [{\n",
    "                \"rank\": x.get(\"rank\"),\n",
    "                \"score_%\": x.get(\"overall_match_percent\"),\n",
    "                \"role_hint\": x.get(\"role_hint\"),\n",
    "                \"title\": x.get(\"title\"),\n",
    "                \"board\": x.get(\"board\"),\n",
    "                \"recency_h\": x.get(\"recency_hours\"),\n",
    "                \"visa_ok\": x.get(\"visa_ok\"),\n",
    "                \"url\": x.get(\"url\"),\n",
    "                \"missing_skills\": \", \".join((x.get(\"missing_skills\") or [])[:6]),\n",
    "            } for x in ranking],\n",
    "            use_container_width=True\n",
    "        )\n",
    "    else:\n",
    "        st.caption(\"Ranking not available yet.\")\n",
    "\n",
    "    # HITL controls\n",
    "    st.markdown(\"### Human-in-the-Loop Controls\")\n",
    "    if status == \"needs_human_approval\" and pending == \"review_ranking\":\n",
    "        c1,c2 = st.columns(2)\n",
    "        with c1:\n",
    "            if st.button(\"✅ Approve Ranking → Generate Draft Packages\", type=\"primary\", use_container_width=True):\n",
    "                _api_post(api, f\"/action/{run_id}\", json={\"action_type\":\"approve_ranking\",\"payload\":{}}, timeout=60)\n",
    "                st.success(\"Approved. Generating ATS resume + cover letters + learning plans…\")\n",
    "        with c2:\n",
    "            reason = st.text_input(\"Reason (optional) to refine ranking\", value=\"\")\n",
    "            if st.button(\"❌ Reject Ranking → Re-run Discovery\", use_container_width=True):\n",
    "                _api_post(api, f\"/action/{run_id}\", json={\"action_type\":\"reject_ranking\",\"payload\":{\"reason\":reason}}, timeout=60)\n",
    "                st.warning(\"Rejected. Discovery rerun started.\")\n",
    "\n",
    "    if status == \"needs_human_approval\" and pending == \"review_drafts\":\n",
    "        c1,c2 = st.columns(2)\n",
    "        with c1:\n",
    "            if st.button(\"✅ Approve Drafts → Complete (Simulated Apply)\", type=\"primary\", use_container_width=True):\n",
    "                _api_post(api, f\"/action/{run_id}\", json={\"action_type\":\"approve_drafts\",\"payload\":{}}, timeout=60)\n",
    "                st.success(\"Approved. Completing run…\")\n",
    "        with c2:\n",
    "            reason = st.text_input(\"Reason (optional) to revise drafts\", value=\"\")\n",
    "            if st.button(\"❌ Reject Drafts → Back to Ranking\", use_container_width=True):\n",
    "                _api_post(api, f\"/action/{run_id}\", json={\"action_type\":\"reject_drafts\",\"payload\":{\"reason\":reason}}, timeout=60)\n",
    "                st.warning(\"Rejected drafts. Returning to ranking.\")\n",
    "\n",
    "    # Drafts bundle preview\n",
    "    st.markdown(\"### Drafts + Learning Plan\")\n",
    "    drafts_ref = artifacts.get(\"drafts_bundle\")\n",
    "    if drafts_ref and _exists(drafts_ref.get(\"path\")):\n",
    "        bundle = json.loads(Path(drafts_ref[\"path\"]).read_text(encoding=\"utf-8\"))\n",
    "        st.json(bundle)\n",
    "    else:\n",
    "        st.caption(\"Draft bundle not available yet.\")\n",
    "\n",
    "    with st.expander(\"Full State JSON\"):\n",
    "        st.json(state)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "''')\n",
    "\n",
    "print(\"\\n✅ FULL AUTOMATION PATCH WRITTEN.\")\n",
    "print(\"Restart backend + frontend.\\n\")\n",
    "print(\"Backend:\")\n",
    "print(\"  PYTHONPATH=src uv run python -m uvicorn careeragent.api.main:app --host 127.0.0.1 --port 8000 --reload\")\n",
    "print(\"Frontend:\")\n",
    "print(\"  API_URL=http://127.0.0.1:8000 PYTHONPATH='.:src' uv run streamlit run app/main.py --server.port 8501\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careeragent-ai (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
