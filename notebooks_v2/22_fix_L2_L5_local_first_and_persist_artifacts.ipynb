{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e544ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/langgraph/runtime_nodes.py.bak_20260221_181355\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/langgraph/runtime_nodes.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/api/run_manager_service.py.bak_20260221_181355\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/api/run_manager_service.py\n",
      "✅ Patched: runtime_nodes L0–L5 are now local-first + L3/L4 persist job URLs + hard timeouts enabled.\n",
      "Restart backend and start a NEW run.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name == \"notebooks_v2\":\n",
    "    os.chdir(ROOT.parent)\n",
    "ROOT = Path.cwd().resolve()\n",
    "assert (ROOT / \"src\").exists(), f\"Not at repo root. CWD={ROOT}\"\n",
    "print(\"✅ CWD =\", ROOT)\n",
    "\n",
    "def backup_write(rel_path: str, content: str) -> None:\n",
    "    p = ROOT / rel_path\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts}\")\n",
    "        bak.write_text(p.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(\"BACKUP:\", bak)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    print(\"WROTE:\", p)\n",
    "\n",
    "# ---------------- runtime_nodes.py (local-first L0-L5 + persist artifacts) ----------------\n",
    "backup_write(\"src/careeragent/langgraph/runtime_nodes.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures as cf\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core helpers\n",
    "# =========================\n",
    "def utc_now() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "def artifacts_root() -> Path:\n",
    "    return Path(\"src/careeragent/artifacts\").resolve()\n",
    "\n",
    "\n",
    "def runs_dir(run_id: str) -> Path:\n",
    "    d = artifacts_root() / \"runs\" / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def daily_dir() -> Path:\n",
    "    day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    d = artifacts_root() / \"daily_jobs\" / day\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def save_json(path: Path, obj: Any) -> str:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "    return str(path)\n",
    "\n",
    "\n",
    "def feed(st: Dict[str, Any], layer: str, agent: str, msg: str) -> None:\n",
    "    st.setdefault(\"live_feed\", [])\n",
    "    st[\"live_feed\"].append({\"layer\": layer, \"agent\": agent, \"message\": msg})\n",
    "\n",
    "\n",
    "def log_attempt(\n",
    "    st: Dict[str, Any],\n",
    "    *,\n",
    "    layer: str,\n",
    "    agent: str,\n",
    "    tool: str,\n",
    "    status: str,\n",
    "    confidence: float,\n",
    "    error: Optional[str] = None,\n",
    "    model: Optional[str] = None,\n",
    ") -> None:\n",
    "    st.setdefault(\"attempts\", [])\n",
    "    st[\"attempts\"].append({\n",
    "        \"layer_id\": layer,\n",
    "        \"agent\": agent,\n",
    "        \"tool\": tool,\n",
    "        \"model\": model,\n",
    "        \"status\": status,\n",
    "        \"confidence\": float(confidence),\n",
    "        \"error\": error,\n",
    "        \"at_utc\": utc_now(),\n",
    "    })\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-zA-Z][a-zA-Z0-9\\+\\#\\.\\-]{1,}\", (text or \"\").lower())\n",
    "\n",
    "\n",
    "def cosine(a: Dict[str, int], b: Dict[str, int]) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    dot = sum(v * b.get(k, 0) for k, v in a.items())\n",
    "    na = math.sqrt(sum(v * v for v in a.values()))\n",
    "    nb = math.sqrt(sum(v * v for v in b.values()))\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return float(dot / (na * nb))\n",
    "\n",
    "\n",
    "def ats_score(resume_text: str) -> float:\n",
    "    t = (resume_text or \"\").lower()\n",
    "    s = 0.0\n",
    "    if re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", t): s += 0.20\n",
    "    if re.search(r\"\\+?\\d[\\d\\-\\s\\(\\)]{8,}\\d\", t): s += 0.10\n",
    "    for h in [\"summary\", \"skills\", \"experience\", \"education\", \"projects\"]:\n",
    "        if h in t: s += 0.12\n",
    "    if \"-\" in resume_text or \"•\" in resume_text: s += 0.10\n",
    "    if len(resume_text) > 1200: s += 0.12\n",
    "    return max(0.0, min(1.0, s))\n",
    "\n",
    "\n",
    "def interview_chance(skill_overlap: float, exp_align: float, ats: float, market: float) -> float:\n",
    "    market = max(1.0, float(market))\n",
    "    score = (0.45 * skill_overlap + 0.35 * exp_align + 0.20 * ats) / market\n",
    "    return max(0.0, min(1.0, float(score)))\n",
    "\n",
    "\n",
    "VISA_NEGATIVE = (\"unable to sponsor\", \"cannot sponsor\", \"no sponsorship\", \"do not sponsor\", \"not sponsor\", \"without sponsorship\", \"no visa\")\n",
    "\n",
    "\n",
    "COMMON_SKILLS = [\n",
    "    \"python\",\"sql\",\"pandas\",\"numpy\",\"scikit-learn\",\"pytorch\",\"tensorflow\",\"spark\",\"mlflow\",\"dvc\",\n",
    "    \"docker\",\"kubernetes\",\"terraform\",\"kafka\",\"airflow\",\"databricks\",\"snowflake\",\n",
    "    \"fastapi\",\"streamlit\",\"langchain\",\"langgraph\",\"rag\",\"faiss\",\"chroma\",\"openai\",\"ollama\",\n",
    "    \"azure\",\"aws\",\"gcp\",\"sagemaker\",\"ecr\",\"ec2\",\"blob\",\"adls\",\"vector\",\"llm\",\"genai\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# L0 (always instant)\n",
    "# =========================\n",
    "def l0_guard(st: Dict[str, Any]) -> None:\n",
    "    txt = (st.get(\"resume_text\") or \"\").lower()\n",
    "    signals = [\"ignore previous instructions\", \"system prompt\", \"developer message\", \"jailbreak\", \"exfiltrate\"]\n",
    "    blocked = any(s in txt for s in signals)\n",
    "    log_attempt(st, layer=\"L0\", agent=\"SanitizeAgent\", tool=\"local.prompt_injection_heuristic\",\n",
    "                status=\"failed\" if blocked else \"ok\", confidence=0.10 if blocked else 0.95,\n",
    "                error=\"prompt_injection\" if blocked else None)\n",
    "    feed(st, \"L0\", \"SanitizeAgent\", \"Security passed.\" if not blocked else \"Blocked: prompt injection detected.\")\n",
    "    if blocked:\n",
    "        st[\"status\"] = \"blocked\"\n",
    "        st[\"pending_action\"] = \"security_blocked\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# L2 (local-first parser, no LLM)\n",
    "# =========================\n",
    "def l2_parse(st: Dict[str, Any]) -> None:\n",
    "    resume = st.get(\"resume_text\") or \"\"\n",
    "    lines = [x.strip() for x in resume.splitlines() if x.strip()]\n",
    "    name = lines[0] if lines else \"Candidate\"\n",
    "\n",
    "    email = \"\"\n",
    "    m = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", resume)\n",
    "    if m: email = m.group(0)\n",
    "\n",
    "    phone = \"\"\n",
    "    m2 = re.search(r\"\\+?\\d[\\d\\-\\s\\(\\)]{8,}\\d\", resume)\n",
    "    if m2: phone = m2.group(0)\n",
    "\n",
    "    linkedin = \"\"\n",
    "    m3 = re.search(r\"https?://(www\\.)?linkedin\\.com/[^\\s]+\", resume)\n",
    "    if m3: linkedin = m3.group(0)\n",
    "\n",
    "    github = \"\"\n",
    "    m4 = re.search(r\"https?://(www\\.)?github\\.com/[^\\s]+\", resume)\n",
    "    if m4: github = m4.group(0)\n",
    "\n",
    "    # Skills: try Skills section first\n",
    "    skills: List[str] = []\n",
    "    lower = resume.lower()\n",
    "    if \"skills\" in lower:\n",
    "        # crude section capture\n",
    "        try:\n",
    "            sidx = lower.index(\"skills\")\n",
    "            sub = resume[sidx:sidx+1500]\n",
    "            tokens = tokenize(sub)\n",
    "            for sk in COMMON_SKILLS:\n",
    "                if sk in tokens and sk not in skills:\n",
    "                    skills.append(sk)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # fallback: scan whole resume for COMMON_SKILLS\n",
    "    if len(skills) < 8:\n",
    "        tokens = set(tokenize(resume))\n",
    "        for sk in COMMON_SKILLS:\n",
    "            if sk in tokens and sk not in skills:\n",
    "                skills.append(sk)\n",
    "\n",
    "    profile = {\n",
    "        \"name\": name,\n",
    "        \"contact\": {\"email\": email, \"phone\": phone, \"linkedin\": linkedin, \"github\": github},\n",
    "        \"skills\": skills[:40],\n",
    "        \"experience\": [],\n",
    "        \"education\": [],\n",
    "    }\n",
    "    st[\"profile\"] = profile\n",
    "\n",
    "    # Persist artifact\n",
    "    rid = str(st.get(\"run_id\") or \"run\")\n",
    "    p = runs_dir(rid) / \"extracted_profile.json\"\n",
    "    st.setdefault(\"artifacts\", {})\n",
    "    st[\"artifacts\"][\"extracted_profile\"] = {\"path\": save_json(p, profile), \"content_type\": \"application/json\"}\n",
    "\n",
    "    log_attempt(st, layer=\"L2\", agent=\"ParserAgent\", tool=\"local.resume_parser\", status=\"ok\",\n",
    "                confidence=0.75 if len(skills) >= 8 else 0.55)\n",
    "    feed(st, \"L2\", \"ParserAgent\", f\"Profile extracted (skills={len(skills)}).\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# L3 (indestructible discovery + persist URLs)\n",
    "# =========================\n",
    "async def serper_search(api_key: str, query: str, num: int = 10, tbs: Optional[str] = None, timeout_s: float = 12.0) -> Tuple[bool, List[Dict[str, Any]], Optional[str]]:\n",
    "    headers = {\"X-API-KEY\": api_key, \"Content-Type\": \"application/json\"}\n",
    "    payload: Dict[str, Any] = {\"q\": query, \"num\": num}\n",
    "    if tbs: payload[\"tbs\"] = tbs\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=timeout_s) as client:\n",
    "            r = await client.post(\"https://google.serper.dev/search\", headers=headers, json=payload)\n",
    "        if r.status_code == 403:\n",
    "            return False, [], \"serper_403_quota\"\n",
    "        if r.status_code >= 400:\n",
    "            return False, [], f\"serper_{r.status_code}\"\n",
    "        organic = (r.json().get(\"organic\") or [])\n",
    "        items = [{\"title\": x.get(\"title\") or \"\", \"url\": x.get(\"link\") or \"\", \"snippet\": x.get(\"snippet\") or \"\"} for x in organic]\n",
    "        return True, items, None\n",
    "    except Exception as e:\n",
    "        return False, [], str(e)\n",
    "\n",
    "\n",
    "def fallback_board_urls(role: str, location: str) -> List[Dict[str, Any]]:\n",
    "    q = urllib.parse.quote_plus(f\"{role} {location}\")\n",
    "    loc = urllib.parse.quote_plus(location)\n",
    "    return [\n",
    "        {\"title\": f\"LinkedIn search: {role}\", \"url\": f\"https://www.linkedin.com/jobs/search/?keywords={q}&location={loc}\", \"snippet\": \"Fallback job-board search\", \"source\":\"fallback\"},\n",
    "        {\"title\": f\"Indeed search: {role}\", \"url\": f\"https://www.indeed.com/jobs?q={q}&l={loc}\", \"snippet\": \"Fallback job-board search\", \"source\":\"fallback\"},\n",
    "        {\"title\": f\"Dice search: {role}\", \"url\": f\"https://www.dice.com/jobs?q={q}&location={loc}\", \"snippet\": \"Fallback job-board search\", \"source\":\"fallback\"},\n",
    "        {\"title\": f\"ZipRecruiter search: {role}\", \"url\": f\"https://www.ziprecruiter.com/jobs-search?search={q}&location={loc}\", \"snippet\": \"Fallback job-board search\", \"source\":\"fallback\"},\n",
    "        {\"title\": f\"Lever search: {role}\", \"url\": f\"https://www.google.com/search?q=site:jobs.lever.co+{q}\", \"snippet\": \"Fallback via Google\", \"source\":\"fallback\"},\n",
    "        {\"title\": f\"Greenhouse search: {role}\", \"url\": f\"https://www.google.com/search?q=site:boards.greenhouse.io+{q}\", \"snippet\": \"Fallback via Google\", \"source\":\"fallback\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "async def l3_discovery_async(st: Dict[str, Any]) -> None:\n",
    "    prefs = st.get(\"preferences\") or {}\n",
    "    roles = prefs.get(\"target_roles\") or [prefs.get(\"target_role\") or \"Data Scientist\"]\n",
    "    roles = [str(r).strip() for r in roles if str(r).strip()][:3]\n",
    "    location = str(prefs.get(\"location\") or \"United States\")\n",
    "    recency_h = float(prefs.get(\"recency_hours\") or 36.0)\n",
    "    tbs = \"qdr:d\" if recency_h <= 36 else None\n",
    "    max_jobs = int(prefs.get(\"max_jobs\") or 40)\n",
    "\n",
    "    api_key = os.getenv(\"SERPER_API_KEY\", \"\")\n",
    "\n",
    "    primary = roles[0] if roles else \"Data Scientist\"\n",
    "    board_domains = [\"linkedin.com/jobs\", \"indeed.com\", \"glassdoor.com\", \"ziprecruiter.com\", \"monster.com\", \"dice.com\", \"jobs.lever.co\", \"boards.greenhouse.io\"]\n",
    "    queries = [f'site:{d} \"{primary}\" \"{location}\"' for d in board_domains]\n",
    "    queries.extend([f\"{r} {location} apply\" for r in roles[:2]])\n",
    "    st[\"discovery_queries\"] = queries\n",
    "\n",
    "    items: List[Dict[str, Any]] = []\n",
    "\n",
    "    if api_key:\n",
    "        async def one(q: str):\n",
    "            return await serper_search(api_key, q, num=10, tbs=tbs)\n",
    "\n",
    "        results = await asyncio.gather(*[one(q) for q in queries[:10]], return_exceptions=False)\n",
    "        for ok, got, err in results:\n",
    "            log_attempt(st, layer=\"L3\", agent=\"DiscoveryAgent\", tool=\"serper.search\",\n",
    "                        status=\"ok\" if ok else \"failed\", confidence=0.75 if (ok and got) else 0.30, error=err)\n",
    "            if ok:\n",
    "                for it in got:\n",
    "                    it[\"source\"] = \"serper\"\n",
    "                items.extend(got)\n",
    "\n",
    "    if not items:\n",
    "        log_attempt(st, layer=\"L3\", agent=\"DiscoveryAgent\", tool=\"local.fallback_board_urls\", status=\"ok\", confidence=0.55)\n",
    "        items = fallback_board_urls(primary, location)\n",
    "\n",
    "    # dedupe + cap\n",
    "    seen = set()\n",
    "    jobs_raw: List[Dict[str, Any]] = []\n",
    "    for it in items:\n",
    "        url = (it.get(\"url\") or \"\").strip()\n",
    "        if not url or url in seen:\n",
    "            continue\n",
    "        seen.add(url)\n",
    "        jobs_raw.append({\n",
    "            \"title\": it.get(\"title\") or \"\",\n",
    "            \"url\": url,\n",
    "            \"snippet\": it.get(\"snippet\") or \"\",\n",
    "            \"source\": it.get(\"source\") or \"unknown\",\n",
    "        })\n",
    "        if len(jobs_raw) >= max_jobs:\n",
    "            break\n",
    "\n",
    "    st[\"jobs_raw\"] = jobs_raw\n",
    "\n",
    "    rid = str(st.get(\"run_id\") or \"run\")\n",
    "    run_path = runs_dir(rid) / \"jobs_raw.json\"\n",
    "    day_path = daily_dir() / f\"{rid}_jobs_raw.json\"\n",
    "\n",
    "    st.setdefault(\"artifacts\", {})\n",
    "    st[\"artifacts\"][\"jobs_raw\"] = {\"path\": save_json(run_path, jobs_raw), \"content_type\": \"application/json\"}\n",
    "    st[\"artifacts\"][\"jobs_raw_daily\"] = {\"path\": save_json(day_path, {\"run_id\": rid, \"at_utc\": utc_now(), \"jobs\": jobs_raw}), \"content_type\": \"application/json\"}\n",
    "\n",
    "    feed(st, \"L3\", \"DiscoveryAgent\", f\"Discovery completed: {len(jobs_raw)} jobs (saved).\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# L4 (scrape + match, timeout-safe)\n",
    "# =========================\n",
    "async def fetch_text(url: str, timeout_s: float = 10.0) -> Tuple[bool, str, Optional[str]]:\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=timeout_s, follow_redirects=True) as client:\n",
    "            r = await client.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        if r.status_code >= 400:\n",
    "            return False, \"\", f\"http_{r.status_code}\"\n",
    "        html = r.text\n",
    "        txt = re.sub(r\"<(script|style)[^>]*>.*?</\\1>\", \" \", html, flags=re.S|re.I)\n",
    "        txt = re.sub(r\"<[^>]+>\", \" \", txt)\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        return True, txt[:12000], None\n",
    "    except Exception as e:\n",
    "        return False, \"\", str(e)\n",
    "\n",
    "\n",
    "async def l4_match_async(st: Dict[str, Any]) -> None:\n",
    "    prefs = st.get(\"preferences\") or {}\n",
    "    max_jobs = int(prefs.get(\"max_jobs\") or 40)\n",
    "    visa_req = bool(prefs.get(\"visa_sponsorship_required\") or False)\n",
    "\n",
    "    resume = st.get(\"resume_text\") or \"\"\n",
    "    prof = st.get(\"profile\") or {}\n",
    "    resume_skills = [str(x).lower() for x in (prof.get(\"skills\") or [])]\n",
    "    ats = ats_score(resume)\n",
    "\n",
    "    exp_tokens: Dict[str, int] = {}\n",
    "    for t in tokenize(resume):\n",
    "        exp_tokens[t] = exp_tokens.get(t, 0) + 1\n",
    "\n",
    "    jobs = (st.get(\"jobs_raw\") or [])[:max_jobs]\n",
    "    scored: List[Dict[str, Any]] = []\n",
    "\n",
    "    async def score_one(j: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        url = j.get(\"url\") or \"\"\n",
    "        title = j.get(\"title\") or \"\"\n",
    "        snippet = j.get(\"snippet\") or \"\"\n",
    "        ok, txt, err = await fetch_text(url, timeout_s=10.0)\n",
    "        log_attempt(st, layer=\"L4\", agent=\"ScraperAgent\", tool=\"httpx.get\",\n",
    "                    status=\"ok\" if ok else \"failed\", confidence=0.65 if ok else 0.25, error=err)\n",
    "\n",
    "        text = txt if ok and txt else snippet\n",
    "        low = (text or \"\").lower()\n",
    "\n",
    "        if visa_req and any(x in low for x in VISA_NEGATIVE):\n",
    "            return None\n",
    "\n",
    "        tokens_job = tokenize(text)\n",
    "        job_tf: Dict[str, int] = {}\n",
    "        for t in tokens_job:\n",
    "            job_tf[t] = job_tf.get(t, 0) + 1\n",
    "\n",
    "        matched = [s for s in resume_skills if s and s in low]\n",
    "        skill_overlap = min(1.0, len(set(matched)) / max(1.0, len(set(tokens_job)) / 40.0))\n",
    "        exp_align = cosine(exp_tokens, job_tf)\n",
    "\n",
    "        # Market factor: verified data is currently inconclusive → default 1.0\n",
    "        market = 1.0\n",
    "\n",
    "        score = interview_chance(skill_overlap, exp_align, ats, market)\n",
    "\n",
    "        missing = []\n",
    "        for sk in COMMON_SKILLS:\n",
    "            if sk in low and sk not in resume_skills:\n",
    "                missing.append(sk)\n",
    "        missing = missing[:15]\n",
    "\n",
    "        return {\n",
    "            \"job_id\": url or title,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"source\": j.get(\"source\") or \"unknown\",\n",
    "            \"snippet\": snippet,\n",
    "            \"full_text\": text,\n",
    "            \"matched_skills\": matched[:15],\n",
    "            \"missing_skills\": missing,\n",
    "            \"components\": {\n",
    "                \"skill_overlap\": float(skill_overlap),\n",
    "                \"experience_alignment\": float(exp_align),\n",
    "                \"ats_score\": float(ats),\n",
    "                \"market_competition_factor\": float(market),\n",
    "            },\n",
    "            \"score\": float(score),\n",
    "            \"match_percent\": round(float(score) * 100.0, 2),\n",
    "        }\n",
    "\n",
    "    # concurrent scoring with cap\n",
    "    tasks = [score_one(j) for j in jobs]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    for r in results:\n",
    "        if isinstance(r, dict):\n",
    "            scored.append(r)\n",
    "\n",
    "    scored.sort(key=lambda x: float(x.get(\"score\", 0.0)), reverse=True)\n",
    "    st[\"jobs_scored\"] = scored\n",
    "\n",
    "    rid = str(st.get(\"run_id\") or \"run\")\n",
    "    p = runs_dir(rid) / \"jobs_scored.json\"\n",
    "    st.setdefault(\"artifacts\", {})\n",
    "    st[\"artifacts\"][\"jobs_scored\"] = {\"path\": save_json(p, scored), \"content_type\": \"application/json\"}\n",
    "\n",
    "    feed(st, \"L4\", \"MatcherAgent\", f\"Matched/scored {len(scored)} jobs (saved).\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# L5 (rank + HITL)\n",
    "# =========================\n",
    "def l5_rank(st: Dict[str, Any]) -> None:\n",
    "    prefs = st.get(\"preferences\") or {}\n",
    "    top_n = int(prefs.get(\"top_n\") or 30)\n",
    "    scored = st.get(\"jobs_scored\") or []\n",
    "    ranking = scored[:top_n]\n",
    "    st[\"ranking\"] = ranking\n",
    "\n",
    "    rid = str(st.get(\"run_id\") or \"run\")\n",
    "    p = runs_dir(rid) / \"ranking.json\"\n",
    "    st.setdefault(\"artifacts\", {})\n",
    "    st[\"artifacts\"][\"ranking\"] = {\"path\": save_json(p, ranking), \"content_type\": \"application/json\"}\n",
    "\n",
    "    st[\"status\"] = \"needs_human_approval\"\n",
    "    st[\"pending_action\"] = \"review_ranking\"\n",
    "    feed(st, \"L5\", \"Ranker\", f\"Ranking ready: {len(ranking)} jobs (saved).\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Public entrypoints used by RunManagerService\n",
    "# =========================\n",
    "async def run_single_layer(state: Dict[str, Any], layer: str) -> Dict[str, Any]:\n",
    "    layer = (layer or \"\").upper().strip()\n",
    "\n",
    "    if layer == \"L0\":\n",
    "        l0_guard(state)\n",
    "        return state\n",
    "\n",
    "    if layer == \"L2\":\n",
    "        l2_parse(state)\n",
    "        return state\n",
    "\n",
    "    if layer == \"L3\":\n",
    "        await l3_discovery_async(state)\n",
    "        return state\n",
    "\n",
    "    if layer == \"L4\":\n",
    "        await l4_match_async(state)\n",
    "        return state\n",
    "\n",
    "    if layer == \"L5\":\n",
    "        l5_rank(state)\n",
    "        return state\n",
    "\n",
    "    # L6–L9: use your existing nodes_l6_l9.py if present\n",
    "    if layer in (\"L6\",\"L7\",\"L8\",\"L9\"):\n",
    "        from careeragent.langgraph.nodes_l6_l9 import (\n",
    "            l6_draft_node, l6_evaluator_node,\n",
    "            l7_apply_node, l7_evaluator_node,\n",
    "            l8_tracker_node, l8_evaluator_node,\n",
    "            l9_analytics_node,\n",
    "        )\n",
    "        if layer == \"L6\":\n",
    "            state.update(await l6_draft_node(state))      # type: ignore[arg-type]\n",
    "            state.update(await l6_evaluator_node(state))  # type: ignore[arg-type]\n",
    "            return state\n",
    "        if layer == \"L7\":\n",
    "            state.update(await l7_apply_node(state))      # type: ignore[arg-type]\n",
    "            state.update(await l7_evaluator_node(state))  # type: ignore[arg-type]\n",
    "            return state\n",
    "        if layer == \"L8\":\n",
    "            state.update(await l8_tracker_node(state))      # type: ignore[arg-type]\n",
    "            state.update(await l8_evaluator_node(state))    # type: ignore[arg-type]\n",
    "            return state\n",
    "        if layer == \"L9\":\n",
    "            state.update(await l9_analytics_node(state))    # type: ignore[arg-type]\n",
    "            return state\n",
    "\n",
    "    feed(state, \"L1\", \"Runtime\", f\"Layer {layer} not implemented.\")\n",
    "    return state\n",
    "\n",
    "\n",
    "async def approve_ranking_flow(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    from careeragent.langgraph.nodes_l6_l9 import l6_draft_node, l6_evaluator_node\n",
    "    state[\"status\"] = \"running\"\n",
    "    state[\"pending_action\"] = None\n",
    "    feed(state, \"L6\", \"HITL\", \"Ranking approved. Generating drafts…\")\n",
    "    state.update(await l6_draft_node(state))      # type: ignore[arg-type]\n",
    "    state.update(await l6_evaluator_node(state))  # type: ignore[arg-type]\n",
    "    state[\"status\"] = \"needs_human_approval\"\n",
    "    state[\"pending_action\"] = \"review_drafts\"\n",
    "    return state\n",
    "\n",
    "\n",
    "async def approve_drafts_flow(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    from careeragent.langgraph.nodes_l6_l9 import (\n",
    "        l7_apply_node, l7_evaluator_node,\n",
    "        l8_tracker_node, l8_evaluator_node,\n",
    "        l9_analytics_node,\n",
    "    )\n",
    "    state[\"status\"] = \"running\"\n",
    "    state[\"pending_action\"] = None\n",
    "    feed(state, \"L7\", \"HITL\", \"Drafts approved. Applying + tracking + analytics…\")\n",
    "    state.update(await l7_apply_node(state))      # type: ignore[arg-type]\n",
    "    state.update(await l7_evaluator_node(state))  # type: ignore[arg-type]\n",
    "    if state.get(\"status\") == \"needs_human_approval\":\n",
    "        return state\n",
    "    state.update(await l8_tracker_node(state))      # type: ignore[arg-type]\n",
    "    state.update(await l8_evaluator_node(state))    # type: ignore[arg-type]\n",
    "    if state.get(\"status\") == \"needs_human_approval\":\n",
    "        return state\n",
    "    state.update(await l9_analytics_node(state))    # type: ignore[arg-type]\n",
    "    state[\"status\"] = \"completed\"\n",
    "    state[\"pending_action\"] = None\n",
    "    feed(state, \"L9\", \"HITL\", \"Run completed.\")\n",
    "    return state\n",
    "''')\n",
    "\n",
    "# ---------------- run_manager_service.py (hard timeout runner + correct step status) ----------------\n",
    "backup_write(\"src/careeragent/api/run_manager_service.py\", r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures as cf\n",
    "import copy\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "from uuid import uuid4\n",
    "\n",
    "from careeragent.services.db_service import SqliteStateStore\n",
    "from careeragent.langgraph.runtime_nodes import run_single_layer, approve_ranking_flow, approve_drafts_flow\n",
    "\n",
    "\n",
    "def utc_now() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "def artifacts_root() -> Path:\n",
    "    return Path(\"src/careeragent/artifacts\").resolve()\n",
    "\n",
    "\n",
    "class RunManagerService:\n",
    "    \"\"\"\n",
    "    Description: Indestructible background runner with hard timeouts + state persistence per layer.\n",
    "    Layer: L8\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._store = SqliteStateStore()\n",
    "\n",
    "    def _runs_dir(self, run_id: str) -> Path:\n",
    "        d = artifacts_root() / \"runs\" / run_id\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        return d\n",
    "\n",
    "    def save_state(self, *, run_id: str, state: Dict[str, Any]) -> None:\n",
    "        state.setdefault(\"meta\", {})\n",
    "        state[\"meta\"][\"heartbeat_utc\"] = utc_now()\n",
    "        self._store.upsert_state(run_id=run_id, status=str(state.get(\"status\", \"unknown\")), state=state, updated_at_utc=utc_now())\n",
    "\n",
    "    def get_state(self, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        return self._store.get_state(run_id=run_id)\n",
    "\n",
    "    def create_run(self, *, resume_filename: str, resume_text: str, resume_bytes: bytes, preferences: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        run_id = uuid4().hex\n",
    "        run_dir = self._runs_dir(run_id)\n",
    "        (run_dir / \"resume_upload.bin\").write_bytes(resume_bytes)\n",
    "        (run_dir / \"resume_raw.txt\").write_text(resume_text, encoding=\"utf-8\")\n",
    "\n",
    "        thresholds = (preferences.get(\"thresholds\") or {})\n",
    "        if \"default\" in thresholds:\n",
    "            d = float(thresholds[\"default\"])\n",
    "            thresholds.setdefault(\"parser\", d)\n",
    "            thresholds.setdefault(\"discovery\", d)\n",
    "            thresholds.setdefault(\"match\", d)\n",
    "            thresholds.setdefault(\"draft\", d)\n",
    "\n",
    "        state: Dict[str, Any] = {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"running\",\n",
    "            \"pending_action\": None,\n",
    "            \"hitl_reason\": None,\n",
    "            \"hitl_payload\": {},\n",
    "            \"thresholds\": thresholds,\n",
    "            \"max_retries\": int(preferences.get(\"max_refinements\", 3)),\n",
    "            \"layer_retry_count\": {},\n",
    "            \"preferences\": preferences,\n",
    "            \"resume_filename\": resume_filename,\n",
    "            \"resume_text\": resume_text,\n",
    "            \"profile\": {},\n",
    "            \"jobs_raw\": [],\n",
    "            \"jobs_scored\": [],\n",
    "            \"ranking\": [],\n",
    "            \"drafts\": {},\n",
    "            \"bridge_docs\": {},\n",
    "            \"meta\": {\n",
    "                \"created_at_utc\": utc_now(),\n",
    "                \"heartbeat_utc\": utc_now(),\n",
    "                \"last_layer\": None,\n",
    "                \"plan_layers\": [\"L0\",\"L2\",\"L3\",\"L4\",\"L5\"],\n",
    "            },\n",
    "            \"steps\": [],\n",
    "            \"live_feed\": [{\"layer\":\"L1\",\"agent\":\"API\",\"message\":\"Run created. Starting background pipeline…\"}],\n",
    "            \"attempts\": [],\n",
    "            \"gates\": [],\n",
    "            \"evaluations\": [],\n",
    "            \"artifacts\": {\n",
    "                \"resume_raw\": {\"path\": str(run_dir / \"resume_raw.txt\"), \"content_type\": \"text/plain\"},\n",
    "                \"resume_upload\": {\"path\": str(run_dir / \"resume_upload.bin\"), \"content_type\": \"application/octet-stream\"},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.save_state(run_id=run_id, state=state)\n",
    "        return state\n",
    "\n",
    "    def start_background(self, run_id: str) -> None:\n",
    "        t = threading.Thread(target=self._bg, args=(run_id,), daemon=True)\n",
    "        t.start()\n",
    "\n",
    "    def _call_layer(self, state: Dict[str, Any], layer: str) -> Dict[str, Any]:\n",
    "        st_copy = copy.deepcopy(state)\n",
    "        return asyncio.run(run_single_layer(st_copy, layer))\n",
    "\n",
    "    def _bg(self, run_id: str) -> None:\n",
    "        state = self.get_state(run_id)\n",
    "        if not state:\n",
    "            return\n",
    "\n",
    "        plan = [(\"L0\", 10), (\"L2\", 20), (\"L3\", 45), (\"L4\", 120), (\"L5\", 15)]\n",
    "\n",
    "        for layer, tmo in plan:\n",
    "            if state.get(\"status\") in (\"blocked\",\"needs_human_approval\",\"failed\",\"completed\"):\n",
    "                self.save_state(run_id=run_id, state=state)\n",
    "                return\n",
    "\n",
    "            state.setdefault(\"meta\", {})\n",
    "            state[\"meta\"][\"last_layer\"] = layer\n",
    "            state.setdefault(\"steps\", []).append({\"layer_id\": layer, \"status\": \"running\", \"started_at_utc\": utc_now()})\n",
    "            state.setdefault(\"live_feed\", []).append({\"layer\": layer, \"agent\":\"Orchestrator\", \"message\": f\"Running {layer}…\"})\n",
    "            self.save_state(run_id=run_id, state=state)\n",
    "\n",
    "            with cf.ThreadPoolExecutor(max_workers=1) as ex:\n",
    "                fut = ex.submit(self._call_layer, state, layer)\n",
    "                try:\n",
    "                    state = fut.result(timeout=tmo)\n",
    "                except cf.TimeoutError:\n",
    "                    state[\"status\"] = \"needs_human_approval\"\n",
    "                    state[\"pending_action\"] = f\"timeout_{layer.lower()}\"\n",
    "                    state.setdefault(\"live_feed\", []).append({\"layer\": layer, \"agent\":\"TimeoutGuard\", \"message\": f\"{layer} timed out after {tmo}s\"})\n",
    "                except Exception as e:\n",
    "                    state[\"status\"] = \"failed\"\n",
    "                    state[\"pending_action\"] = f\"error_{layer.lower()}\"\n",
    "                    state.setdefault(\"live_feed\", []).append({\"layer\": layer, \"agent\":\"CrashGuard\", \"message\": f\"{layer} crashed: {e}\"})\n",
    "\n",
    "            # mark step finished (ok/failed/blocked)\n",
    "            step_status = \"ok\"\n",
    "            if str(state.get(\"pending_action\",\"\")).startswith((\"timeout_\",\"error_\")):\n",
    "                step_status = \"failed\"\n",
    "            if state.get(\"status\") in (\"blocked\",\"failed\"):\n",
    "                step_status = \"failed\"\n",
    "\n",
    "            state[\"steps\"][-1][\"status\"] = step_status\n",
    "            state[\"steps\"][-1][\"finished_at_utc\"] = utc_now()\n",
    "            self.save_state(run_id=run_id, state=state)\n",
    "\n",
    "            if state.get(\"pending_action\") == \"review_ranking\":\n",
    "                state[\"status\"] = \"needs_human_approval\"\n",
    "                self.save_state(run_id=run_id, state=state)\n",
    "                return\n",
    "\n",
    "        if state.get(\"status\") == \"running\":\n",
    "            state[\"status\"] = \"needs_human_approval\"\n",
    "            state[\"pending_action\"] = \"review_ranking\"\n",
    "            state.setdefault(\"live_feed\", []).append({\"layer\":\"L5\",\"agent\":\"Orchestrator\",\"message\":\"Ranking ready for review.\"})\n",
    "            self.save_state(run_id=run_id, state=state)\n",
    "\n",
    "    async def handle_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        state = self.get_state(run_id)\n",
    "        if not state:\n",
    "            raise ValueError(\"run_id not found\")\n",
    "\n",
    "        if action_type == \"execute_layer\":\n",
    "            layer = str(payload.get(\"layer\",\"\")).upper()\n",
    "            state = await asyncio.to_thread(lambda: self._call_layer(state, layer))\n",
    "            self.save_state(run_id=run_id, state=state)\n",
    "            return state\n",
    "\n",
    "        if action_type == \"approve_ranking\":\n",
    "            state = await asyncio.to_thread(lambda: asyncio.run(approve_ranking_flow(copy.deepcopy(state))))\n",
    "            self.save_state(run_id=run_id, state=state)\n",
    "            return state\n",
    "\n",
    "        if action_type == \"approve_drafts\":\n",
    "            state = await asyncio.to_thread(lambda: asyncio.run(approve_drafts_flow(copy.deepcopy(state))))\n",
    "            self.save_state(run_id=run_id, state=state)\n",
    "            return state\n",
    "\n",
    "        state.setdefault(\"live_feed\", []).append({\"layer\":\"L5\",\"agent\":\"HITL\",\"message\":f\"Unhandled action_type={action_type}\"})\n",
    "        self.save_state(run_id=run_id, state=state)\n",
    "        return state\n",
    "''')\n",
    "\n",
    "print(\"✅ Patched: runtime_nodes L0–L5 are now local-first + L3/L4 persist job URLs + hard timeouts enabled.\")\n",
    "print(\"Restart backend and start a NEW run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a50b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1313367093.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m7f93696f145648c0a644b36c4752c74e\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "RUN_ID=7f93696f145648c0a644b36c4752c74e\n",
    "curl -sS \"http://127.0.0.1:8000/status/$RUN_ID\" | python -c \"import sys,json; s=json.load(sys.stdin); print('steps',[(x.get('layer_id'),x.get('status'),x.get('finished_at_utc')) for x in (s.get('steps') or [])]); print('jobs_raw',len(s.get('jobs_raw') or [])); print('art_jobs_raw',(s.get('artifacts') or {}).get('jobs_raw')); print('art_scored',(s.get('artifacts') or {}).get('jobs_scored')); print('art_rank',(s.get('artifacts') or {}).get('ranking'))\"\n",
    "ls -la \"src/careeragent/artifacts/runs/$RUN_ID/\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67f7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careeragent-ai (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
