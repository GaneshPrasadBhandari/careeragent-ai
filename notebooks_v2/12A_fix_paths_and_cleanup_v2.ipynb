{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(12):\n",
    "        if (cur / \"pyproject.toml\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"Repo root not found. Run this notebook from inside the repo.\")\n",
    "\n",
    "# 1) Find repo root even if notebook is in notebooks_v2/\n",
    "CWD = Path.cwd()\n",
    "REPO_ROOT = find_repo_root(CWD)\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# 2) Delete accidental notebooks_v2/src (SAFE)\n",
    "bad_src = REPO_ROOT / \"notebooks_v2\" / \"src\"\n",
    "if bad_src.exists() and bad_src.is_dir():\n",
    "    shutil.rmtree(bad_src)\n",
    "    print(\"âœ… Deleted accidental:\", bad_src)\n",
    "else:\n",
    "    print(\"â„¹ï¸ No accidental notebooks_v2/src found.\")\n",
    "\n",
    "# 3) Ensure root src exists\n",
    "(REPO_ROOT / \"src\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"âœ… Root src exists:\", (REPO_ROOT / \"src\"))\n",
    "\n",
    "# 4) Force notebook process to operate from repo root (so relative paths go to root)\n",
    "os.chdir(REPO_ROOT)\n",
    "print(\"âœ… Changed working directory to repo root:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec4727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cbdea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "print(\"CWD =\", pathlib.Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d76cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before chdir CWD: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2\n",
      "Detected REPO_ROOT: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "âœ… Deleted accidental: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2/src\n",
      "After chdir CWD: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "âœ… Now writing to: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(12):\n",
    "        if (cur / \"pyproject.toml\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"Repo root not found. Make sure you opened the notebook inside the repo.\")\n",
    "\n",
    "# Your current CWD is notebooks_v2\n",
    "CWD = Path.cwd()\n",
    "REPO_ROOT = find_repo_root(CWD)\n",
    "\n",
    "print(\"Before chdir CWD:\", CWD)\n",
    "print(\"Detected REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# Delete only the accidental notebooks_v2/src\n",
    "bad_src = CWD / \"src\"\n",
    "if bad_src.exists() and bad_src.is_dir():\n",
    "    shutil.rmtree(bad_src)\n",
    "    print(\"âœ… Deleted accidental:\", bad_src)\n",
    "else:\n",
    "    print(\"â„¹ï¸ No accidental notebooks_v2/src found.\")\n",
    "\n",
    "# Switch to repo root so ALL future relative paths go to root\n",
    "os.chdir(REPO_ROOT)\n",
    "print(\"After chdir CWD:\", Path.cwd())\n",
    "\n",
    "# Sanity check\n",
    "assert (Path.cwd() / \"src\").exists(), \"Root src/ not found after chdir.\"\n",
    "print(\"âœ… Now writing to:\", (Path.cwd() / \"src\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96347e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "API file exists = False\n",
      "Dashboard exists = True\n",
      "Accidental notebooks_v2/src exists = True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"CWD =\", Path.cwd())\n",
    "print(\"API file exists =\", (Path(\"src/careeragent/api/main.py\")).exists())\n",
    "print(\"Dashboard exists =\", (Path(\"app/ui/dashboard.py\")).exists())\n",
    "print(\"Accidental notebooks_v2/src exists =\", (Path(\"notebooks_v2/src\")).exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d52bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/api\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"src/careeragent/api\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"src/careeragent/api/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "Path(\"src/careeragent/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Ready:\", Path(\"src/careeragent/api\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d920e238",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Not in repo root. CWD=/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m ts = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Safety: ensure we're in the repo root\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (ROOT / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).exists(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNot in repo root. CWD=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Repo root:\u001b[39m\u001b[33m\"\u001b[39m, ROOT)\n\u001b[32m     13\u001b[39m archive = ROOT / \u001b[33m\"\u001b[39m\u001b[33m_legacy_cleanup\u001b[39m\u001b[33m\"\u001b[39m / ts\n",
      "\u001b[31mAssertionError\u001b[39m: Not in repo root. CWD=/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Safety: ensure we're in the repo root\n",
    "assert (ROOT / \"src\").exists(), f\"Not in repo root. CWD={ROOT}\"\n",
    "print(\"âœ… Repo root:\", ROOT)\n",
    "\n",
    "archive = ROOT / \"_legacy_cleanup\" / ts\n",
    "archive.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def archive_move(p: Path, label: str) -> None:\n",
    "    if p.exists():\n",
    "        dest = archive / f\"{label}_{p.name}\"\n",
    "        shutil.move(str(p), str(dest))\n",
    "        print(f\"ðŸ“¦ Archived: {p} -> {dest}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ Not found: {p}\")\n",
    "\n",
    "# 1) Archive accidental folders created inside notebooks_v2\n",
    "archive_move(ROOT / \"notebooks_v2\" / \"app\", \"notebooks_v2\")\n",
    "archive_move(ROOT / \"notebooks_v2\" / \"src\", \"notebooks_v2\")\n",
    "archive_move(ROOT / \"notebooks_v2\" / \"api\", \"notebooks_v2\")\n",
    "\n",
    "# 2) Optional: if you have an old backend under root/app/api, archive it too (to avoid running the wrong API)\n",
    "archive_move(ROOT / \"app\" / \"api\", \"root_app\")\n",
    "\n",
    "# 3) Verify canonical targets exist\n",
    "need = [\n",
    "    ROOT / \"src/careeragent/api/main.py\",\n",
    "    ROOT / \"app/ui/dashboard.py\",\n",
    "    ROOT / \"app/main.py\",\n",
    "]\n",
    "for n in need:\n",
    "    print(\"âœ… EXISTS\" if n.exists() else \"âŒ MISSING\", n)\n",
    "\n",
    "print(\"\\nArchive folder:\", archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9343019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpath\u001b[49m.cwd()\n",
      "\u001b[31mNameError\u001b[39m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1bfcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/notebooks_v2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46977df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff435f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESTORED: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/_legacy_cleanup/20260220_231414/notebooks_v2_src/careeragent/api/main.py  ->  /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/api/main.py\n",
      "OK: dashboard already exists at /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py\n",
      "\n",
      "VERIFY:\n",
      "API main exists: True -> /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/api/main.py\n",
      "Dashboard exists: True -> /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/ui/dashboard.py\n",
      "\n",
      "âœ… DELETED: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/_legacy_cleanup\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "legacy = ROOT / \"_legacy_cleanup\"\n",
    "\n",
    "# --- safety checks ---\n",
    "assert (ROOT / \"src\").exists(), f\"Not at repo root. CWD={ROOT}\"\n",
    "assert legacy.exists(), f\"_legacy_cleanup not found at: {legacy}\"\n",
    "\n",
    "def backup_if_exists(dst: Path) -> None:\n",
    "    if dst.exists():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = dst.with_suffix(dst.suffix + f\".bak_{ts}\")\n",
    "        shutil.copy2(dst, bak)\n",
    "        print(f\"BACKUP: {dst} -> {bak}\")\n",
    "\n",
    "def pick_latest_api_main() -> Path:\n",
    "    # Find any .../api/main.py inside _legacy_cleanup\n",
    "    candidates = [p for p in legacy.rglob(\"main.py\") if p.parent.name == \"api\"]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No api/main.py found inside _legacy_cleanup.\")\n",
    "    # pick most recently modified\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "# 1) Restore api/main.py into src/careeragent/api/main.py\n",
    "src_api_dir = ROOT / \"src\" / \"careeragent\" / \"api\"\n",
    "src_api_dir.mkdir(parents=True, exist_ok=True)\n",
    "(ROOT / \"src\" / \"careeragent\" / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(src_api_dir / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "latest_api_main = pick_latest_api_main()\n",
    "dst_api_main = src_api_dir / \"main.py\"\n",
    "\n",
    "backup_if_exists(dst_api_main)\n",
    "shutil.copy2(latest_api_main, dst_api_main)\n",
    "print(f\"RESTORED: {latest_api_main}  ->  {dst_api_main}\")\n",
    "\n",
    "# 2) (Optional but recommended) Restore dashboard.py if the canonical one is missing\n",
    "dst_dash = ROOT / \"app\" / \"ui\" / \"dashboard.py\"\n",
    "if not dst_dash.exists():\n",
    "    dash_candidates = [p for p in legacy.rglob(\"dashboard.py\") if \"ui\" in [x.name for x in p.parents]]\n",
    "    if dash_candidates:\n",
    "        dash_candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        (ROOT / \"app\" / \"ui\").mkdir(parents=True, exist_ok=True)\n",
    "        (ROOT / \"app\" / \"ui\" / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "        backup_if_exists(dst_dash)\n",
    "        shutil.copy2(dash_candidates[0], dst_dash)\n",
    "        print(f\"RESTORED: {dash_candidates[0]} -> {dst_dash}\")\n",
    "    else:\n",
    "        print(\"WARN: dashboard.py not found in legacy to restore (but you already have one in app/ui).\")\n",
    "else:\n",
    "    print(f\"OK: dashboard already exists at {dst_dash}\")\n",
    "\n",
    "# 3) Verify canonical targets\n",
    "print(\"\\nVERIFY:\")\n",
    "print(\"API main exists:\", dst_api_main.exists(), \"->\", dst_api_main)\n",
    "print(\"Dashboard exists:\", dst_dash.exists(), \"->\", dst_dash)\n",
    "\n",
    "# 4) DELETE the entire _legacy_cleanup folder (as requested)\n",
    "shutil.rmtree(legacy)\n",
    "print(\"\\nâœ… DELETED:\", legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68904b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbf6a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# If you're currently inside notebooks_v2, this moves you to repo root\n",
    "os.chdir(Path.cwd().parent)\n",
    "print(\"CWD =\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aec8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CWD = /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai\n",
      "pyproject exists = True\n",
      ".git exists = True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(15):\n",
    "        if (cur / \"pyproject.toml\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(f\"Repo root not found starting from: {start}\")\n",
    "\n",
    "# If you're sitting in .../clark, this will likely resolve to .../clark/careeragent-ai\n",
    "# Prefer: if a child folder named careeragent-ai exists, jump into it first.\n",
    "cwd = Path.cwd().resolve()\n",
    "candidate = cwd / \"careeragent-ai\"\n",
    "if candidate.exists() and candidate.is_dir():\n",
    "    os.chdir(candidate)\n",
    "else:\n",
    "    # otherwise search upward for pyproject/.git\n",
    "    os.chdir(find_repo_root(cwd))\n",
    "\n",
    "print(\"âœ… CWD =\", Path.cwd())\n",
    "print(\"pyproject exists =\", (Path.cwd() / \"pyproject.toml\").exists())\n",
    "print(\".git exists =\", (Path.cwd() / \".git\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db768ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367f7a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/engine.py\n",
      "BACKUP: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py.bak_20260220_235251\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/src/careeragent/orchestration/__init__.py\n",
      "WROTE: /Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai/app/main.py\n",
      "\n",
      "âœ… FIXED: ENGINE export + app/main.py restored.\n",
      "Now run:\n",
      "uv run python run_app.py\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "assert (ROOT / \"src\").exists(), f\"Run from repo root. CWD={ROOT}\"\n",
    "\n",
    "def backup_write(path: str, content: str) -> None:\n",
    "    p = ROOT / path\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts}\")\n",
    "        bak.write_text(p.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(\"BACKUP:\", bak)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    print(\"WROTE:\", p)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) src/careeragent/orchestration/engine.py  (canonical ENGINE lives here)\n",
    "# -------------------------------------------------------------------\n",
    "ENGINE_PY = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from uuid import uuid4\n",
    "\n",
    "import httpx\n",
    "\n",
    "from careeragent.config import artifacts_root, get_settings\n",
    "from careeragent.orchestration.state import OrchestrationState\n",
    "from careeragent.services.db_service import SqliteStateStore\n",
    "from careeragent.services.notification_service import NotificationService\n",
    "from careeragent.services.health_service import HealthService\n",
    "\n",
    "from careeragent.agents.security_agent import SanitizeAgent\n",
    "from careeragent.agents.parser_agent_service import ParserAgentService, ExtractedResume\n",
    "from careeragent.agents.parser_evaluator_service import ParserEvaluatorService\n",
    "from careeragent.agents.matcher_agent_schema import JobDescription\n",
    "from careeragent.agents.matcher_agent_service import MatcherAgentService\n",
    "from careeragent.services.xai_service import XAIService\n",
    "from careeragent.services.exporter import CareerDossierExporter\n",
    "\n",
    "\n",
    "class LiveFeed:\n",
    "    \"\"\"\n",
    "    Description: Live Agent Feed logger stored in state.meta['live_feed'] for UI.\n",
    "    Layer: L1\n",
    "    Input: OrchestrationState + event\n",
    "    Output: appended feed\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def emit(state: OrchestrationState, *, layer: str, agent: str, message: str) -> None:\n",
    "        state.meta.setdefault(\"live_feed\", [])\n",
    "        state.meta[\"live_feed\"].append({\"layer\": layer, \"agent\": agent, \"message\": message})\n",
    "        state.touch()\n",
    "\n",
    "\n",
    "def _run_dir(run_id: str) -> Path:\n",
    "    d = artifacts_root() / \"runs\" / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _save_json(p: Path, obj: Any) -> None:\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "class LocalResumeExtractor:\n",
    "    \"\"\"\n",
    "    Description: Extract resume text from uploaded PDF/TXT/DOCX (best-effort local).\n",
    "    Layer: L2\n",
    "    Input: filename + bytes\n",
    "    Output: extracted text\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def extract_text(*, filename: str, data: bytes) -> str:\n",
    "        name = (filename or \"\").lower()\n",
    "        if name.endswith(\".txt\"):\n",
    "            return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "        if name.endswith(\".pdf\"):\n",
    "            try:\n",
    "                from pypdf import PdfReader  # type: ignore\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"PDF extraction needs pypdf. Install: uv add pypdf\") from e\n",
    "            import io\n",
    "            reader = PdfReader(io.BytesIO(data))\n",
    "            return \"\\n\".join([(pg.extract_text() or \"\") for pg in reader.pages])\n",
    "\n",
    "        if name.endswith(\".docx\"):\n",
    "            try:\n",
    "                import docx  # type: ignore\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"DOCX extraction needs python-docx. Install: uv add python-docx\") from e\n",
    "            import io\n",
    "            f = io.BytesIO(data)\n",
    "            d = docx.Document(f)\n",
    "            return \"\\n\".join([p.text for p in d.paragraphs if p.text])\n",
    "\n",
    "        return data.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "class SerperDiscoveryService:\n",
    "    \"\"\"\n",
    "    Description: Serper-based job discovery.\n",
    "    Layer: L3\n",
    "    Input: query\n",
    "    Output: list of search results\n",
    "    \"\"\"\n",
    "    SERPER_URL = \"https://google.serper.dev/search\"\n",
    "\n",
    "    def __init__(self, *, api_key: str, health: HealthService) -> None:\n",
    "        self._key = api_key\n",
    "        self._health = health\n",
    "\n",
    "    def search(self, *, state: OrchestrationState, step_id: str, query: str, num: int = 20) -> List[Dict[str, Any]]:\n",
    "        headers = {\"X-API-KEY\": self._key, \"Content-Type\": \"application/json\"}\n",
    "        try:\n",
    "            with httpx.Client(timeout=30.0) as client:\n",
    "                r = client.post(self.SERPER_URL, headers=headers, json={\"q\": query, \"num\": num})\n",
    "        except Exception as e:\n",
    "            state.status = \"api_failure\"\n",
    "            state.meta[\"run_failure_code\"] = \"API_FAILURE\"\n",
    "            state.meta[\"run_failure_provider\"] = \"serper\"\n",
    "            LiveFeed.emit(state, layer=\"L3\", agent=\"DiscoveryService\", message=f\"Serper request failed: {e}\")\n",
    "            return []\n",
    "\n",
    "        # quota enforcement (403 -> blocked)\n",
    "        if self._health.quota.handle_serper_response(\n",
    "            state=state, step_id=step_id, status_code=r.status_code, tool_name=\"serper.search\", error_detail=r.text[:200]\n",
    "        ):\n",
    "            return []\n",
    "\n",
    "        if r.status_code >= 400:\n",
    "            state.status = \"api_failure\"\n",
    "            state.meta[\"run_failure_code\"] = \"API_FAILURE\"\n",
    "            state.meta[\"run_failure_provider\"] = \"serper\"\n",
    "            LiveFeed.emit(state, layer=\"L3\", agent=\"DiscoveryService\", message=f\"Serper error {r.status_code}: {r.text[:200]}\")\n",
    "            return []\n",
    "\n",
    "        organic = (r.json().get(\"organic\") or [])\n",
    "        out = []\n",
    "        for it in organic:\n",
    "            out.append({\"title\": it.get(\"title\") or \"\", \"link\": it.get(\"link\") or \"\", \"snippet\": it.get(\"snippet\") or \"\"})\n",
    "        return out\n",
    "\n",
    "\n",
    "class OneClickAutomationEngine:\n",
    "    \"\"\"\n",
    "    Description: One-click autonomous pipeline runner with HITL pause points.\n",
    "    Layer: L0-L9\n",
    "    Input: resume upload + preferences\n",
    "    Output: state persisted for UI polling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._s = get_settings()\n",
    "        self._store = SqliteStateStore()\n",
    "\n",
    "        self._health = HealthService()\n",
    "        self._health.load_env(dotenv_path=str(Path(\".env\")))\n",
    "        self._health.enable_langsmith_tracing(project=self._s.langsmith_project)\n",
    "\n",
    "        self._notifier = NotificationService(dry_run=not bool(self._s.twilio_account_sid))\n",
    "        self._sanitize = SanitizeAgent()\n",
    "\n",
    "        self._parser = ParserAgentService()\n",
    "        self._parser_eval = ParserEvaluatorService()\n",
    "        self._matcher = MatcherAgentService()\n",
    "\n",
    "    def _persist(self, state: OrchestrationState) -> None:\n",
    "        d = _run_dir(state.run_id)\n",
    "        _save_json(d / \"state.json\", state.model_dump())\n",
    "        self._store.upsert_state(run_id=state.run_id, status=state.status, state=state.model_dump(), updated_at_utc=state.updated_at_utc)\n",
    "\n",
    "    def load(self, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        return self._store.get_state(run_id=run_id)\n",
    "\n",
    "    def start_run(self, *, filename: str, data: bytes, prefs: Dict[str, Any]) -> OrchestrationState:\n",
    "        state = OrchestrationState.new(env=self._s.environment, mode=\"agentic\", git_sha=None)\n",
    "        state.meta[\"preferences\"] = prefs\n",
    "        LiveFeed.emit(state, layer=\"L1\", agent=\"Dashboard\", message=\"Run created. Starting autonomous pipelineâ€¦\")\n",
    "        self._persist(state)\n",
    "\n",
    "        t = threading.Thread(target=self._run, args=(state.run_id, filename, data), daemon=True)\n",
    "        t.start()\n",
    "        return state\n",
    "\n",
    "    def submit_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any]) -> OrchestrationState:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            raise ValueError(\"run_id not found\")\n",
    "        state = OrchestrationState(**raw)\n",
    "        state.meta[\"last_user_action\"] = {\"type\": action_type, \"payload\": payload}\n",
    "        LiveFeed.emit(state, layer=\"L5\", agent=\"HITL\", message=f\"User action: {action_type}\")\n",
    "        self._persist(state)\n",
    "\n",
    "        # For beta: just flip pending_action & continue in background if needed\n",
    "        t = threading.Thread(target=self._continue, args=(run_id,), daemon=True)\n",
    "        t.start()\n",
    "        return state\n",
    "\n",
    "    # ---------------- core run ----------------\n",
    "    def _run(self, run_id: str, filename: str, data: bytes) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        state = OrchestrationState(**raw)\n",
    "        run_dir = _run_dir(run_id)\n",
    "\n",
    "        # L2 extract\n",
    "        state.start_step(\"l2_extract\", layer_id=\"L2\", tool_name=\"ResumeExtractor\", input_ref={\"filename\": filename})\n",
    "        try:\n",
    "            text = LocalResumeExtractor.extract_text(filename=filename, data=data)\n",
    "        except Exception as e:\n",
    "            state.end_step(\"l2_extract\", status=\"failed\", output_ref={}, message=str(e))\n",
    "            state.status = \"needs_human_approval\"\n",
    "            state.meta[\"pending_action\"] = \"resume_extract_failed\"\n",
    "            LiveFeed.emit(state, layer=\"L2\", agent=\"ParserAgent\", message=f\"Resume extraction failed: {e}\")\n",
    "            self._persist(state)\n",
    "            return\n",
    "\n",
    "        (run_dir / \"resume_raw.txt\").write_text(text, encoding=\"utf-8\")\n",
    "        state.add_artifact(\"resume_raw\", str(run_dir / \"resume_raw.txt\"), content_type=\"text/plain\")\n",
    "        state.end_step(\"l2_extract\", status=\"ok\", output_ref={\"artifact_key\": \"resume_raw\"}, message=\"extracted\")\n",
    "        LiveFeed.emit(state, layer=\"L2\", agent=\"ParserAgent\", message=\"Resume extracted from upload.\")\n",
    "        self._persist(state)\n",
    "\n",
    "        # L0 sanitize\n",
    "        state.start_step(\"l0_sanitize\", layer_id=\"L0\", tool_name=\"SanitizeAgent\", input_ref={})\n",
    "        safe = self._sanitize.sanitize_before_llm(state=state, step_id=\"l0_sanitize\", tool_name=\"sanitize_before_llm\", user_text=text, context=\"resume\")\n",
    "        if safe is None:\n",
    "            LiveFeed.emit(state, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Prompt injection detected. Run blocked.\")\n",
    "            self._persist(state)\n",
    "            return\n",
    "        state.end_step(\"l0_sanitize\", status=\"ok\", output_ref={\"sanitized\": True}, message=\"pass\")\n",
    "        LiveFeed.emit(state, layer=\"L0\", agent=\"SanitizeAgent\", message=\"Security passed.\")\n",
    "        self._persist(state)\n",
    "\n",
    "        # L2/L3 parse loop\n",
    "        extracted = None\n",
    "        fb: List[str] = []\n",
    "        for attempt in range(4):\n",
    "            sid = f\"l2_parse_{attempt+1}\"\n",
    "            state.start_step(sid, layer_id=\"L2\", tool_name=\"ParserAgentService\", input_ref={\"attempt\": attempt+1})\n",
    "            extracted = self._parser.parse(raw_text=safe, orchestration_state=state, feedback=fb)\n",
    "            p = run_dir / f\"extracted_resume_attempt_{attempt+1}.json\"\n",
    "            _save_json(p, extracted.to_json_dict())\n",
    "            state.add_artifact(f\"extracted_resume_attempt_{attempt+1}\", str(p), content_type=\"application/json\")\n",
    "            state.end_step(sid, status=\"ok\", output_ref={\"artifact_key\": f\"extracted_resume_attempt_{attempt+1}\"}, message=\"parsed\")\n",
    "\n",
    "            ev = self._parser_eval.evaluate(\n",
    "                orchestration_state=state, raw_text=safe, extracted=extracted,\n",
    "                target_id=\"resume_main\", threshold=0.80, retry_count=attempt, max_retries=3\n",
    "            )\n",
    "            decision = state.apply_recursive_gate(target_id=\"resume_main\", layer_id=\"L3\")\n",
    "            LiveFeed.emit(state, layer=\"L3\", agent=\"ParserEvaluator\", message=f\"Parse score={ev.evaluation_score:.2f} decision={decision}\")\n",
    "            self._persist(state)\n",
    "\n",
    "            if decision == \"pass\":\n",
    "                state.add_artifact(\"extracted_resume\", str(p), content_type=\"application/json\")\n",
    "                break\n",
    "            if decision == \"human_approval\":\n",
    "                state.status = \"needs_human_approval\"\n",
    "                state.meta[\"pending_action\"] = \"resume_cleanup\"\n",
    "                self._persist(state)\n",
    "                return\n",
    "            fb = ev.feedback\n",
    "\n",
    "        if not extracted:\n",
    "            state.status = \"needs_human_approval\"\n",
    "            state.meta[\"pending_action\"] = \"resume_cleanup\"\n",
    "            self._persist(state)\n",
    "            return\n",
    "\n",
    "        # L3 discovery (simple beta)\n",
    "        prefs = state.meta.get(\"preferences\", {}) or {}\n",
    "        target_role = str(prefs.get(\"target_role\", \"Data Scientist\"))\n",
    "        location = str(prefs.get(\"location\", \"United States\"))\n",
    "        remote = bool(prefs.get(\"remote\", True))\n",
    "        skills = \" \".join((extracted.skills or [])[:6])\n",
    "        intent = \"remote\" if remote else \"on-site\"\n",
    "\n",
    "        if not self._s.serper_api_key:\n",
    "            state.status = \"needs_human_approval\"\n",
    "            state.meta[\"pending_action\"] = \"missing_serper_key\"\n",
    "            LiveFeed.emit(state, layer=\"L3\", agent=\"DiscoveryAgent\", message=\"Missing SERPER_API_KEY in .env.\")\n",
    "            self._persist(state)\n",
    "            return\n",
    "\n",
    "        discovery = SerperDiscoveryService(api_key=self._s.serper_api_key, health=self._health)\n",
    "        q = f'{target_role} {location} {intent} {skills} \"apply\"'\n",
    "        state.start_step(\"l3_discovery\", layer_id=\"L3\", tool_name=\"DiscoveryService\", input_ref={\"query\": q})\n",
    "        LiveFeed.emit(state, layer=\"L3\", agent=\"DiscoveryAgent\", message=\"Searching job boardsâ€¦\")\n",
    "        results = discovery.search(state=state, step_id=\"l3_discovery\", query=q, num=20)\n",
    "        state.end_step(\"l3_discovery\", status=\"ok\", output_ref={\"results\": len(results)}, message=\"discovered\")\n",
    "        self._persist(state)\n",
    "\n",
    "        if state.status in (\"blocked\", \"api_failure\"):\n",
    "            return\n",
    "\n",
    "        # L4 match top 8 results using snippets as job text (beta)\n",
    "        ranked: List[Dict[str, Any]] = []\n",
    "        for it in results[:8]:\n",
    "            job_id = uuid4().hex\n",
    "            jd = JobDescription(\n",
    "                job_id=job_id,\n",
    "                role_title=it.get(\"title\") or target_role,\n",
    "                company=\"JobBoard\",\n",
    "                country_code=str(prefs.get(\"country\", \"US\")),\n",
    "                required_skills=(extracted.skills or [])[:12],\n",
    "                preferred_skills=[],\n",
    "                responsibilities=[],\n",
    "                requirements_text=(it.get(\"snippet\") or \"\"),\n",
    "                applicants_count=None,\n",
    "                market_competition_factor=None,\n",
    "            )\n",
    "            rep = self._matcher.match(resume=extracted, job=jd, orchestration_state=state)\n",
    "            ranked.append({\n",
    "                \"job_id\": job_id,\n",
    "                \"role_title\": jd.role_title,\n",
    "                \"company\": jd.company,\n",
    "                \"url\": it.get(\"link\"),\n",
    "                \"interview_chance_score\": float(rep.interview_chance_score),\n",
    "                \"overall_match_percent\": float(rep.overall_match_percent),\n",
    "                \"matched_skills\": rep.matched_skills[:10],\n",
    "                \"missing_required_skills\": rep.missing_required_skills[:10],\n",
    "            })\n",
    "            state.meta.setdefault(\"job_scores\", {})\n",
    "            state.meta.setdefault(\"job_components\", {})\n",
    "            state.meta.setdefault(\"job_meta\", {})\n",
    "            state.meta[\"job_scores\"][job_id] = float(rep.interview_chance_score)\n",
    "            state.meta[\"job_components\"][job_id] = rep.components.model_dump()\n",
    "            state.meta[\"job_meta\"][job_id] = {\"role_title\": jd.role_title, \"company\": jd.company, \"url\": it.get(\"link\")}\n",
    "\n",
    "        ranked.sort(key=lambda x: x[\"interview_chance_score\"], reverse=True)\n",
    "        _save_json(run_dir / \"ranking.json\", ranked)\n",
    "        state.add_artifact(\"ranking\", str(run_dir / \"ranking.json\"), content_type=\"application/json\")\n",
    "\n",
    "        # HITL pause for ranking review\n",
    "        state.status = \"needs_human_approval\"\n",
    "        state.meta[\"pending_action\"] = \"review_ranking\"\n",
    "        LiveFeed.emit(state, layer=\"L1\", agent=\"Dashboard\", message=\"Ranking ready for review.\")\n",
    "        self._persist(state)\n",
    "\n",
    "        # Pre-generate XAI + ZIP for download\n",
    "        try:\n",
    "            xai = XAIService()\n",
    "            xai_paths = xai.write_outputs(state=state, require_reportlab=False)\n",
    "            state.add_artifact(\"xai_transparency_pdf\", xai_paths[\"pdf\"], content_type=\"application/pdf\")\n",
    "            state.add_artifact(\"transparency_matrix_json\", xai_paths[\"json\"], content_type=\"application/json\")\n",
    "\n",
    "            exporter = CareerDossierExporter()\n",
    "            bundle = exporter.bundle_reports(run_id=state.run_id, final_pdf_path=xai_paths[\"pdf\"])\n",
    "            state.add_artifact(\"career_dossier_zip\", bundle[\"zip\"], content_type=\"application/zip\")\n",
    "            self._persist(state)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _continue(self, run_id: str) -> None:\n",
    "        raw = self.load(run_id)\n",
    "        if not raw:\n",
    "            return\n",
    "        state = OrchestrationState(**raw)\n",
    "        # For now, beta continues are handled by your existing action logic elsewhere.\n",
    "        self._persist(state)\n",
    "\n",
    "\n",
    "ENGINE = OneClickAutomationEngine()\n",
    "'''\n",
    "\n",
    "backup_write(\"src/careeragent/orchestration/engine.py\", ENGINE_PY)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) src/careeragent/orchestration/__init__.py  (EXPORT ENGINE here)\n",
    "# -------------------------------------------------------------------\n",
    "INIT_PY = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "# Export state models/utilities\n",
    "from .state import OrchestrationState  # noqa: F401\n",
    "\n",
    "# Export the one-click engine\n",
    "from .engine import OneClickAutomationEngine, ENGINE  # noqa: F401\n",
    "'''\n",
    "backup_write(\"src/careeragent/orchestration/__init__.py\", INIT_PY)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) app/main.py (Streamlit entry exists in root)\n",
    "# -------------------------------------------------------------------\n",
    "APP_MAIN = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "from app.ui.dashboard import main\n",
    "\n",
    "main()\n",
    "'''\n",
    "backup_write(\"app/main.py\", APP_MAIN)\n",
    "\n",
    "print(\"\\nâœ… FIXED: ENGINE export + app/main.py restored.\")\n",
    "print(\"Now run:\")\n",
    "print(\"uv run python run_app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29b4aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ganeshprasadbhandari/Documents/D_drive/clark/careeragent-ai'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e121fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote: src/careeragent/services/db_service.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ensure package path exists\n",
    "Path(\"src/careeragent/services\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"src/careeragent/services/__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "db_code = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from careeragent.config import artifacts_root\n",
    "\n",
    "\n",
    "class SqliteStateStore:\n",
    "    \"\"\"\n",
    "    Description: Local-first persistence for OrchestrationState using SQLite.\n",
    "    Layer: L8\n",
    "    Input: state JSON snapshots + action logs\n",
    "    Output: durable run state + polling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Description: Initialize sqlite DB under artifacts/db/.\n",
    "        Layer: L0\n",
    "        Input: None\n",
    "        Output: SqliteStateStore\n",
    "        \"\"\"\n",
    "        db_dir = artifacts_root() / \"db\"\n",
    "        db_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self._db_path = db_dir / \"careeragent.db\"\n",
    "        self._init_schema()\n",
    "\n",
    "    def _init_schema(self) -> None:\n",
    "        \"\"\"\n",
    "        Description: Create tables if they do not exist.\n",
    "        Layer: L0\n",
    "        Input: None\n",
    "        Output: SQLite schema initialized\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            con.execute(\n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS runs (\n",
    "                    run_id TEXT PRIMARY KEY,\n",
    "                    status TEXT NOT NULL,\n",
    "                    state_json TEXT NOT NULL,\n",
    "                    updated_at_utc TEXT NOT NULL\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "            con.execute(\n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS actions (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    run_id TEXT NOT NULL,\n",
    "                    action_type TEXT NOT NULL,\n",
    "                    payload_json TEXT NOT NULL,\n",
    "                    created_at_utc TEXT NOT NULL\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "            con.commit()\n",
    "\n",
    "    def upsert_state(self, *, run_id: str, status: str, state: Dict[str, Any], updated_at_utc: str) -> None:\n",
    "        \"\"\"\n",
    "        Description: Upsert run state snapshot.\n",
    "        Layer: L8\n",
    "        Input: run_id + status + state JSON + timestamp\n",
    "        Output: persisted snapshot\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            con.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO runs(run_id, status, state_json, updated_at_utc)\n",
    "                VALUES(?,?,?,?)\n",
    "                ON CONFLICT(run_id) DO UPDATE SET\n",
    "                    status=excluded.status,\n",
    "                    state_json=excluded.state_json,\n",
    "                    updated_at_utc=excluded.updated_at_utc\n",
    "                \"\"\",\n",
    "                (run_id, status, json.dumps(state), updated_at_utc),\n",
    "            )\n",
    "            con.commit()\n",
    "\n",
    "    def get_state(self, *, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Description: Load latest run state snapshot.\n",
    "        Layer: L8\n",
    "        Input: run_id\n",
    "        Output: state dict or None\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            cur = con.execute(\"SELECT state_json FROM runs WHERE run_id=?\", (run_id,))\n",
    "            row = cur.fetchone()\n",
    "            if not row:\n",
    "                return None\n",
    "            return json.loads(row[0])\n",
    "\n",
    "    def insert_action(self, *, run_id: str, action_type: str, payload: Dict[str, Any], created_at_utc: str) -> None:\n",
    "        \"\"\"\n",
    "        Description: Store HITL actions for audit.\n",
    "        Layer: L5\n",
    "        Input: action record\n",
    "        Output: persisted action row\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self._db_path) as con:\n",
    "            con.execute(\n",
    "                \"INSERT INTO actions(run_id, action_type, payload_json, created_at_utc) VALUES(?,?,?,?)\",\n",
    "                (run_id, action_type, json.dumps(payload), created_at_utc),\n",
    "            )\n",
    "            con.commit()\n",
    "'''\n",
    "\n",
    "Path(\"src/careeragent/services/db_service.py\").write_text(db_code.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"âœ… Wrote: src/careeragent/services/db_service.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careeragent-ai (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
